{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "materials-xception-modified-cross-validated.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkmsvFPdzTPB8W7xwGsihw"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERKJLyg2z-Uq"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fIbsvD01EQm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04770108-5136-4dec-ab3f-c6f9f7ef9347"
      },
      "source": [
        "import pathlib\n",
        "\n",
        "# set path to FMD image directory\n",
        "data_dir = pathlib.Path(\"image\")\n",
        "\n",
        "# total no. of images in FMD dataset\n",
        "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
        "print(image_count)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL3rBqoe3sK5"
      },
      "source": [
        "# set batch size for training\n",
        "batch_size = 16\n",
        "\n",
        "# set dimensions to change images into for training\n",
        "# 299x299 images is used to train for consistency between different pre-trained models\n",
        "img_height = 299\n",
        "img_width = 299"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap_FpvbEqWrS"
      },
      "source": [
        "# create dataset from the image directory\n",
        "list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*.jpg'), shuffle=False)\n",
        "# shuffle the 1,000 images with the random seed value of 123 before training\n",
        "list_ds = list_ds.shuffle(image_count, seed=123, reshuffle_each_iteration=False)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFKuhNRxqxcB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9c80b6e-5827-4476-9c82-590c676c53f8"
      },
      "source": [
        "# get class names from the folder names in data_dir\n",
        "class_names = np.array(sorted([item.name for item in data_dir.glob('*') if item.name != \"LICENSE.txt\"]))\n",
        "print(class_names)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['fabric' 'foliage' 'glass' 'leather' 'metal' 'paper' 'plastic' 'stone'\n",
            " 'water' 'wood']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACILObxurdXN"
      },
      "source": [
        "# split dataset into 5 equal sized parts for 5-fold cross validation\n",
        "A = list_ds.shard(num_shards=5, index=0)\n",
        "B = list_ds.shard(num_shards=5, index=1)\n",
        "C = list_ds.shard(num_shards=5, index=2)\n",
        "D = list_ds.shard(num_shards=5, index=3)\n",
        "E = list_ds.shard(num_shards=5, index=4)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9oYdHkhrwdz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef9c91cb-1e3f-4a57-ea6e-94b34f3d1524"
      },
      "source": [
        "# check no. of samples in each partition is the same\n",
        "print(A.cardinality().numpy())\n",
        "print(B.cardinality().numpy())\n",
        "print(C.cardinality().numpy())\n",
        "print(D.cardinality().numpy())\n",
        "print(E.cardinality().numpy())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BdD3FMHtGRV"
      },
      "source": [
        "def get_label(file_path):\n",
        "  # convert the path to a list of path components\n",
        "  parts = tf.strings.split(file_path, os.path.sep)\n",
        "  # The second to last is the class-directory\n",
        "  one_hot = parts[-2] == class_names\n",
        "  # Integer encode the label\n",
        "  return tf.argmax(one_hot)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfWduaL4tKDV"
      },
      "source": [
        "def decode_img(img):\n",
        "  # convert the compressed string to a 3D uint8 tensor\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\n",
        "  # resize the image to the desired size\n",
        "  return tf.image.resize(img, [img_height, img_width])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bGGe0KltMTC"
      },
      "source": [
        "def process_path(file_path):\n",
        "  label = get_label(file_path)\n",
        "  # load the raw data from the file as a string\n",
        "  img = tf.io.read_file(file_path)\n",
        "  img = decode_img(img)\n",
        "  return img, label"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcyZLwRxt1dy"
      },
      "source": [
        "# prompt the tf.data runtime to tune the number of elements to prefetch dynamically at runtime\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkBqAQlHtblh"
      },
      "source": [
        "# use the path of image to load the image into each partition of the dataset\n",
        "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
        "A = A.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "B = B.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "C = C.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "D = D.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "E = E.map(process_path, num_parallel_calls=AUTOTUNE)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc9pmaQ5t9H5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dad3b2b-eb83-4062-e03f-2b317de7a3a1"
      },
      "source": [
        "for image, label in A.take(1):\n",
        "  print(\"Image shape: \", image.numpy().shape)\n",
        "  print(\"Label: \", label.numpy())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image shape:  (299, 299, 3)\n",
            "Label:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_T2KNdGub1X"
      },
      "source": [
        "# shuffle, batch, and prefetch the dataset\n",
        "def configure_for_performance(ds):\n",
        "  ds = ds.cache()\n",
        "  ds = ds.shuffle(buffer_size=1000)\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
        "  return ds"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWcojoVRuok5"
      },
      "source": [
        "# map the dataset partitions to integers for building training/test sets during cross-validation\n",
        "ds_fold_dict = {0:A, 1:B, 2:C, 3:D, 4:E}"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx3xKoMZU9Yv"
      },
      "source": [
        "# normalise the input values to the pre-trained model's required range of values\n",
        "preprocess_input = keras.applications.xception.preprocess_input"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVOP5fIPwEx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfff30fd-e991-4e99-b14b-b83a6e180219"
      },
      "source": [
        "# get pre-trained model\n",
        "base_model = keras.applications.Xception(include_top=False, input_shape=(img_height, img_width, 3))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS2kAceGVW0e"
      },
      "source": [
        "# don't train base model weights\n",
        "base_model.trainable = False"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGkReMX60ScJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc5dcc54-b900-4e4a-95ed-93679965e73b"
      },
      "source": [
        "base_model.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"xception\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 299, 299, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1 (Conv2D)           (None, 149, 149, 32) 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_bn (BatchNormaliza (None, 149, 149, 32) 128         block1_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_act (Activation)   (None, 149, 149, 32) 0           block1_conv1_bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2 (Conv2D)           (None, 147, 147, 64) 18432       block1_conv1_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_bn (BatchNormaliza (None, 147, 147, 64) 256         block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_act (Activation)   (None, 147, 147, 64) 0           block1_conv2_bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv1 (SeparableConv2 (None, 147, 147, 128 8768        block1_conv2_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv1_bn (BatchNormal (None, 147, 147, 128 512         block2_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2_act (Activation (None, 147, 147, 128 0           block2_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2 (SeparableConv2 (None, 147, 147, 128 17536       block2_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2_bn (BatchNormal (None, 147, 147, 128 512         block2_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 74, 74, 128)  8192        block1_conv2_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool (MaxPooling2D)      (None, 74, 74, 128)  0           block2_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 74, 74, 128)  512         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 74, 74, 128)  0           block2_pool[0][0]                \n",
            "                                                                 batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1_act (Activation (None, 74, 74, 128)  0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1 (SeparableConv2 (None, 74, 74, 256)  33920       block3_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1_bn (BatchNormal (None, 74, 74, 256)  1024        block3_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2_act (Activation (None, 74, 74, 256)  0           block3_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2 (SeparableConv2 (None, 74, 74, 256)  67840       block3_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2_bn (BatchNormal (None, 74, 74, 256)  1024        block3_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 37, 37, 256)  32768       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool (MaxPooling2D)      (None, 37, 37, 256)  0           block3_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 37, 37, 256)  1024        conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 37, 37, 256)  0           block3_pool[0][0]                \n",
            "                                                                 batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1_act (Activation (None, 37, 37, 256)  0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1 (SeparableConv2 (None, 37, 37, 728)  188672      block4_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1_bn (BatchNormal (None, 37, 37, 728)  2912        block4_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2_act (Activation (None, 37, 37, 728)  0           block4_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2 (SeparableConv2 (None, 37, 37, 728)  536536      block4_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2_bn (BatchNormal (None, 37, 37, 728)  2912        block4_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 19, 19, 728)  186368      add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool (MaxPooling2D)      (None, 19, 19, 728)  0           block4_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 19, 19, 728)  2912        conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 19, 19, 728)  0           block4_pool[0][0]                \n",
            "                                                                 batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1_act (Activation (None, 19, 19, 728)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block5_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block5_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2_act (Activation (None, 19, 19, 728)  0           block5_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block5_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block5_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3_act (Activation (None, 19, 19, 728)  0           block5_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block5_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block5_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 19, 19, 728)  0           block5_sepconv3_bn[0][0]         \n",
            "                                                                 add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1_act (Activation (None, 19, 19, 728)  0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block6_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block6_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2_act (Activation (None, 19, 19, 728)  0           block6_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block6_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block6_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3_act (Activation (None, 19, 19, 728)  0           block6_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block6_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block6_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 19, 19, 728)  0           block6_sepconv3_bn[0][0]         \n",
            "                                                                 add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1_act (Activation (None, 19, 19, 728)  0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block7_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block7_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2_act (Activation (None, 19, 19, 728)  0           block7_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block7_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block7_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3_act (Activation (None, 19, 19, 728)  0           block7_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block7_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block7_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 19, 19, 728)  0           block7_sepconv3_bn[0][0]         \n",
            "                                                                 add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1_act (Activation (None, 19, 19, 728)  0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block8_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block8_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2_act (Activation (None, 19, 19, 728)  0           block8_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block8_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block8_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3_act (Activation (None, 19, 19, 728)  0           block8_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block8_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block8_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 19, 19, 728)  0           block8_sepconv3_bn[0][0]         \n",
            "                                                                 add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1_act (Activation (None, 19, 19, 728)  0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block9_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block9_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2_act (Activation (None, 19, 19, 728)  0           block9_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block9_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block9_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3_act (Activation (None, 19, 19, 728)  0           block9_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block9_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block9_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 19, 19, 728)  0           block9_sepconv3_bn[0][0]         \n",
            "                                                                 add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block10_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block10_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2_act (Activatio (None, 19, 19, 728)  0           block10_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2 (SeparableConv (None, 19, 19, 728)  536536      block10_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2_bn (BatchNorma (None, 19, 19, 728)  2912        block10_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3_act (Activatio (None, 19, 19, 728)  0           block10_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3 (SeparableConv (None, 19, 19, 728)  536536      block10_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3_bn (BatchNorma (None, 19, 19, 728)  2912        block10_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 19, 19, 728)  0           block10_sepconv3_bn[0][0]        \n",
            "                                                                 add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block11_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block11_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2_act (Activatio (None, 19, 19, 728)  0           block11_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2 (SeparableConv (None, 19, 19, 728)  536536      block11_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2_bn (BatchNorma (None, 19, 19, 728)  2912        block11_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3_act (Activatio (None, 19, 19, 728)  0           block11_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3 (SeparableConv (None, 19, 19, 728)  536536      block11_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3_bn (BatchNorma (None, 19, 19, 728)  2912        block11_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 19, 19, 728)  0           block11_sepconv3_bn[0][0]        \n",
            "                                                                 add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block12_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block12_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2_act (Activatio (None, 19, 19, 728)  0           block12_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2 (SeparableConv (None, 19, 19, 728)  536536      block12_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2_bn (BatchNorma (None, 19, 19, 728)  2912        block12_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3_act (Activatio (None, 19, 19, 728)  0           block12_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3 (SeparableConv (None, 19, 19, 728)  536536      block12_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3_bn (BatchNorma (None, 19, 19, 728)  2912        block12_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 19, 19, 728)  0           block12_sepconv3_bn[0][0]        \n",
            "                                                                 add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block13_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block13_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2_act (Activatio (None, 19, 19, 728)  0           block13_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2 (SeparableConv (None, 19, 19, 1024) 752024      block13_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2_bn (BatchNorma (None, 19, 19, 1024) 4096        block13_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 10, 10, 1024) 745472      add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_pool (MaxPooling2D)     (None, 10, 10, 1024) 0           block13_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 10, 10, 1024) 4096        conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 10, 10, 1024) 0           block13_pool[0][0]               \n",
            "                                                                 batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1 (SeparableConv (None, 10, 10, 1536) 1582080     add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1_bn (BatchNorma (None, 10, 10, 1536) 6144        block14_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1_act (Activatio (None, 10, 10, 1536) 0           block14_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2 (SeparableConv (None, 10, 10, 2048) 3159552     block14_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2_bn (BatchNorma (None, 10, 10, 2048) 8192        block14_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2_act (Activatio (None, 10, 10, 2048) 0           block14_sepconv2_bn[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 20,861,480\n",
            "Trainable params: 0\n",
            "Non-trainable params: 20,861,480\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzhaWb2aX2if"
      },
      "source": [
        "# set a low learning rate to avoid overfitting too quickly\n",
        "base_learning_rate = 0.0001\n",
        "\n",
        "# create the model to train using the pre-trained model as base model\n",
        "def create_model():\n",
        "  # generate additional training data from input training data by augmenting them using random flip, rotation & zoom\n",
        "  data_augmentation = keras.Sequential(\n",
        "    [\n",
        "      layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n",
        "                                                  input_shape=(img_height, \n",
        "                                                                img_width,\n",
        "                                                                3)),\n",
        "      layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "      layers.experimental.preprocessing.RandomZoom(0.1),\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  # average over the spatial locations to convert the features to a single vector per image\n",
        "  global_average_layer = keras.layers.GlobalAveragePooling2D()\n",
        "  # increase network depth by adding additional fully connected layer of size 128 with ReLU activation\n",
        "  fully_connected_layer = keras.layers.Dense(128, activation='relu')\n",
        "  # convert these features into a single prediction per image\n",
        "  prediction_layer = keras.layers.Dense(10)\n",
        "\n",
        "  # Build a model by chaining together the layers using the Keras Functional API.\n",
        "  inputs = keras.Input(shape=(img_height, img_width, 3))\n",
        "  x = data_augmentation(inputs)\n",
        "  x = preprocess_input(x)\n",
        "  x = base_model(x, training=False) # use training=False as the base model contains a BatchNormalization layer\n",
        "  x = global_average_layer(x)\n",
        "  x = keras.layers.Dropout(0.2)(x) # add dropout to fully connected layer to reduce overfitting\n",
        "  x = fully_connected_layer(x)\n",
        "  x = keras.layers.Dropout(0.2)(x) # add dropout to fully connected layer to reduce overfitting\n",
        "  outputs = prediction_layer(x)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  optimizer = keras.optimizers.Adam(lr=base_learning_rate)\n",
        "  loss= keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  # compile model with Adam optimizer with specified learning rate\n",
        "  model.compile(optimizer=optimizer,\n",
        "                loss=loss,\n",
        "                metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-5TEZRgY2l_"
      },
      "source": [
        "no_epochs = 100"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya698zRbu7Ep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44bf15be-0365-4270-b34b-45ca4b7b8325"
      },
      "source": [
        "# store results to plot graphs and get cross-validated accuracies\n",
        "history_map = {}\n",
        "base_model_acc_list = []\n",
        "final_acc_list = []\n",
        "\n",
        "# do 5-fold cross-validation\n",
        "for i in range(5):\n",
        "  print('fold', i + 1)\n",
        "  temp_dict = ds_fold_dict.copy()\n",
        "  # get test set for this iteration\n",
        "  current_val_ds = temp_dict[i]\n",
        "\n",
        "  # get training set for this iteration from remaining data samples\n",
        "  del temp_dict[i]\n",
        "  current_train_ds = None\n",
        "  for ds_shard in temp_dict.values():\n",
        "    if current_train_ds is None:\n",
        "      current_train_ds = ds_shard\n",
        "    else:\n",
        "      current_train_ds = current_train_ds.concatenate(ds_shard)\n",
        "  \n",
        "  # configure both training and test sets to improve performance\n",
        "  current_train_ds = configure_for_performance(current_train_ds)\n",
        "  current_val_ds = configure_for_performance(current_val_ds)\n",
        "\n",
        "  # create a new model\n",
        "  model = create_model()\n",
        "  # get initial test accuracy\n",
        "  base_model_acc_list.append(model.evaluate(current_val_ds)[1])\n",
        "  # train for specified epochs\n",
        "  history = model.fit(current_train_ds,\n",
        "                    epochs=no_epochs,\n",
        "                    validation_data=current_val_ds)\n",
        "  \n",
        "  # save results\n",
        "  if i == 0:\n",
        "    history_map['accuracy'] = [history.history['accuracy']]\n",
        "    history_map['val_accuracy'] = [history.history['val_accuracy']]\n",
        "    history_map['loss'] = [history.history['loss']]\n",
        "    history_map['val_loss'] = [history.history['val_loss']]\n",
        "  else:\n",
        "    history_map['accuracy'].append(history.history['accuracy'])\n",
        "    history_map['val_accuracy'].append(history.history['val_accuracy'])\n",
        "    history_map['loss'].append(history.history['loss'])\n",
        "    history_map['val_loss'].append(history.history['val_loss'])\n",
        "  \n",
        "  # get final test accuracy by taking the max accuracy over the whole training\n",
        "  final_acc_list.append(np.amax(history.history['val_accuracy']))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fold 1\n",
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential (Sequential)      (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv (TensorF [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub (TensorFlowO [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "xception (Functional)        (None, 10, 10, 2048)      20861480  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               262272    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 21,125,042\n",
            "Trainable params: 263,562\n",
            "Non-trainable params: 20,861,480\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 1s 78ms/step - loss: 2.3507 - accuracy: 0.1100\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 2.2325 - accuracy: 0.2075 - val_loss: 2.0740 - val_accuracy: 0.4150\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 1.9474 - accuracy: 0.4338 - val_loss: 1.8225 - val_accuracy: 0.5750\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 1.6910 - accuracy: 0.5713 - val_loss: 1.5618 - val_accuracy: 0.6200\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 1.4137 - accuracy: 0.6775 - val_loss: 1.3351 - val_accuracy: 0.7200\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 1.2133 - accuracy: 0.7138 - val_loss: 1.1647 - val_accuracy: 0.7400\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 1.0408 - accuracy: 0.7588 - val_loss: 1.0405 - val_accuracy: 0.7400\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.9344 - accuracy: 0.7738 - val_loss: 0.9531 - val_accuracy: 0.7600\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.8136 - accuracy: 0.8025 - val_loss: 0.8707 - val_accuracy: 0.7700\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.7770 - accuracy: 0.8012 - val_loss: 0.8167 - val_accuracy: 0.7750\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.6794 - accuracy: 0.8338 - val_loss: 0.7670 - val_accuracy: 0.7750\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.6535 - accuracy: 0.8275 - val_loss: 0.7464 - val_accuracy: 0.7800\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.5785 - accuracy: 0.8662 - val_loss: 0.7090 - val_accuracy: 0.7900\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.5735 - accuracy: 0.8662 - val_loss: 0.6796 - val_accuracy: 0.8050\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.5671 - accuracy: 0.8425 - val_loss: 0.6536 - val_accuracy: 0.8000\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.5258 - accuracy: 0.8625 - val_loss: 0.6502 - val_accuracy: 0.7900\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.4779 - accuracy: 0.8675 - val_loss: 0.6503 - val_accuracy: 0.8100\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.4875 - accuracy: 0.8662 - val_loss: 0.6203 - val_accuracy: 0.8050\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.4742 - accuracy: 0.8675 - val_loss: 0.5996 - val_accuracy: 0.8150\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.4573 - accuracy: 0.8750 - val_loss: 0.5850 - val_accuracy: 0.8250\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.4205 - accuracy: 0.8913 - val_loss: 0.5937 - val_accuracy: 0.8100\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 5s 99ms/step - loss: 0.4038 - accuracy: 0.9075 - val_loss: 0.5760 - val_accuracy: 0.8050\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.4046 - accuracy: 0.8888 - val_loss: 0.5713 - val_accuracy: 0.8100\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.3395 - accuracy: 0.9162 - val_loss: 0.5643 - val_accuracy: 0.8150\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.3573 - accuracy: 0.8913 - val_loss: 0.5505 - val_accuracy: 0.8250\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.3686 - accuracy: 0.8888 - val_loss: 0.5692 - val_accuracy: 0.8100\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.3213 - accuracy: 0.9162 - val_loss: 0.5504 - val_accuracy: 0.8200\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.3219 - accuracy: 0.9200 - val_loss: 0.5476 - val_accuracy: 0.8150\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.3234 - accuracy: 0.9087 - val_loss: 0.5331 - val_accuracy: 0.8300\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.3270 - accuracy: 0.9112 - val_loss: 0.5228 - val_accuracy: 0.8400\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.3044 - accuracy: 0.9175 - val_loss: 0.5481 - val_accuracy: 0.8150\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.3061 - accuracy: 0.9162 - val_loss: 0.5147 - val_accuracy: 0.8400\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.2597 - accuracy: 0.9438 - val_loss: 0.5332 - val_accuracy: 0.8150\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.2853 - accuracy: 0.9287 - val_loss: 0.5334 - val_accuracy: 0.8200\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.2724 - accuracy: 0.9287 - val_loss: 0.5045 - val_accuracy: 0.8350\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.2905 - accuracy: 0.9187 - val_loss: 0.5266 - val_accuracy: 0.8200\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.2582 - accuracy: 0.9325 - val_loss: 0.5246 - val_accuracy: 0.8250\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.2385 - accuracy: 0.9400 - val_loss: 0.5142 - val_accuracy: 0.8250\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.2466 - accuracy: 0.9350 - val_loss: 0.5116 - val_accuracy: 0.8250\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.2459 - accuracy: 0.9350 - val_loss: 0.5189 - val_accuracy: 0.8300\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.2287 - accuracy: 0.9438 - val_loss: 0.5110 - val_accuracy: 0.8400\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.2235 - accuracy: 0.9400 - val_loss: 0.5141 - val_accuracy: 0.8250\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.2162 - accuracy: 0.9550 - val_loss: 0.5204 - val_accuracy: 0.8300\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.2268 - accuracy: 0.9413 - val_loss: 0.5019 - val_accuracy: 0.8300\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.2182 - accuracy: 0.9413 - val_loss: 0.5084 - val_accuracy: 0.8100\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.2204 - accuracy: 0.9413 - val_loss: 0.4971 - val_accuracy: 0.8250\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1963 - accuracy: 0.9475 - val_loss: 0.4930 - val_accuracy: 0.8500\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1989 - accuracy: 0.9450 - val_loss: 0.4927 - val_accuracy: 0.8350\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.2087 - accuracy: 0.9413 - val_loss: 0.4929 - val_accuracy: 0.8200\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1902 - accuracy: 0.9525 - val_loss: 0.4905 - val_accuracy: 0.8350\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 5s 99ms/step - loss: 0.1838 - accuracy: 0.9575 - val_loss: 0.4905 - val_accuracy: 0.8500\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1961 - accuracy: 0.9563 - val_loss: 0.5006 - val_accuracy: 0.8300\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1922 - accuracy: 0.9500 - val_loss: 0.4887 - val_accuracy: 0.8400\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1690 - accuracy: 0.9650 - val_loss: 0.4938 - val_accuracy: 0.8400\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1744 - accuracy: 0.9613 - val_loss: 0.5058 - val_accuracy: 0.8300\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1713 - accuracy: 0.9575 - val_loss: 0.4914 - val_accuracy: 0.8300\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1549 - accuracy: 0.9550 - val_loss: 0.5128 - val_accuracy: 0.8250\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1758 - accuracy: 0.9563 - val_loss: 0.5004 - val_accuracy: 0.8400\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1568 - accuracy: 0.9625 - val_loss: 0.5043 - val_accuracy: 0.8350\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1576 - accuracy: 0.9663 - val_loss: 0.5048 - val_accuracy: 0.8350\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1555 - accuracy: 0.9688 - val_loss: 0.4989 - val_accuracy: 0.8400\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1564 - accuracy: 0.9600 - val_loss: 0.5013 - val_accuracy: 0.8500\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1330 - accuracy: 0.9737 - val_loss: 0.4997 - val_accuracy: 0.8500\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1616 - accuracy: 0.9538 - val_loss: 0.5052 - val_accuracy: 0.8300\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1574 - accuracy: 0.9613 - val_loss: 0.5085 - val_accuracy: 0.8400\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1412 - accuracy: 0.9737 - val_loss: 0.5087 - val_accuracy: 0.8300\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 5s 99ms/step - loss: 0.1409 - accuracy: 0.9638 - val_loss: 0.5040 - val_accuracy: 0.8450\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 5s 99ms/step - loss: 0.1424 - accuracy: 0.9600 - val_loss: 0.5073 - val_accuracy: 0.8400\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1492 - accuracy: 0.9625 - val_loss: 0.4931 - val_accuracy: 0.8650\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1239 - accuracy: 0.9737 - val_loss: 0.5076 - val_accuracy: 0.8400\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1302 - accuracy: 0.9688 - val_loss: 0.5141 - val_accuracy: 0.8400\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1246 - accuracy: 0.9700 - val_loss: 0.5183 - val_accuracy: 0.8100\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1196 - accuracy: 0.9737 - val_loss: 0.5304 - val_accuracy: 0.8350\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1214 - accuracy: 0.9762 - val_loss: 0.4978 - val_accuracy: 0.8350\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 5s 99ms/step - loss: 0.1203 - accuracy: 0.9725 - val_loss: 0.5138 - val_accuracy: 0.8300\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1297 - accuracy: 0.9675 - val_loss: 0.5003 - val_accuracy: 0.8450\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1327 - accuracy: 0.9663 - val_loss: 0.5173 - val_accuracy: 0.8300\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1226 - accuracy: 0.9712 - val_loss: 0.5239 - val_accuracy: 0.8350\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1246 - accuracy: 0.9675 - val_loss: 0.5169 - val_accuracy: 0.8400\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1156 - accuracy: 0.9750 - val_loss: 0.5359 - val_accuracy: 0.8350\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1016 - accuracy: 0.9725 - val_loss: 0.5139 - val_accuracy: 0.8250\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 5s 99ms/step - loss: 0.1090 - accuracy: 0.9787 - val_loss: 0.5165 - val_accuracy: 0.8350\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1247 - accuracy: 0.9625 - val_loss: 0.5155 - val_accuracy: 0.8300\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 5s 99ms/step - loss: 0.1208 - accuracy: 0.9737 - val_loss: 0.5268 - val_accuracy: 0.8200\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1200 - accuracy: 0.9663 - val_loss: 0.5231 - val_accuracy: 0.8350\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1048 - accuracy: 0.9800 - val_loss: 0.5222 - val_accuracy: 0.8300\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 5s 99ms/step - loss: 0.1122 - accuracy: 0.9700 - val_loss: 0.5165 - val_accuracy: 0.8300\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1019 - accuracy: 0.9775 - val_loss: 0.5259 - val_accuracy: 0.8300\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1177 - accuracy: 0.9712 - val_loss: 0.5167 - val_accuracy: 0.8250\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.0865 - accuracy: 0.9825 - val_loss: 0.5341 - val_accuracy: 0.8300\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.0920 - accuracy: 0.9812 - val_loss: 0.5214 - val_accuracy: 0.8300\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1032 - accuracy: 0.9775 - val_loss: 0.5422 - val_accuracy: 0.8250\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.0947 - accuracy: 0.9787 - val_loss: 0.5312 - val_accuracy: 0.8200\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.0779 - accuracy: 0.9887 - val_loss: 0.5358 - val_accuracy: 0.8200\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1092 - accuracy: 0.9725 - val_loss: 0.5168 - val_accuracy: 0.8350\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.0934 - accuracy: 0.9775 - val_loss: 0.5065 - val_accuracy: 0.8300\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.0898 - accuracy: 0.9775 - val_loss: 0.5303 - val_accuracy: 0.8300\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.0900 - accuracy: 0.9775 - val_loss: 0.5403 - val_accuracy: 0.8200\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.0856 - accuracy: 0.9837 - val_loss: 0.5152 - val_accuracy: 0.8200\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.0859 - accuracy: 0.9837 - val_loss: 0.5420 - val_accuracy: 0.8250\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.0753 - accuracy: 0.9862 - val_loss: 0.5154 - val_accuracy: 0.8250\n",
            "fold 2\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_1 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_1 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_1 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "xception (Functional)        (None, 10, 10, 2048)      20861480  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               262272    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 21,125,042\n",
            "Trainable params: 263,562\n",
            "Non-trainable params: 20,861,480\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 1s 68ms/step - loss: 2.3467 - accuracy: 0.0850\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 2.2193 - accuracy: 0.2000 - val_loss: 2.0582 - val_accuracy: 0.4250\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 1.9350 - accuracy: 0.4175 - val_loss: 1.7938 - val_accuracy: 0.5600\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 1.6382 - accuracy: 0.6112 - val_loss: 1.5267 - val_accuracy: 0.7000\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 1.3976 - accuracy: 0.6550 - val_loss: 1.2943 - val_accuracy: 0.7450\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 1.1896 - accuracy: 0.7188 - val_loss: 1.1330 - val_accuracy: 0.7550\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 1.0348 - accuracy: 0.7563 - val_loss: 1.0022 - val_accuracy: 0.7800\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.9075 - accuracy: 0.7800 - val_loss: 0.9102 - val_accuracy: 0.7700\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.8445 - accuracy: 0.7825 - val_loss: 0.8444 - val_accuracy: 0.7850\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.7452 - accuracy: 0.8175 - val_loss: 0.7897 - val_accuracy: 0.8100\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.7085 - accuracy: 0.8225 - val_loss: 0.7507 - val_accuracy: 0.8100\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.6410 - accuracy: 0.8400 - val_loss: 0.7154 - val_accuracy: 0.8200\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.5943 - accuracy: 0.8637 - val_loss: 0.6893 - val_accuracy: 0.8100\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.5918 - accuracy: 0.8425 - val_loss: 0.6572 - val_accuracy: 0.8200\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.5502 - accuracy: 0.8575 - val_loss: 0.6572 - val_accuracy: 0.8000\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.5195 - accuracy: 0.8637 - val_loss: 0.6373 - val_accuracy: 0.8050\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.5062 - accuracy: 0.8675 - val_loss: 0.6146 - val_accuracy: 0.8200\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.4727 - accuracy: 0.8687 - val_loss: 0.6034 - val_accuracy: 0.8100\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.4620 - accuracy: 0.8825 - val_loss: 0.6042 - val_accuracy: 0.8200\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.4224 - accuracy: 0.8863 - val_loss: 0.5796 - val_accuracy: 0.8300\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.4059 - accuracy: 0.8838 - val_loss: 0.5716 - val_accuracy: 0.8250\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.4063 - accuracy: 0.8913 - val_loss: 0.5651 - val_accuracy: 0.8250\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.3920 - accuracy: 0.8950 - val_loss: 0.5484 - val_accuracy: 0.8400\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.3888 - accuracy: 0.8888 - val_loss: 0.5529 - val_accuracy: 0.8250\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.3760 - accuracy: 0.9038 - val_loss: 0.5398 - val_accuracy: 0.8400\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.3327 - accuracy: 0.9162 - val_loss: 0.5529 - val_accuracy: 0.8250\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.3466 - accuracy: 0.9050 - val_loss: 0.5294 - val_accuracy: 0.8300\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.3249 - accuracy: 0.9212 - val_loss: 0.5368 - val_accuracy: 0.8400\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.3246 - accuracy: 0.9125 - val_loss: 0.5276 - val_accuracy: 0.8300\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.3007 - accuracy: 0.9150 - val_loss: 0.5281 - val_accuracy: 0.8400\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.2915 - accuracy: 0.9337 - val_loss: 0.5208 - val_accuracy: 0.8450\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.2984 - accuracy: 0.9225 - val_loss: 0.5236 - val_accuracy: 0.8350\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.2728 - accuracy: 0.9362 - val_loss: 0.5146 - val_accuracy: 0.8400\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.2915 - accuracy: 0.9225 - val_loss: 0.5220 - val_accuracy: 0.8350\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.2766 - accuracy: 0.9275 - val_loss: 0.5263 - val_accuracy: 0.8250\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.2664 - accuracy: 0.9287 - val_loss: 0.5028 - val_accuracy: 0.8500\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.2746 - accuracy: 0.9350 - val_loss: 0.5062 - val_accuracy: 0.8400\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.2459 - accuracy: 0.9337 - val_loss: 0.5025 - val_accuracy: 0.8450\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.2200 - accuracy: 0.9538 - val_loss: 0.5002 - val_accuracy: 0.8500\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.2316 - accuracy: 0.9362 - val_loss: 0.4959 - val_accuracy: 0.8400\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.2436 - accuracy: 0.9425 - val_loss: 0.4892 - val_accuracy: 0.8500\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1979 - accuracy: 0.9575 - val_loss: 0.4909 - val_accuracy: 0.8550\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.2202 - accuracy: 0.9550 - val_loss: 0.4897 - val_accuracy: 0.8550\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.2213 - accuracy: 0.9388 - val_loss: 0.4849 - val_accuracy: 0.8600\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.2077 - accuracy: 0.9425 - val_loss: 0.4852 - val_accuracy: 0.8500\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.2060 - accuracy: 0.9488 - val_loss: 0.4807 - val_accuracy: 0.8600\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.2144 - accuracy: 0.9463 - val_loss: 0.4857 - val_accuracy: 0.8600\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.2063 - accuracy: 0.9450 - val_loss: 0.4737 - val_accuracy: 0.8500\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.2045 - accuracy: 0.9500 - val_loss: 0.4837 - val_accuracy: 0.8550\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1932 - accuracy: 0.9513 - val_loss: 0.4736 - val_accuracy: 0.8600\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1864 - accuracy: 0.9575 - val_loss: 0.4768 - val_accuracy: 0.8600\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1822 - accuracy: 0.9600 - val_loss: 0.4752 - val_accuracy: 0.8650\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1868 - accuracy: 0.9475 - val_loss: 0.4681 - val_accuracy: 0.8650\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1683 - accuracy: 0.9600 - val_loss: 0.4716 - val_accuracy: 0.8650\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1774 - accuracy: 0.9588 - val_loss: 0.4797 - val_accuracy: 0.8650\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1854 - accuracy: 0.9513 - val_loss: 0.4706 - val_accuracy: 0.8700\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1533 - accuracy: 0.9663 - val_loss: 0.4716 - val_accuracy: 0.8650\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1722 - accuracy: 0.9500 - val_loss: 0.4698 - val_accuracy: 0.8550\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1621 - accuracy: 0.9525 - val_loss: 0.4831 - val_accuracy: 0.8500\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1650 - accuracy: 0.9550 - val_loss: 0.4628 - val_accuracy: 0.8800\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1474 - accuracy: 0.9688 - val_loss: 0.4896 - val_accuracy: 0.8550\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1518 - accuracy: 0.9613 - val_loss: 0.4702 - val_accuracy: 0.8650\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1448 - accuracy: 0.9688 - val_loss: 0.4896 - val_accuracy: 0.8600\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1583 - accuracy: 0.9575 - val_loss: 0.4825 - val_accuracy: 0.8500\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1551 - accuracy: 0.9625 - val_loss: 0.4742 - val_accuracy: 0.8650\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1273 - accuracy: 0.9737 - val_loss: 0.4693 - val_accuracy: 0.8700\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1525 - accuracy: 0.9650 - val_loss: 0.4783 - val_accuracy: 0.8550\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1437 - accuracy: 0.9625 - val_loss: 0.4827 - val_accuracy: 0.8700\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1428 - accuracy: 0.9638 - val_loss: 0.4783 - val_accuracy: 0.8550\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1518 - accuracy: 0.9638 - val_loss: 0.5017 - val_accuracy: 0.8500\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1351 - accuracy: 0.9688 - val_loss: 0.4654 - val_accuracy: 0.8600\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1312 - accuracy: 0.9700 - val_loss: 0.4767 - val_accuracy: 0.8600\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.1379 - accuracy: 0.9750 - val_loss: 0.4584 - val_accuracy: 0.8750\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1504 - accuracy: 0.9663 - val_loss: 0.4662 - val_accuracy: 0.8650\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1314 - accuracy: 0.9762 - val_loss: 0.4743 - val_accuracy: 0.8550\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1212 - accuracy: 0.9675 - val_loss: 0.4686 - val_accuracy: 0.8650\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.1443 - accuracy: 0.9613 - val_loss: 0.4760 - val_accuracy: 0.8600\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1203 - accuracy: 0.9712 - val_loss: 0.4704 - val_accuracy: 0.8700\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.1145 - accuracy: 0.9737 - val_loss: 0.4859 - val_accuracy: 0.8750\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1362 - accuracy: 0.9588 - val_loss: 0.4919 - val_accuracy: 0.8500\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1207 - accuracy: 0.9700 - val_loss: 0.4964 - val_accuracy: 0.8600\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.1172 - accuracy: 0.9737 - val_loss: 0.4795 - val_accuracy: 0.8650\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.1166 - accuracy: 0.9638 - val_loss: 0.4732 - val_accuracy: 0.8700\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1088 - accuracy: 0.9762 - val_loss: 0.4753 - val_accuracy: 0.8700\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.0921 - accuracy: 0.9850 - val_loss: 0.4879 - val_accuracy: 0.8700\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.1000 - accuracy: 0.9725 - val_loss: 0.4771 - val_accuracy: 0.8650\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.1035 - accuracy: 0.9787 - val_loss: 0.4764 - val_accuracy: 0.8550\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1058 - accuracy: 0.9725 - val_loss: 0.4752 - val_accuracy: 0.8700\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.0987 - accuracy: 0.9812 - val_loss: 0.4869 - val_accuracy: 0.8600\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.0931 - accuracy: 0.9850 - val_loss: 0.4794 - val_accuracy: 0.8600\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.0885 - accuracy: 0.9812 - val_loss: 0.4773 - val_accuracy: 0.8600\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1084 - accuracy: 0.9737 - val_loss: 0.4876 - val_accuracy: 0.8550\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1047 - accuracy: 0.9725 - val_loss: 0.4951 - val_accuracy: 0.8600\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1171 - accuracy: 0.9688 - val_loss: 0.4845 - val_accuracy: 0.8500\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.0872 - accuracy: 0.9775 - val_loss: 0.4852 - val_accuracy: 0.8550\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.0767 - accuracy: 0.9837 - val_loss: 0.4890 - val_accuracy: 0.8700\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.0913 - accuracy: 0.9725 - val_loss: 0.4822 - val_accuracy: 0.8600\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.0887 - accuracy: 0.9800 - val_loss: 0.4815 - val_accuracy: 0.8700\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.0906 - accuracy: 0.9837 - val_loss: 0.4940 - val_accuracy: 0.8450\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.0872 - accuracy: 0.9800 - val_loss: 0.4819 - val_accuracy: 0.8600\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.0786 - accuracy: 0.9800 - val_loss: 0.4845 - val_accuracy: 0.8800\n",
            "fold 3\n",
            "Model: \"functional_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_2 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_2 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_2 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "xception (Functional)        (None, 10, 10, 2048)      20861480  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               262272    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 21,125,042\n",
            "Trainable params: 263,562\n",
            "Non-trainable params: 20,861,480\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 1s 68ms/step - loss: 2.3762 - accuracy: 0.0650\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 2.2420 - accuracy: 0.1787 - val_loss: 2.0944 - val_accuracy: 0.4050\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 1.9635 - accuracy: 0.4000 - val_loss: 1.8077 - val_accuracy: 0.6350\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 1.6627 - accuracy: 0.5625 - val_loss: 1.5197 - val_accuracy: 0.7000\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 1.3850 - accuracy: 0.6675 - val_loss: 1.2745 - val_accuracy: 0.7600\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 1.1808 - accuracy: 0.7387 - val_loss: 1.0933 - val_accuracy: 0.7850\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 1.0608 - accuracy: 0.7325 - val_loss: 0.9718 - val_accuracy: 0.7900\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.9399 - accuracy: 0.7837 - val_loss: 0.8798 - val_accuracy: 0.8000\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.8495 - accuracy: 0.7975 - val_loss: 0.8128 - val_accuracy: 0.7750\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.7529 - accuracy: 0.8112 - val_loss: 0.7605 - val_accuracy: 0.7900\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.7238 - accuracy: 0.8125 - val_loss: 0.7172 - val_accuracy: 0.7900\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.6501 - accuracy: 0.8475 - val_loss: 0.6730 - val_accuracy: 0.8000\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.6413 - accuracy: 0.8275 - val_loss: 0.6556 - val_accuracy: 0.8000\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.5989 - accuracy: 0.8438 - val_loss: 0.6132 - val_accuracy: 0.8150\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.5689 - accuracy: 0.8375 - val_loss: 0.6129 - val_accuracy: 0.8200\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.5332 - accuracy: 0.8562 - val_loss: 0.5870 - val_accuracy: 0.8200\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.4855 - accuracy: 0.8813 - val_loss: 0.5727 - val_accuracy: 0.8300\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.4764 - accuracy: 0.8650 - val_loss: 0.5749 - val_accuracy: 0.8250\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.4502 - accuracy: 0.8875 - val_loss: 0.5511 - val_accuracy: 0.8300\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.4442 - accuracy: 0.8925 - val_loss: 0.5418 - val_accuracy: 0.8300\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.4458 - accuracy: 0.8775 - val_loss: 0.5362 - val_accuracy: 0.8200\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.4133 - accuracy: 0.8863 - val_loss: 0.5292 - val_accuracy: 0.8400\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.4095 - accuracy: 0.8900 - val_loss: 0.5108 - val_accuracy: 0.8350\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.3605 - accuracy: 0.9112 - val_loss: 0.5149 - val_accuracy: 0.8250\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.3760 - accuracy: 0.9013 - val_loss: 0.5082 - val_accuracy: 0.8400\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.3448 - accuracy: 0.9137 - val_loss: 0.5084 - val_accuracy: 0.8300\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.3330 - accuracy: 0.9038 - val_loss: 0.5065 - val_accuracy: 0.8200\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.3303 - accuracy: 0.9125 - val_loss: 0.5080 - val_accuracy: 0.8250\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.3159 - accuracy: 0.9237 - val_loss: 0.4995 - val_accuracy: 0.8350\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.3097 - accuracy: 0.9187 - val_loss: 0.5004 - val_accuracy: 0.8300\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.2966 - accuracy: 0.9212 - val_loss: 0.4891 - val_accuracy: 0.8400\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.2947 - accuracy: 0.9175 - val_loss: 0.4835 - val_accuracy: 0.8550\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.2960 - accuracy: 0.9175 - val_loss: 0.4859 - val_accuracy: 0.8350\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.2858 - accuracy: 0.9250 - val_loss: 0.4850 - val_accuracy: 0.8400\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.2745 - accuracy: 0.9337 - val_loss: 0.4851 - val_accuracy: 0.8400\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.2802 - accuracy: 0.9262 - val_loss: 0.4711 - val_accuracy: 0.8500\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.2397 - accuracy: 0.9525 - val_loss: 0.4919 - val_accuracy: 0.8350\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.2576 - accuracy: 0.9287 - val_loss: 0.4817 - val_accuracy: 0.8350\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.2460 - accuracy: 0.9375 - val_loss: 0.4806 - val_accuracy: 0.8400\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.2329 - accuracy: 0.9438 - val_loss: 0.4866 - val_accuracy: 0.8550\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.2279 - accuracy: 0.9438 - val_loss: 0.4765 - val_accuracy: 0.8550\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.2254 - accuracy: 0.9488 - val_loss: 0.4979 - val_accuracy: 0.8450\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.2329 - accuracy: 0.9337 - val_loss: 0.4862 - val_accuracy: 0.8500\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1991 - accuracy: 0.9563 - val_loss: 0.4819 - val_accuracy: 0.8550\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.2185 - accuracy: 0.9438 - val_loss: 0.4851 - val_accuracy: 0.8500\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.2033 - accuracy: 0.9425 - val_loss: 0.4731 - val_accuracy: 0.8700\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.2048 - accuracy: 0.9500 - val_loss: 0.4920 - val_accuracy: 0.8500\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.2004 - accuracy: 0.9563 - val_loss: 0.4850 - val_accuracy: 0.8450\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.2090 - accuracy: 0.9488 - val_loss: 0.4882 - val_accuracy: 0.8450\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.1872 - accuracy: 0.9538 - val_loss: 0.4779 - val_accuracy: 0.8500\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.2055 - accuracy: 0.9513 - val_loss: 0.5006 - val_accuracy: 0.8350\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1937 - accuracy: 0.9525 - val_loss: 0.4888 - val_accuracy: 0.8400\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.1883 - accuracy: 0.9500 - val_loss: 0.4911 - val_accuracy: 0.8400\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.1758 - accuracy: 0.9563 - val_loss: 0.4891 - val_accuracy: 0.8400\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1918 - accuracy: 0.9500 - val_loss: 0.4744 - val_accuracy: 0.8650\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1640 - accuracy: 0.9663 - val_loss: 0.4998 - val_accuracy: 0.8250\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1799 - accuracy: 0.9525 - val_loss: 0.4828 - val_accuracy: 0.8350\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1736 - accuracy: 0.9575 - val_loss: 0.4797 - val_accuracy: 0.8500\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.1714 - accuracy: 0.9525 - val_loss: 0.4738 - val_accuracy: 0.8550\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1624 - accuracy: 0.9600 - val_loss: 0.4840 - val_accuracy: 0.8550\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.1518 - accuracy: 0.9650 - val_loss: 0.4802 - val_accuracy: 0.8500\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.1502 - accuracy: 0.9625 - val_loss: 0.4865 - val_accuracy: 0.8550\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.1574 - accuracy: 0.9588 - val_loss: 0.4876 - val_accuracy: 0.8400\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1546 - accuracy: 0.9625 - val_loss: 0.4742 - val_accuracy: 0.8550\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.1402 - accuracy: 0.9675 - val_loss: 0.4760 - val_accuracy: 0.8550\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1521 - accuracy: 0.9638 - val_loss: 0.4760 - val_accuracy: 0.8600\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.1560 - accuracy: 0.9588 - val_loss: 0.4882 - val_accuracy: 0.8450\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1196 - accuracy: 0.9787 - val_loss: 0.4927 - val_accuracy: 0.8300\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1326 - accuracy: 0.9675 - val_loss: 0.4967 - val_accuracy: 0.8350\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.1536 - accuracy: 0.9600 - val_loss: 0.4817 - val_accuracy: 0.8400\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1406 - accuracy: 0.9638 - val_loss: 0.4905 - val_accuracy: 0.8350\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1273 - accuracy: 0.9725 - val_loss: 0.5139 - val_accuracy: 0.8300\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.1222 - accuracy: 0.9737 - val_loss: 0.4990 - val_accuracy: 0.8450\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1447 - accuracy: 0.9625 - val_loss: 0.5016 - val_accuracy: 0.8400\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1332 - accuracy: 0.9725 - val_loss: 0.5009 - val_accuracy: 0.8450\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.1297 - accuracy: 0.9725 - val_loss: 0.5026 - val_accuracy: 0.8350\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.1202 - accuracy: 0.9725 - val_loss: 0.5001 - val_accuracy: 0.8400\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.1135 - accuracy: 0.9688 - val_loss: 0.4949 - val_accuracy: 0.8350\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.1261 - accuracy: 0.9712 - val_loss: 0.5016 - val_accuracy: 0.8300\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1154 - accuracy: 0.9725 - val_loss: 0.4920 - val_accuracy: 0.8350\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.1126 - accuracy: 0.9762 - val_loss: 0.4927 - val_accuracy: 0.8400\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1274 - accuracy: 0.9737 - val_loss: 0.4937 - val_accuracy: 0.8500\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1029 - accuracy: 0.9787 - val_loss: 0.4939 - val_accuracy: 0.8450\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.0980 - accuracy: 0.9825 - val_loss: 0.5047 - val_accuracy: 0.8400\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.0959 - accuracy: 0.9825 - val_loss: 0.4961 - val_accuracy: 0.8550\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.1088 - accuracy: 0.9750 - val_loss: 0.5096 - val_accuracy: 0.8400\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1205 - accuracy: 0.9650 - val_loss: 0.5060 - val_accuracy: 0.8450\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.1110 - accuracy: 0.9750 - val_loss: 0.5053 - val_accuracy: 0.8450\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1136 - accuracy: 0.9762 - val_loss: 0.5044 - val_accuracy: 0.8400\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.0983 - accuracy: 0.9887 - val_loss: 0.5104 - val_accuracy: 0.8450\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.0937 - accuracy: 0.9750 - val_loss: 0.5194 - val_accuracy: 0.8400\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.1086 - accuracy: 0.9737 - val_loss: 0.5060 - val_accuracy: 0.8400\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.0881 - accuracy: 0.9787 - val_loss: 0.5033 - val_accuracy: 0.8550\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.0944 - accuracy: 0.9750 - val_loss: 0.5092 - val_accuracy: 0.8450\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1081 - accuracy: 0.9737 - val_loss: 0.5191 - val_accuracy: 0.8300\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.1045 - accuracy: 0.9775 - val_loss: 0.4999 - val_accuracy: 0.8450\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.0940 - accuracy: 0.9750 - val_loss: 0.5240 - val_accuracy: 0.8350\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.0953 - accuracy: 0.9750 - val_loss: 0.5174 - val_accuracy: 0.8300\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.0863 - accuracy: 0.9812 - val_loss: 0.4980 - val_accuracy: 0.8450\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.0851 - accuracy: 0.9825 - val_loss: 0.5056 - val_accuracy: 0.8600\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.0896 - accuracy: 0.9825 - val_loss: 0.5005 - val_accuracy: 0.8450\n",
            "fold 4\n",
            "Model: \"functional_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_3 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_3 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_3 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "xception (Functional)        (None, 10, 10, 2048)      20861480  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_3 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               262272    \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 21,125,042\n",
            "Trainable params: 263,562\n",
            "Non-trainable params: 20,861,480\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 1s 69ms/step - loss: 2.3951 - accuracy: 0.0700\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 2.2267 - accuracy: 0.1875 - val_loss: 2.0622 - val_accuracy: 0.3550\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 1.9163 - accuracy: 0.4263 - val_loss: 1.7953 - val_accuracy: 0.5750\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 1.6135 - accuracy: 0.5763 - val_loss: 1.4762 - val_accuracy: 0.7300\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 1.3320 - accuracy: 0.6913 - val_loss: 1.2494 - val_accuracy: 0.7650\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 1.1690 - accuracy: 0.7063 - val_loss: 1.0787 - val_accuracy: 0.8050\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 1.0076 - accuracy: 0.7337 - val_loss: 0.9504 - val_accuracy: 0.8300\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.8818 - accuracy: 0.7887 - val_loss: 0.8627 - val_accuracy: 0.8200\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.7811 - accuracy: 0.8288 - val_loss: 0.7868 - val_accuracy: 0.8400\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.7553 - accuracy: 0.8200 - val_loss: 0.7401 - val_accuracy: 0.8400\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.6808 - accuracy: 0.8363 - val_loss: 0.6930 - val_accuracy: 0.8400\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.6489 - accuracy: 0.8300 - val_loss: 0.6896 - val_accuracy: 0.8400\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.5710 - accuracy: 0.8575 - val_loss: 0.6441 - val_accuracy: 0.8400\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.5735 - accuracy: 0.8475 - val_loss: 0.6423 - val_accuracy: 0.8450\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.5456 - accuracy: 0.8600 - val_loss: 0.6103 - val_accuracy: 0.8450\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.5077 - accuracy: 0.8562 - val_loss: 0.5961 - val_accuracy: 0.8500\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.4768 - accuracy: 0.8750 - val_loss: 0.5994 - val_accuracy: 0.8450\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.4443 - accuracy: 0.8900 - val_loss: 0.5689 - val_accuracy: 0.8550\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.4368 - accuracy: 0.8838 - val_loss: 0.5768 - val_accuracy: 0.8500\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.4443 - accuracy: 0.8838 - val_loss: 0.5646 - val_accuracy: 0.8550\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.4055 - accuracy: 0.8900 - val_loss: 0.5445 - val_accuracy: 0.8550\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.4050 - accuracy: 0.8900 - val_loss: 0.5430 - val_accuracy: 0.8450\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.3851 - accuracy: 0.8938 - val_loss: 0.5484 - val_accuracy: 0.8450\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.3688 - accuracy: 0.8988 - val_loss: 0.5433 - val_accuracy: 0.8450\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.3405 - accuracy: 0.9262 - val_loss: 0.5336 - val_accuracy: 0.8450\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.3498 - accuracy: 0.9038 - val_loss: 0.5387 - val_accuracy: 0.8450\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.3227 - accuracy: 0.9250 - val_loss: 0.5288 - val_accuracy: 0.8450\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.3277 - accuracy: 0.9125 - val_loss: 0.5310 - val_accuracy: 0.8450\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.3090 - accuracy: 0.9275 - val_loss: 0.5307 - val_accuracy: 0.8450\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.3058 - accuracy: 0.9162 - val_loss: 0.5359 - val_accuracy: 0.8350\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.2923 - accuracy: 0.9300 - val_loss: 0.5395 - val_accuracy: 0.8450\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.3010 - accuracy: 0.9187 - val_loss: 0.5348 - val_accuracy: 0.8350\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.2681 - accuracy: 0.9337 - val_loss: 0.5315 - val_accuracy: 0.8250\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.2614 - accuracy: 0.9350 - val_loss: 0.5370 - val_accuracy: 0.8300\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.2646 - accuracy: 0.9275 - val_loss: 0.5358 - val_accuracy: 0.8350\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.2490 - accuracy: 0.9400 - val_loss: 0.5313 - val_accuracy: 0.8300\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.2507 - accuracy: 0.9362 - val_loss: 0.5226 - val_accuracy: 0.8450\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.2550 - accuracy: 0.9200 - val_loss: 0.5252 - val_accuracy: 0.8350\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.2501 - accuracy: 0.9287 - val_loss: 0.5393 - val_accuracy: 0.8400\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.2262 - accuracy: 0.9400 - val_loss: 0.5111 - val_accuracy: 0.8300\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.2282 - accuracy: 0.9362 - val_loss: 0.5324 - val_accuracy: 0.8400\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.2040 - accuracy: 0.9463 - val_loss: 0.5171 - val_accuracy: 0.8400\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.2154 - accuracy: 0.9438 - val_loss: 0.5251 - val_accuracy: 0.8350\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.2196 - accuracy: 0.9337 - val_loss: 0.5244 - val_accuracy: 0.8350\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.2046 - accuracy: 0.9488 - val_loss: 0.5374 - val_accuracy: 0.8300\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1964 - accuracy: 0.9563 - val_loss: 0.5236 - val_accuracy: 0.8400\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1760 - accuracy: 0.9663 - val_loss: 0.5345 - val_accuracy: 0.8400\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.1989 - accuracy: 0.9463 - val_loss: 0.5282 - val_accuracy: 0.8300\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1893 - accuracy: 0.9575 - val_loss: 0.5425 - val_accuracy: 0.8300\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1604 - accuracy: 0.9688 - val_loss: 0.5352 - val_accuracy: 0.8350\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1618 - accuracy: 0.9638 - val_loss: 0.5347 - val_accuracy: 0.8250\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.2055 - accuracy: 0.9450 - val_loss: 0.5353 - val_accuracy: 0.8350\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1597 - accuracy: 0.9725 - val_loss: 0.5376 - val_accuracy: 0.8350\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1777 - accuracy: 0.9538 - val_loss: 0.5312 - val_accuracy: 0.8250\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1768 - accuracy: 0.9625 - val_loss: 0.5409 - val_accuracy: 0.8350\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1744 - accuracy: 0.9575 - val_loss: 0.5345 - val_accuracy: 0.8300\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1557 - accuracy: 0.9625 - val_loss: 0.5184 - val_accuracy: 0.8450\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1475 - accuracy: 0.9675 - val_loss: 0.5552 - val_accuracy: 0.8200\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1439 - accuracy: 0.9663 - val_loss: 0.5306 - val_accuracy: 0.8450\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1573 - accuracy: 0.9575 - val_loss: 0.5541 - val_accuracy: 0.8100\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1510 - accuracy: 0.9663 - val_loss: 0.5479 - val_accuracy: 0.8300\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1611 - accuracy: 0.9563 - val_loss: 0.5198 - val_accuracy: 0.8450\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 5s 105ms/step - loss: 0.1483 - accuracy: 0.9625 - val_loss: 0.5192 - val_accuracy: 0.8400\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1407 - accuracy: 0.9688 - val_loss: 0.5338 - val_accuracy: 0.8350\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 5s 105ms/step - loss: 0.1442 - accuracy: 0.9688 - val_loss: 0.5359 - val_accuracy: 0.8400\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1207 - accuracy: 0.9675 - val_loss: 0.5455 - val_accuracy: 0.8200\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1346 - accuracy: 0.9675 - val_loss: 0.5295 - val_accuracy: 0.8250\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1214 - accuracy: 0.9800 - val_loss: 0.5482 - val_accuracy: 0.8200\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1211 - accuracy: 0.9750 - val_loss: 0.5410 - val_accuracy: 0.8400\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1237 - accuracy: 0.9787 - val_loss: 0.5513 - val_accuracy: 0.8250\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1250 - accuracy: 0.9775 - val_loss: 0.5370 - val_accuracy: 0.8400\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1270 - accuracy: 0.9787 - val_loss: 0.5315 - val_accuracy: 0.8350\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1235 - accuracy: 0.9712 - val_loss: 0.5434 - val_accuracy: 0.8450\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1069 - accuracy: 0.9825 - val_loss: 0.5349 - val_accuracy: 0.8350\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1350 - accuracy: 0.9638 - val_loss: 0.5398 - val_accuracy: 0.8250\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1239 - accuracy: 0.9700 - val_loss: 0.5290 - val_accuracy: 0.8350\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1126 - accuracy: 0.9750 - val_loss: 0.5566 - val_accuracy: 0.8350\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1125 - accuracy: 0.9762 - val_loss: 0.5332 - val_accuracy: 0.8300\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1133 - accuracy: 0.9700 - val_loss: 0.5465 - val_accuracy: 0.8300\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.1019 - accuracy: 0.9787 - val_loss: 0.5496 - val_accuracy: 0.8300\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1146 - accuracy: 0.9737 - val_loss: 0.5570 - val_accuracy: 0.8350\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.0984 - accuracy: 0.9750 - val_loss: 0.5367 - val_accuracy: 0.8350\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 5s 102ms/step - loss: 0.1116 - accuracy: 0.9750 - val_loss: 0.5535 - val_accuracy: 0.8250\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1053 - accuracy: 0.9775 - val_loss: 0.5744 - val_accuracy: 0.8200\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.0956 - accuracy: 0.9750 - val_loss: 0.5519 - val_accuracy: 0.8250\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1157 - accuracy: 0.9638 - val_loss: 0.5406 - val_accuracy: 0.8300\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1167 - accuracy: 0.9650 - val_loss: 0.5565 - val_accuracy: 0.8300\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.0917 - accuracy: 0.9812 - val_loss: 0.5547 - val_accuracy: 0.8200\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1017 - accuracy: 0.9725 - val_loss: 0.5457 - val_accuracy: 0.8250\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.0915 - accuracy: 0.9762 - val_loss: 0.5610 - val_accuracy: 0.8300\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1001 - accuracy: 0.9825 - val_loss: 0.5713 - val_accuracy: 0.8300\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.0745 - accuracy: 0.9900 - val_loss: 0.5824 - val_accuracy: 0.8200\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.0963 - accuracy: 0.9762 - val_loss: 0.5490 - val_accuracy: 0.8250\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.0808 - accuracy: 0.9825 - val_loss: 0.5825 - val_accuracy: 0.8250\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1026 - accuracy: 0.9688 - val_loss: 0.5701 - val_accuracy: 0.8250\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.0830 - accuracy: 0.9850 - val_loss: 0.5798 - val_accuracy: 0.8300\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.0710 - accuracy: 0.9862 - val_loss: 0.5749 - val_accuracy: 0.8200\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.0857 - accuracy: 0.9800 - val_loss: 0.5836 - val_accuracy: 0.8200\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.0873 - accuracy: 0.9800 - val_loss: 0.5904 - val_accuracy: 0.8250\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.0891 - accuracy: 0.9750 - val_loss: 0.5880 - val_accuracy: 0.8250\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.0891 - accuracy: 0.9762 - val_loss: 0.5903 - val_accuracy: 0.8150\n",
            "fold 5\n",
            "Model: \"functional_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_4 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_4 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_4 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "xception (Functional)        (None, 10, 10, 2048)      20861480  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_4 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               262272    \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 21,125,042\n",
            "Trainable params: 263,562\n",
            "Non-trainable params: 20,861,480\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 1s 72ms/step - loss: 2.3754 - accuracy: 0.1200\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 2.2422 - accuracy: 0.1762 - val_loss: 2.0553 - val_accuracy: 0.4000\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 1.9230 - accuracy: 0.4175 - val_loss: 1.7749 - val_accuracy: 0.6050\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 5s 105ms/step - loss: 1.6411 - accuracy: 0.5888 - val_loss: 1.4947 - val_accuracy: 0.6650\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 1.3749 - accuracy: 0.6800 - val_loss: 1.2847 - val_accuracy: 0.7400\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 5s 105ms/step - loss: 1.1796 - accuracy: 0.7550 - val_loss: 1.1134 - val_accuracy: 0.7700\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 5s 106ms/step - loss: 1.0185 - accuracy: 0.7563 - val_loss: 0.9942 - val_accuracy: 0.8050\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.9347 - accuracy: 0.7763 - val_loss: 0.9206 - val_accuracy: 0.7900\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 5s 105ms/step - loss: 0.8201 - accuracy: 0.8050 - val_loss: 0.8369 - val_accuracy: 0.8200\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 5s 105ms/step - loss: 0.7359 - accuracy: 0.8350 - val_loss: 0.7856 - val_accuracy: 0.8300\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 5s 105ms/step - loss: 0.7105 - accuracy: 0.8050 - val_loss: 0.7469 - val_accuracy: 0.8400\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 5s 105ms/step - loss: 0.6320 - accuracy: 0.8375 - val_loss: 0.7053 - val_accuracy: 0.8400\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 5s 105ms/step - loss: 0.6014 - accuracy: 0.8388 - val_loss: 0.6826 - val_accuracy: 0.8300\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.5796 - accuracy: 0.8450 - val_loss: 0.6447 - val_accuracy: 0.8400\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.5527 - accuracy: 0.8462 - val_loss: 0.6387 - val_accuracy: 0.8400\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.4845 - accuracy: 0.8587 - val_loss: 0.6179 - val_accuracy: 0.8350\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.4860 - accuracy: 0.8875 - val_loss: 0.5956 - val_accuracy: 0.8350\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.4607 - accuracy: 0.8737 - val_loss: 0.5821 - val_accuracy: 0.8350\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 5s 105ms/step - loss: 0.4547 - accuracy: 0.8800 - val_loss: 0.5730 - val_accuracy: 0.8300\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 5s 105ms/step - loss: 0.4283 - accuracy: 0.8913 - val_loss: 0.5597 - val_accuracy: 0.8350\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.4126 - accuracy: 0.8963 - val_loss: 0.5655 - val_accuracy: 0.8350\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.4191 - accuracy: 0.8712 - val_loss: 0.5486 - val_accuracy: 0.8350\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.3708 - accuracy: 0.9013 - val_loss: 0.5562 - val_accuracy: 0.8250\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.3559 - accuracy: 0.9150 - val_loss: 0.5392 - val_accuracy: 0.8300\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.3662 - accuracy: 0.9050 - val_loss: 0.5277 - val_accuracy: 0.8450\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.3575 - accuracy: 0.9025 - val_loss: 0.5245 - val_accuracy: 0.8500\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.3475 - accuracy: 0.9062 - val_loss: 0.5170 - val_accuracy: 0.8350\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.3098 - accuracy: 0.9162 - val_loss: 0.5102 - val_accuracy: 0.8400\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.3131 - accuracy: 0.9112 - val_loss: 0.5038 - val_accuracy: 0.8350\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.3020 - accuracy: 0.9275 - val_loss: 0.5029 - val_accuracy: 0.8350\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.3027 - accuracy: 0.9212 - val_loss: 0.4909 - val_accuracy: 0.8400\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.2901 - accuracy: 0.9225 - val_loss: 0.5062 - val_accuracy: 0.8500\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.2766 - accuracy: 0.9262 - val_loss: 0.4964 - val_accuracy: 0.8300\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.2807 - accuracy: 0.9275 - val_loss: 0.5008 - val_accuracy: 0.8450\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.2795 - accuracy: 0.9175 - val_loss: 0.4792 - val_accuracy: 0.8400\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.2500 - accuracy: 0.9312 - val_loss: 0.4894 - val_accuracy: 0.8400\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.2430 - accuracy: 0.9425 - val_loss: 0.4778 - val_accuracy: 0.8400\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.2500 - accuracy: 0.9287 - val_loss: 0.4837 - val_accuracy: 0.8350\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.2360 - accuracy: 0.9337 - val_loss: 0.4705 - val_accuracy: 0.8300\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.2240 - accuracy: 0.9413 - val_loss: 0.4748 - val_accuracy: 0.8300\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.2128 - accuracy: 0.9475 - val_loss: 0.4793 - val_accuracy: 0.8300\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.2337 - accuracy: 0.9438 - val_loss: 0.4577 - val_accuracy: 0.8400\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.2261 - accuracy: 0.9400 - val_loss: 0.4808 - val_accuracy: 0.8400\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.2124 - accuracy: 0.9413 - val_loss: 0.4600 - val_accuracy: 0.8300\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.2421 - accuracy: 0.9413 - val_loss: 0.4673 - val_accuracy: 0.8550\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1985 - accuracy: 0.9538 - val_loss: 0.4649 - val_accuracy: 0.8350\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.2072 - accuracy: 0.9400 - val_loss: 0.4678 - val_accuracy: 0.8350\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1794 - accuracy: 0.9613 - val_loss: 0.4731 - val_accuracy: 0.8200\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1872 - accuracy: 0.9538 - val_loss: 0.4834 - val_accuracy: 0.8400\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1817 - accuracy: 0.9525 - val_loss: 0.4598 - val_accuracy: 0.8400\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1905 - accuracy: 0.9513 - val_loss: 0.4649 - val_accuracy: 0.8350\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1749 - accuracy: 0.9625 - val_loss: 0.4679 - val_accuracy: 0.8200\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1926 - accuracy: 0.9450 - val_loss: 0.4686 - val_accuracy: 0.8350\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1759 - accuracy: 0.9613 - val_loss: 0.4589 - val_accuracy: 0.8350\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 5s 105ms/step - loss: 0.1816 - accuracy: 0.9525 - val_loss: 0.4808 - val_accuracy: 0.8400\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1634 - accuracy: 0.9650 - val_loss: 0.4618 - val_accuracy: 0.8450\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1891 - accuracy: 0.9563 - val_loss: 0.4676 - val_accuracy: 0.8500\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1625 - accuracy: 0.9538 - val_loss: 0.4539 - val_accuracy: 0.8400\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1761 - accuracy: 0.9488 - val_loss: 0.4703 - val_accuracy: 0.8500\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1561 - accuracy: 0.9625 - val_loss: 0.4592 - val_accuracy: 0.8400\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1713 - accuracy: 0.9563 - val_loss: 0.4601 - val_accuracy: 0.8400\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 5s 105ms/step - loss: 0.1851 - accuracy: 0.9488 - val_loss: 0.4711 - val_accuracy: 0.8400\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1636 - accuracy: 0.9525 - val_loss: 0.4653 - val_accuracy: 0.8350\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1451 - accuracy: 0.9712 - val_loss: 0.4712 - val_accuracy: 0.8400\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1339 - accuracy: 0.9762 - val_loss: 0.4510 - val_accuracy: 0.8350\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1423 - accuracy: 0.9663 - val_loss: 0.4561 - val_accuracy: 0.8400\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1291 - accuracy: 0.9775 - val_loss: 0.4645 - val_accuracy: 0.8300\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1257 - accuracy: 0.9663 - val_loss: 0.4669 - val_accuracy: 0.8300\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 5s 105ms/step - loss: 0.1407 - accuracy: 0.9588 - val_loss: 0.4700 - val_accuracy: 0.8350\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1450 - accuracy: 0.9600 - val_loss: 0.4653 - val_accuracy: 0.8500\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 5s 105ms/step - loss: 0.1388 - accuracy: 0.9613 - val_loss: 0.4581 - val_accuracy: 0.8300\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 5s 105ms/step - loss: 0.1391 - accuracy: 0.9625 - val_loss: 0.4759 - val_accuracy: 0.8250\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 5s 106ms/step - loss: 0.1240 - accuracy: 0.9700 - val_loss: 0.4676 - val_accuracy: 0.8350\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 5s 105ms/step - loss: 0.1304 - accuracy: 0.9725 - val_loss: 0.4618 - val_accuracy: 0.8400\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 5s 105ms/step - loss: 0.1194 - accuracy: 0.9750 - val_loss: 0.4690 - val_accuracy: 0.8350\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 5s 105ms/step - loss: 0.1084 - accuracy: 0.9688 - val_loss: 0.4615 - val_accuracy: 0.8350\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1279 - accuracy: 0.9725 - val_loss: 0.4768 - val_accuracy: 0.8350\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 5s 105ms/step - loss: 0.1115 - accuracy: 0.9787 - val_loss: 0.4837 - val_accuracy: 0.8300\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1368 - accuracy: 0.9650 - val_loss: 0.4630 - val_accuracy: 0.8400\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.1309 - accuracy: 0.9663 - val_loss: 0.4923 - val_accuracy: 0.8300\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1260 - accuracy: 0.9712 - val_loss: 0.4671 - val_accuracy: 0.8350\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.0958 - accuracy: 0.9800 - val_loss: 0.4838 - val_accuracy: 0.8250\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1124 - accuracy: 0.9737 - val_loss: 0.4685 - val_accuracy: 0.8350\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1243 - accuracy: 0.9725 - val_loss: 0.4629 - val_accuracy: 0.8300\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 5s 105ms/step - loss: 0.0986 - accuracy: 0.9787 - val_loss: 0.4691 - val_accuracy: 0.8350\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.0863 - accuracy: 0.9862 - val_loss: 0.4878 - val_accuracy: 0.8300\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 5s 105ms/step - loss: 0.1033 - accuracy: 0.9787 - val_loss: 0.4534 - val_accuracy: 0.8400\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1047 - accuracy: 0.9750 - val_loss: 0.4743 - val_accuracy: 0.8350\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 5s 105ms/step - loss: 0.1133 - accuracy: 0.9688 - val_loss: 0.4651 - val_accuracy: 0.8300\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.0916 - accuracy: 0.9862 - val_loss: 0.4484 - val_accuracy: 0.8400\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1129 - accuracy: 0.9700 - val_loss: 0.4733 - val_accuracy: 0.8350\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 5s 105ms/step - loss: 0.0864 - accuracy: 0.9787 - val_loss: 0.4850 - val_accuracy: 0.8250\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.0979 - accuracy: 0.9750 - val_loss: 0.4643 - val_accuracy: 0.8350\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 5s 105ms/step - loss: 0.1089 - accuracy: 0.9737 - val_loss: 0.4706 - val_accuracy: 0.8300\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 5s 105ms/step - loss: 0.0990 - accuracy: 0.9787 - val_loss: 0.4694 - val_accuracy: 0.8350\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.1009 - accuracy: 0.9737 - val_loss: 0.4625 - val_accuracy: 0.8400\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.0908 - accuracy: 0.9775 - val_loss: 0.4714 - val_accuracy: 0.8400\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 5s 103ms/step - loss: 0.0887 - accuracy: 0.9825 - val_loss: 0.4553 - val_accuracy: 0.8400\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.0915 - accuracy: 0.9775 - val_loss: 0.4694 - val_accuracy: 0.8300\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.0887 - accuracy: 0.9775 - val_loss: 0.4682 - val_accuracy: 0.8300\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.0917 - accuracy: 0.9800 - val_loss: 0.4748 - val_accuracy: 0.8300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2E72C3O30fv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a25d41c-5cbf-4610-ef04-fc3caacf9444"
      },
      "source": [
        "# cross-validated accuracy for pre-trained model before training\n",
        "print(\"Base model accuracy:\", np.mean(base_model_acc_list))\n",
        "# cross-validated accuracy after training\n",
        "print(\"Final accuracy:\", np.mean(final_acc_list))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Base model accuracy: 0.08999999910593033\n",
            "Final accuracy: 0.8650000095367432\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dtn-03T_ZaRX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "caf6e9ec-9174-462d-c9c1-fed4091ab763"
      },
      "source": [
        "# plot graph of cross-validated accuracies\n",
        "acc = np.mean(history_map['accuracy'], axis=0)\n",
        "val_acc = np.mean(history_map['val_accuracy'], axis=0)\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftCYvBkKZmGa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "5f0ba9a6-03ea-49f1-ca58-c9320775a877"
      },
      "source": [
        "# plot graph of cross-validated losses\n",
        "loss = np.mean(history_map['loss'], axis=0)\n",
        "val_loss = np.mean(history_map['val_loss'], axis=0)\n",
        "\n",
        "plt.figure(2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "#plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}