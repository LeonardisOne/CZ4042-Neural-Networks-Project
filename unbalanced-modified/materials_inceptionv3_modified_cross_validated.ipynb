{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "materials-modified-cross-validated.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP8JUCgkwZiH8gI38Mqs41Y"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERKJLyg2z-Uq"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fIbsvD01EQm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e06802b5-edc9-42ee-8236-031ecd1257e8"
      },
      "source": [
        "import pathlib\n",
        "\n",
        "# set path to FMD image directory\n",
        "data_dir = pathlib.Path(\"image\")\n",
        "\n",
        "# total no. of images in FMD dataset\n",
        "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
        "print(image_count)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL3rBqoe3sK5"
      },
      "source": [
        "# set batch size for training\n",
        "batch_size = 16\n",
        "\n",
        "# set dimensions to change images into for training\n",
        "# 299x299 images is required for pre-trained models like Inception V3\n",
        "img_height = 299\n",
        "img_width = 299"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap_FpvbEqWrS"
      },
      "source": [
        "# create dataset from the image directory\n",
        "list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*.jpg'), shuffle=False)\n",
        "# shuffle the 1,000 images with the random seed value of 123 before training\n",
        "list_ds = list_ds.shuffle(image_count, seed=123, reshuffle_each_iteration=False)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFKuhNRxqxcB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "868c62e0-393c-449b-fbf4-6b3356c03023"
      },
      "source": [
        "# get class names from the folder names in data_dir\n",
        "class_names = np.array(sorted([item.name for item in data_dir.glob('*') if item.name != \"LICENSE.txt\"]))\n",
        "print(class_names)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['fabric' 'foliage' 'glass' 'leather' 'metal' 'paper' 'plastic' 'stone'\n",
            " 'water' 'wood']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACILObxurdXN"
      },
      "source": [
        "# split dataset into 5 equal sized parts for 5-fold cross validation\n",
        "A = list_ds.shard(num_shards=5, index=0)\n",
        "B = list_ds.shard(num_shards=5, index=1)\n",
        "C = list_ds.shard(num_shards=5, index=2)\n",
        "D = list_ds.shard(num_shards=5, index=3)\n",
        "E = list_ds.shard(num_shards=5, index=4)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9oYdHkhrwdz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d766bd8d-90cb-4ed2-dab2-aee18777427b"
      },
      "source": [
        "# check no. of samples in each partition is the same\n",
        "print(A.cardinality().numpy())\n",
        "print(B.cardinality().numpy())\n",
        "print(C.cardinality().numpy())\n",
        "print(D.cardinality().numpy())\n",
        "print(E.cardinality().numpy())"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BdD3FMHtGRV"
      },
      "source": [
        "def get_label(file_path):\n",
        "  # convert the path to a list of path components\n",
        "  parts = tf.strings.split(file_path, os.path.sep)\n",
        "  # The second to last is the class-directory\n",
        "  one_hot = parts[-2] == class_names\n",
        "  # Integer encode the label\n",
        "  return tf.argmax(one_hot)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfWduaL4tKDV"
      },
      "source": [
        "def decode_img(img):\n",
        "  # convert the compressed string to a 3D uint8 tensor\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\n",
        "  # resize the image to the desired size\n",
        "  return tf.image.resize(img, [img_height, img_width])"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bGGe0KltMTC"
      },
      "source": [
        "def process_path(file_path):\n",
        "  label = get_label(file_path)\n",
        "  # load the raw data from the file as a string\n",
        "  img = tf.io.read_file(file_path)\n",
        "  img = decode_img(img)\n",
        "  return img, label"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcyZLwRxt1dy"
      },
      "source": [
        "# prompt the tf.data runtime to tune the number of elements to prefetch dynamically at runtime\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkBqAQlHtblh"
      },
      "source": [
        "# use the path of image to load the image into each partition of the dataset\n",
        "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
        "A = A.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "B = B.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "C = C.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "D = D.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "E = E.map(process_path, num_parallel_calls=AUTOTUNE)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc9pmaQ5t9H5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab3cd486-ac7a-413b-bb86-4f27827ccd32"
      },
      "source": [
        "for image, label in A.take(1):\n",
        "  print(\"Image shape: \", image.numpy().shape)\n",
        "  print(\"Label: \", label.numpy())"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image shape:  (299, 299, 3)\n",
            "Label:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_T2KNdGub1X"
      },
      "source": [
        "# shuffle, batch, and prefetch the dataset\n",
        "def configure_for_performance(ds):\n",
        "  ds = ds.cache()\n",
        "  ds = ds.shuffle(buffer_size=1000)\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
        "  return ds"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWcojoVRuok5"
      },
      "source": [
        "# map the dataset partitions to integers for building training/test sets during cross-validation\n",
        "ds_fold_dict = {0:A, 1:B, 2:C, 3:D, 4:E}"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx3xKoMZU9Yv"
      },
      "source": [
        "# normalise the input values to the pre-trained model's required range of values\n",
        "preprocess_input = keras.applications.inception_v3.preprocess_input"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVOP5fIPwEx8"
      },
      "source": [
        "# get pre-trained model\n",
        "base_model = keras.applications.InceptionV3(include_top=False, input_shape=(img_height, img_width, 3))"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS2kAceGVW0e"
      },
      "source": [
        "# don't train base model weights\n",
        "base_model.trainable = False"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGkReMX60ScJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40d07a20-dd27-4e79-c704-012beedac63b"
      },
      "source": [
        "base_model.summary()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"inception_v3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 299, 299, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 149, 149, 32) 864         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_94 (BatchNo (None, 149, 149, 32) 96          conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_94 (Activation)      (None, 149, 149, 32) 0           batch_normalization_94[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 147, 147, 32) 9216        activation_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_95 (BatchNo (None, 147, 147, 32) 96          conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_95 (Activation)      (None, 147, 147, 32) 0           batch_normalization_95[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 147, 147, 64) 18432       activation_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_96 (BatchNo (None, 147, 147, 64) 192         conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_96 (Activation)      (None, 147, 147, 64) 0           batch_normalization_96[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_96[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, 73, 73, 80)   5120        max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_97 (BatchNo (None, 73, 73, 80)   240         conv2d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_97 (Activation)      (None, 73, 73, 80)   0           batch_normalization_97[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 71, 71, 192)  138240      activation_97[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_98 (BatchNo (None, 71, 71, 192)  576         conv2d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_98 (Activation)      (None, 71, 71, 192)  0           batch_normalization_98[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_98[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_102 (Conv2D)             (None, 35, 35, 64)   12288       max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_102 (BatchN (None, 35, 35, 64)   192         conv2d_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_102 (Activation)     (None, 35, 35, 64)   0           batch_normalization_102[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_100 (Conv2D)             (None, 35, 35, 48)   9216        max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_103 (Conv2D)             (None, 35, 35, 96)   55296       activation_102[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_100 (BatchN (None, 35, 35, 48)   144         conv2d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_103 (BatchN (None, 35, 35, 96)   288         conv2d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_100 (Activation)     (None, 35, 35, 48)   0           batch_normalization_100[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_103 (Activation)     (None, 35, 35, 96)   0           batch_normalization_103[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_9 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, 35, 35, 64)   12288       max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_101 (Conv2D)             (None, 35, 35, 64)   76800       activation_100[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_104 (Conv2D)             (None, 35, 35, 96)   82944       activation_103[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_105 (Conv2D)             (None, 35, 35, 32)   6144        average_pooling2d_9[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_99 (BatchNo (None, 35, 35, 64)   192         conv2d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_101 (BatchN (None, 35, 35, 64)   192         conv2d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_104 (BatchN (None, 35, 35, 96)   288         conv2d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_105 (BatchN (None, 35, 35, 32)   96          conv2d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_99 (Activation)      (None, 35, 35, 64)   0           batch_normalization_99[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_101 (Activation)     (None, 35, 35, 64)   0           batch_normalization_101[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_104 (Activation)     (None, 35, 35, 96)   0           batch_normalization_104[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_105 (Activation)     (None, 35, 35, 32)   0           batch_normalization_105[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_99[0][0]              \n",
            "                                                                 activation_101[0][0]             \n",
            "                                                                 activation_104[0][0]             \n",
            "                                                                 activation_105[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_109 (Conv2D)             (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_109 (BatchN (None, 35, 35, 64)   192         conv2d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_109 (Activation)     (None, 35, 35, 64)   0           batch_normalization_109[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_107 (Conv2D)             (None, 35, 35, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_110 (Conv2D)             (None, 35, 35, 96)   55296       activation_109[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_107 (BatchN (None, 35, 35, 48)   144         conv2d_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_110 (BatchN (None, 35, 35, 96)   288         conv2d_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_107 (Activation)     (None, 35, 35, 48)   0           batch_normalization_107[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_110 (Activation)     (None, 35, 35, 96)   0           batch_normalization_110[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_10 (AveragePo (None, 35, 35, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_106 (Conv2D)             (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_108 (Conv2D)             (None, 35, 35, 64)   76800       activation_107[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_111 (Conv2D)             (None, 35, 35, 96)   82944       activation_110[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_112 (Conv2D)             (None, 35, 35, 64)   16384       average_pooling2d_10[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_106 (BatchN (None, 35, 35, 64)   192         conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_108 (BatchN (None, 35, 35, 64)   192         conv2d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_111 (BatchN (None, 35, 35, 96)   288         conv2d_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_112 (BatchN (None, 35, 35, 64)   192         conv2d_112[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_106 (Activation)     (None, 35, 35, 64)   0           batch_normalization_106[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_108 (Activation)     (None, 35, 35, 64)   0           batch_normalization_108[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_111 (Activation)     (None, 35, 35, 96)   0           batch_normalization_111[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_112 (Activation)     (None, 35, 35, 64)   0           batch_normalization_112[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_106[0][0]             \n",
            "                                                                 activation_108[0][0]             \n",
            "                                                                 activation_111[0][0]             \n",
            "                                                                 activation_112[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_116 (Conv2D)             (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_116 (BatchN (None, 35, 35, 64)   192         conv2d_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_116 (Activation)     (None, 35, 35, 64)   0           batch_normalization_116[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_114 (Conv2D)             (None, 35, 35, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_117 (Conv2D)             (None, 35, 35, 96)   55296       activation_116[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_114 (BatchN (None, 35, 35, 48)   144         conv2d_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_117 (BatchN (None, 35, 35, 96)   288         conv2d_117[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_114 (Activation)     (None, 35, 35, 48)   0           batch_normalization_114[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_117 (Activation)     (None, 35, 35, 96)   0           batch_normalization_117[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_11 (AveragePo (None, 35, 35, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_113 (Conv2D)             (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_115 (Conv2D)             (None, 35, 35, 64)   76800       activation_114[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_118 (Conv2D)             (None, 35, 35, 96)   82944       activation_117[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_119 (Conv2D)             (None, 35, 35, 64)   18432       average_pooling2d_11[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_113 (BatchN (None, 35, 35, 64)   192         conv2d_113[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_115 (BatchN (None, 35, 35, 64)   192         conv2d_115[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_118 (BatchN (None, 35, 35, 96)   288         conv2d_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_119 (BatchN (None, 35, 35, 64)   192         conv2d_119[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_113 (Activation)     (None, 35, 35, 64)   0           batch_normalization_113[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_115 (Activation)     (None, 35, 35, 64)   0           batch_normalization_115[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_118 (Activation)     (None, 35, 35, 96)   0           batch_normalization_118[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_119 (Activation)     (None, 35, 35, 64)   0           batch_normalization_119[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_113[0][0]             \n",
            "                                                                 activation_115[0][0]             \n",
            "                                                                 activation_118[0][0]             \n",
            "                                                                 activation_119[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_121 (Conv2D)             (None, 35, 35, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_121 (BatchN (None, 35, 35, 64)   192         conv2d_121[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_121 (Activation)     (None, 35, 35, 64)   0           batch_normalization_121[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_122 (Conv2D)             (None, 35, 35, 96)   55296       activation_121[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_122 (BatchN (None, 35, 35, 96)   288         conv2d_122[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_122 (Activation)     (None, 35, 35, 96)   0           batch_normalization_122[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_120 (Conv2D)             (None, 17, 17, 384)  995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_123 (Conv2D)             (None, 17, 17, 96)   82944       activation_122[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_120 (BatchN (None, 17, 17, 384)  1152        conv2d_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_123 (BatchN (None, 17, 17, 96)   288         conv2d_123[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_120 (Activation)     (None, 17, 17, 384)  0           batch_normalization_120[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_123 (Activation)     (None, 17, 17, 96)   0           batch_normalization_123[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_120[0][0]             \n",
            "                                                                 activation_123[0][0]             \n",
            "                                                                 max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_128 (Conv2D)             (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_128 (BatchN (None, 17, 17, 128)  384         conv2d_128[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_128 (Activation)     (None, 17, 17, 128)  0           batch_normalization_128[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_129 (Conv2D)             (None, 17, 17, 128)  114688      activation_128[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_129 (BatchN (None, 17, 17, 128)  384         conv2d_129[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_129 (Activation)     (None, 17, 17, 128)  0           batch_normalization_129[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_125 (Conv2D)             (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_130 (Conv2D)             (None, 17, 17, 128)  114688      activation_129[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_125 (BatchN (None, 17, 17, 128)  384         conv2d_125[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_130 (BatchN (None, 17, 17, 128)  384         conv2d_130[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_125 (Activation)     (None, 17, 17, 128)  0           batch_normalization_125[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_130 (Activation)     (None, 17, 17, 128)  0           batch_normalization_130[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_126 (Conv2D)             (None, 17, 17, 128)  114688      activation_125[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_131 (Conv2D)             (None, 17, 17, 128)  114688      activation_130[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_126 (BatchN (None, 17, 17, 128)  384         conv2d_126[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_131 (BatchN (None, 17, 17, 128)  384         conv2d_131[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_126 (Activation)     (None, 17, 17, 128)  0           batch_normalization_126[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_131 (Activation)     (None, 17, 17, 128)  0           batch_normalization_131[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_12 (AveragePo (None, 17, 17, 768)  0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_124 (Conv2D)             (None, 17, 17, 192)  147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_127 (Conv2D)             (None, 17, 17, 192)  172032      activation_126[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_132 (Conv2D)             (None, 17, 17, 192)  172032      activation_131[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_133 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_12[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_124 (BatchN (None, 17, 17, 192)  576         conv2d_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_127 (BatchN (None, 17, 17, 192)  576         conv2d_127[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_132 (BatchN (None, 17, 17, 192)  576         conv2d_132[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_133 (BatchN (None, 17, 17, 192)  576         conv2d_133[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_124 (Activation)     (None, 17, 17, 192)  0           batch_normalization_124[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_127 (Activation)     (None, 17, 17, 192)  0           batch_normalization_127[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_132 (Activation)     (None, 17, 17, 192)  0           batch_normalization_132[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_133 (Activation)     (None, 17, 17, 192)  0           batch_normalization_133[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_124[0][0]             \n",
            "                                                                 activation_127[0][0]             \n",
            "                                                                 activation_132[0][0]             \n",
            "                                                                 activation_133[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_138 (Conv2D)             (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_138 (BatchN (None, 17, 17, 160)  480         conv2d_138[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_138 (Activation)     (None, 17, 17, 160)  0           batch_normalization_138[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_139 (Conv2D)             (None, 17, 17, 160)  179200      activation_138[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_139 (BatchN (None, 17, 17, 160)  480         conv2d_139[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_139 (Activation)     (None, 17, 17, 160)  0           batch_normalization_139[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_135 (Conv2D)             (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_140 (Conv2D)             (None, 17, 17, 160)  179200      activation_139[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_135 (BatchN (None, 17, 17, 160)  480         conv2d_135[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_140 (BatchN (None, 17, 17, 160)  480         conv2d_140[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_135 (Activation)     (None, 17, 17, 160)  0           batch_normalization_135[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_140 (Activation)     (None, 17, 17, 160)  0           batch_normalization_140[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_136 (Conv2D)             (None, 17, 17, 160)  179200      activation_135[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_141 (Conv2D)             (None, 17, 17, 160)  179200      activation_140[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_136 (BatchN (None, 17, 17, 160)  480         conv2d_136[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_141 (BatchN (None, 17, 17, 160)  480         conv2d_141[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_136 (Activation)     (None, 17, 17, 160)  0           batch_normalization_136[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_141 (Activation)     (None, 17, 17, 160)  0           batch_normalization_141[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_13 (AveragePo (None, 17, 17, 768)  0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_134 (Conv2D)             (None, 17, 17, 192)  147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_137 (Conv2D)             (None, 17, 17, 192)  215040      activation_136[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_142 (Conv2D)             (None, 17, 17, 192)  215040      activation_141[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_143 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_13[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_134 (BatchN (None, 17, 17, 192)  576         conv2d_134[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_137 (BatchN (None, 17, 17, 192)  576         conv2d_137[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_142 (BatchN (None, 17, 17, 192)  576         conv2d_142[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_143 (BatchN (None, 17, 17, 192)  576         conv2d_143[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_134 (Activation)     (None, 17, 17, 192)  0           batch_normalization_134[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_137 (Activation)     (None, 17, 17, 192)  0           batch_normalization_137[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_142 (Activation)     (None, 17, 17, 192)  0           batch_normalization_142[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_143 (Activation)     (None, 17, 17, 192)  0           batch_normalization_143[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_134[0][0]             \n",
            "                                                                 activation_137[0][0]             \n",
            "                                                                 activation_142[0][0]             \n",
            "                                                                 activation_143[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_148 (Conv2D)             (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_148 (BatchN (None, 17, 17, 160)  480         conv2d_148[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_148 (Activation)     (None, 17, 17, 160)  0           batch_normalization_148[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_149 (Conv2D)             (None, 17, 17, 160)  179200      activation_148[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_149 (BatchN (None, 17, 17, 160)  480         conv2d_149[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_149 (Activation)     (None, 17, 17, 160)  0           batch_normalization_149[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_145 (Conv2D)             (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_150 (Conv2D)             (None, 17, 17, 160)  179200      activation_149[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_145 (BatchN (None, 17, 17, 160)  480         conv2d_145[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_150 (BatchN (None, 17, 17, 160)  480         conv2d_150[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_145 (Activation)     (None, 17, 17, 160)  0           batch_normalization_145[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_150 (Activation)     (None, 17, 17, 160)  0           batch_normalization_150[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_146 (Conv2D)             (None, 17, 17, 160)  179200      activation_145[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_151 (Conv2D)             (None, 17, 17, 160)  179200      activation_150[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_146 (BatchN (None, 17, 17, 160)  480         conv2d_146[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_151 (BatchN (None, 17, 17, 160)  480         conv2d_151[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_146 (Activation)     (None, 17, 17, 160)  0           batch_normalization_146[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_151 (Activation)     (None, 17, 17, 160)  0           batch_normalization_151[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_14 (AveragePo (None, 17, 17, 768)  0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_144 (Conv2D)             (None, 17, 17, 192)  147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_147 (Conv2D)             (None, 17, 17, 192)  215040      activation_146[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_152 (Conv2D)             (None, 17, 17, 192)  215040      activation_151[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_153 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_14[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_144 (BatchN (None, 17, 17, 192)  576         conv2d_144[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_147 (BatchN (None, 17, 17, 192)  576         conv2d_147[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_152 (BatchN (None, 17, 17, 192)  576         conv2d_152[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_153 (BatchN (None, 17, 17, 192)  576         conv2d_153[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_144 (Activation)     (None, 17, 17, 192)  0           batch_normalization_144[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_147 (Activation)     (None, 17, 17, 192)  0           batch_normalization_147[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_152 (Activation)     (None, 17, 17, 192)  0           batch_normalization_152[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_153 (Activation)     (None, 17, 17, 192)  0           batch_normalization_153[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_144[0][0]             \n",
            "                                                                 activation_147[0][0]             \n",
            "                                                                 activation_152[0][0]             \n",
            "                                                                 activation_153[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_158 (Conv2D)             (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_158 (BatchN (None, 17, 17, 192)  576         conv2d_158[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_158 (Activation)     (None, 17, 17, 192)  0           batch_normalization_158[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_159 (Conv2D)             (None, 17, 17, 192)  258048      activation_158[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_159 (BatchN (None, 17, 17, 192)  576         conv2d_159[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_159 (Activation)     (None, 17, 17, 192)  0           batch_normalization_159[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_155 (Conv2D)             (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_160 (Conv2D)             (None, 17, 17, 192)  258048      activation_159[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_155 (BatchN (None, 17, 17, 192)  576         conv2d_155[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_160 (BatchN (None, 17, 17, 192)  576         conv2d_160[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_155 (Activation)     (None, 17, 17, 192)  0           batch_normalization_155[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_160 (Activation)     (None, 17, 17, 192)  0           batch_normalization_160[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_156 (Conv2D)             (None, 17, 17, 192)  258048      activation_155[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_161 (Conv2D)             (None, 17, 17, 192)  258048      activation_160[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_156 (BatchN (None, 17, 17, 192)  576         conv2d_156[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_161 (BatchN (None, 17, 17, 192)  576         conv2d_161[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_156 (Activation)     (None, 17, 17, 192)  0           batch_normalization_156[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_161 (Activation)     (None, 17, 17, 192)  0           batch_normalization_161[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_15 (AveragePo (None, 17, 17, 768)  0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_154 (Conv2D)             (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_157 (Conv2D)             (None, 17, 17, 192)  258048      activation_156[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_162 (Conv2D)             (None, 17, 17, 192)  258048      activation_161[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_163 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_15[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_154 (BatchN (None, 17, 17, 192)  576         conv2d_154[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_157 (BatchN (None, 17, 17, 192)  576         conv2d_157[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_162 (BatchN (None, 17, 17, 192)  576         conv2d_162[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_163 (BatchN (None, 17, 17, 192)  576         conv2d_163[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_154 (Activation)     (None, 17, 17, 192)  0           batch_normalization_154[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_157 (Activation)     (None, 17, 17, 192)  0           batch_normalization_157[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_162 (Activation)     (None, 17, 17, 192)  0           batch_normalization_162[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_163 (Activation)     (None, 17, 17, 192)  0           batch_normalization_163[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_154[0][0]             \n",
            "                                                                 activation_157[0][0]             \n",
            "                                                                 activation_162[0][0]             \n",
            "                                                                 activation_163[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_166 (Conv2D)             (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_166 (BatchN (None, 17, 17, 192)  576         conv2d_166[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_166 (Activation)     (None, 17, 17, 192)  0           batch_normalization_166[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_167 (Conv2D)             (None, 17, 17, 192)  258048      activation_166[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_167 (BatchN (None, 17, 17, 192)  576         conv2d_167[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_167 (Activation)     (None, 17, 17, 192)  0           batch_normalization_167[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_164 (Conv2D)             (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_168 (Conv2D)             (None, 17, 17, 192)  258048      activation_167[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_164 (BatchN (None, 17, 17, 192)  576         conv2d_164[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_168 (BatchN (None, 17, 17, 192)  576         conv2d_168[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_164 (Activation)     (None, 17, 17, 192)  0           batch_normalization_164[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_168 (Activation)     (None, 17, 17, 192)  0           batch_normalization_168[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_165 (Conv2D)             (None, 8, 8, 320)    552960      activation_164[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_169 (Conv2D)             (None, 8, 8, 192)    331776      activation_168[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_165 (BatchN (None, 8, 8, 320)    960         conv2d_165[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_169 (BatchN (None, 8, 8, 192)    576         conv2d_169[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_165 (Activation)     (None, 8, 8, 320)    0           batch_normalization_165[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_169 (Activation)     (None, 8, 8, 192)    0           batch_normalization_169[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_165[0][0]             \n",
            "                                                                 activation_169[0][0]             \n",
            "                                                                 max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_174 (Conv2D)             (None, 8, 8, 448)    573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_174 (BatchN (None, 8, 8, 448)    1344        conv2d_174[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_174 (Activation)     (None, 8, 8, 448)    0           batch_normalization_174[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_171 (Conv2D)             (None, 8, 8, 384)    491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_175 (Conv2D)             (None, 8, 8, 384)    1548288     activation_174[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_171 (BatchN (None, 8, 8, 384)    1152        conv2d_171[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_175 (BatchN (None, 8, 8, 384)    1152        conv2d_175[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_171 (Activation)     (None, 8, 8, 384)    0           batch_normalization_171[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_175 (Activation)     (None, 8, 8, 384)    0           batch_normalization_175[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_172 (Conv2D)             (None, 8, 8, 384)    442368      activation_171[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_173 (Conv2D)             (None, 8, 8, 384)    442368      activation_171[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_176 (Conv2D)             (None, 8, 8, 384)    442368      activation_175[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_177 (Conv2D)             (None, 8, 8, 384)    442368      activation_175[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_16 (AveragePo (None, 8, 8, 1280)   0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_170 (Conv2D)             (None, 8, 8, 320)    409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_172 (BatchN (None, 8, 8, 384)    1152        conv2d_172[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_173 (BatchN (None, 8, 8, 384)    1152        conv2d_173[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_176 (BatchN (None, 8, 8, 384)    1152        conv2d_176[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_177 (BatchN (None, 8, 8, 384)    1152        conv2d_177[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_178 (Conv2D)             (None, 8, 8, 192)    245760      average_pooling2d_16[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_170 (BatchN (None, 8, 8, 320)    960         conv2d_170[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_172 (Activation)     (None, 8, 8, 384)    0           batch_normalization_172[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_173 (Activation)     (None, 8, 8, 384)    0           batch_normalization_173[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_176 (Activation)     (None, 8, 8, 384)    0           batch_normalization_176[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_177 (Activation)     (None, 8, 8, 384)    0           batch_normalization_177[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_178 (BatchN (None, 8, 8, 192)    576         conv2d_178[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_170 (Activation)     (None, 8, 8, 320)    0           batch_normalization_170[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_172[0][0]             \n",
            "                                                                 activation_173[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_176[0][0]             \n",
            "                                                                 activation_177[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_178 (Activation)     (None, 8, 8, 192)    0           batch_normalization_178[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_170[0][0]             \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate_2[0][0]              \n",
            "                                                                 activation_178[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_183 (Conv2D)             (None, 8, 8, 448)    917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_183 (BatchN (None, 8, 8, 448)    1344        conv2d_183[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_183 (Activation)     (None, 8, 8, 448)    0           batch_normalization_183[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_180 (Conv2D)             (None, 8, 8, 384)    786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_184 (Conv2D)             (None, 8, 8, 384)    1548288     activation_183[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_180 (BatchN (None, 8, 8, 384)    1152        conv2d_180[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_184 (BatchN (None, 8, 8, 384)    1152        conv2d_184[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_180 (Activation)     (None, 8, 8, 384)    0           batch_normalization_180[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_184 (Activation)     (None, 8, 8, 384)    0           batch_normalization_184[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_181 (Conv2D)             (None, 8, 8, 384)    442368      activation_180[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_182 (Conv2D)             (None, 8, 8, 384)    442368      activation_180[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_185 (Conv2D)             (None, 8, 8, 384)    442368      activation_184[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_186 (Conv2D)             (None, 8, 8, 384)    442368      activation_184[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_17 (AveragePo (None, 8, 8, 2048)   0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_179 (Conv2D)             (None, 8, 8, 320)    655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_181 (BatchN (None, 8, 8, 384)    1152        conv2d_181[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_182 (BatchN (None, 8, 8, 384)    1152        conv2d_182[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_185 (BatchN (None, 8, 8, 384)    1152        conv2d_185[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_186 (BatchN (None, 8, 8, 384)    1152        conv2d_186[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_187 (Conv2D)             (None, 8, 8, 192)    393216      average_pooling2d_17[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_179 (BatchN (None, 8, 8, 320)    960         conv2d_179[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_181 (Activation)     (None, 8, 8, 384)    0           batch_normalization_181[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_182 (Activation)     (None, 8, 8, 384)    0           batch_normalization_182[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_185 (Activation)     (None, 8, 8, 384)    0           batch_normalization_185[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_186 (Activation)     (None, 8, 8, 384)    0           batch_normalization_186[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_187 (BatchN (None, 8, 8, 192)    576         conv2d_187[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_179 (Activation)     (None, 8, 8, 320)    0           batch_normalization_179[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_181[0][0]             \n",
            "                                                                 activation_182[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 8, 8, 768)    0           activation_185[0][0]             \n",
            "                                                                 activation_186[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_187 (Activation)     (None, 8, 8, 192)    0           batch_normalization_187[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_179[0][0]             \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_3[0][0]              \n",
            "                                                                 activation_187[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 21,802,784\n",
            "Trainable params: 0\n",
            "Non-trainable params: 21,802,784\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzhaWb2aX2if"
      },
      "source": [
        "# set a low learning rate to avoid overfitting too quickly\n",
        "base_learning_rate = 0.0001\n",
        "\n",
        "# create the model to train using the pre-trained model as base model\n",
        "def create_model():\n",
        "  # generate additional training data from input training data by augmenting them using random flip, rotation & zoom\n",
        "  data_augmentation = keras.Sequential(\n",
        "    [\n",
        "      layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n",
        "                                                  input_shape=(img_height, \n",
        "                                                                img_width,\n",
        "                                                                3)),\n",
        "      layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "      layers.experimental.preprocessing.RandomZoom(0.1),\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  # average over the spatial locations to convert the features to a single vector per image\n",
        "  global_average_layer = keras.layers.GlobalAveragePooling2D()\n",
        "  # increase network depth by adding additional fully connected layer of size 128 with ReLU activation\n",
        "  fully_connected_layer = keras.layers.Dense(128, activation='relu')\n",
        "  # convert these features into a single prediction per image\n",
        "  prediction_layer = keras.layers.Dense(10)\n",
        "\n",
        "  # Build a model by chaining together the layers using the Keras Functional API.\n",
        "  inputs = keras.Input(shape=(img_height, img_width, 3))\n",
        "  x = data_augmentation(inputs)\n",
        "  x = preprocess_input(x)\n",
        "  x = base_model(x, training=False) # use training=False as the base model contains a BatchNormalization layer\n",
        "  x = global_average_layer(x)\n",
        "  x = keras.layers.Dropout(0.2)(x) # add dropout to fully connected layer to reduce overfitting\n",
        "  x = fully_connected_layer(x)\n",
        "  x = keras.layers.Dropout(0.2)(x) # add dropout to fully connected layer to reduce overfitting\n",
        "  outputs = prediction_layer(x)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  optimizer = keras.optimizers.Adam(lr=base_learning_rate)\n",
        "  loss= keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  # compile model with Adam optimizer with specified learning rate\n",
        "  model.compile(optimizer=optimizer,\n",
        "                loss=loss,\n",
        "                metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-5TEZRgY2l_"
      },
      "source": [
        "no_epochs = 100"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya698zRbu7Ep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc2f6e95-d645-4ff9-886a-038ad7499dce"
      },
      "source": [
        "# store results to plot graphs and get cross-validated accuracies\n",
        "history_map = {}\n",
        "base_model_acc_list = []\n",
        "final_acc_list = []\n",
        "\n",
        "# do 5-fold cross-validation\n",
        "for i in range(5):\n",
        "  print('fold', i + 1)\n",
        "  temp_dict = ds_fold_dict.copy()\n",
        "  # get test set for this iteration\n",
        "  current_val_ds = temp_dict[i]\n",
        "\n",
        "  # get training set for this iteration from remaining data samples\n",
        "  del temp_dict[i]\n",
        "  current_train_ds = None\n",
        "  for ds_shard in temp_dict.values():\n",
        "    if current_train_ds is None:\n",
        "      current_train_ds = ds_shard\n",
        "    else:\n",
        "      current_train_ds = current_train_ds.concatenate(ds_shard)\n",
        "  \n",
        "  # configure both training and test sets to improve performance\n",
        "  current_train_ds = configure_for_performance(current_train_ds)\n",
        "  current_val_ds = configure_for_performance(current_val_ds)\n",
        "\n",
        "  # create a new model\n",
        "  model = create_model()\n",
        "  # get initial test accuracy\n",
        "  base_model_acc_list.append(model.evaluate(current_val_ds)[1])\n",
        "  # train for specified epochs\n",
        "  history = model.fit(current_train_ds,\n",
        "                    epochs=no_epochs,\n",
        "                    validation_data=current_val_ds)\n",
        "  \n",
        "  # save results\n",
        "  if i == 0:\n",
        "    history_map['accuracy'] = [history.history['accuracy']]\n",
        "    history_map['val_accuracy'] = [history.history['val_accuracy']]\n",
        "    history_map['loss'] = [history.history['loss']]\n",
        "    history_map['val_loss'] = [history.history['val_loss']]\n",
        "  else:\n",
        "    history_map['accuracy'].append(history.history['accuracy'])\n",
        "    history_map['val_accuracy'].append(history.history['val_accuracy'])\n",
        "    history_map['loss'].append(history.history['loss'])\n",
        "    history_map['val_loss'].append(history.history['val_loss'])\n",
        "  \n",
        "  # get final test accuracy by taking the max accuracy over the whole training\n",
        "  final_acc_list.append(np.amax(history.history['val_accuracy']))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fold 1\n",
            "Model: \"functional_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_2 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_2 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_2 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "inception_v3 (Functional)    (None, 8, 8, 2048)        21802784  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               262272    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 22,066,346\n",
            "Trainable params: 263,562\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 2.4921 - accuracy: 0.0850\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 2.3100 - accuracy: 0.1725 - val_loss: 2.0874 - val_accuracy: 0.2950\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 2.0224 - accuracy: 0.3187 - val_loss: 1.8315 - val_accuracy: 0.4750\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 1.7295 - accuracy: 0.4563 - val_loss: 1.5721 - val_accuracy: 0.6000\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 1.4756 - accuracy: 0.5525 - val_loss: 1.3671 - val_accuracy: 0.6150\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 1.2366 - accuracy: 0.6375 - val_loss: 1.2289 - val_accuracy: 0.6300\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 1.0964 - accuracy: 0.6762 - val_loss: 1.1012 - val_accuracy: 0.6700\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 1.0131 - accuracy: 0.6963 - val_loss: 1.0235 - val_accuracy: 0.6900\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 0.9206 - accuracy: 0.7300 - val_loss: 0.9569 - val_accuracy: 0.6850\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 0.8413 - accuracy: 0.7550 - val_loss: 0.8796 - val_accuracy: 0.7400\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 0.7886 - accuracy: 0.7538 - val_loss: 0.8490 - val_accuracy: 0.7200\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.7472 - accuracy: 0.7625 - val_loss: 0.8220 - val_accuracy: 0.7200\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 0.7009 - accuracy: 0.7900 - val_loss: 0.8097 - val_accuracy: 0.7250\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.6603 - accuracy: 0.8025 - val_loss: 0.7852 - val_accuracy: 0.7350\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 0.6371 - accuracy: 0.8100 - val_loss: 0.7603 - val_accuracy: 0.7400\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.6386 - accuracy: 0.7962 - val_loss: 0.7490 - val_accuracy: 0.7500\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.5774 - accuracy: 0.8213 - val_loss: 0.7323 - val_accuracy: 0.7500\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 0.5493 - accuracy: 0.8375 - val_loss: 0.7341 - val_accuracy: 0.7650\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.5053 - accuracy: 0.8550 - val_loss: 0.7281 - val_accuracy: 0.7450\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.5270 - accuracy: 0.8325 - val_loss: 0.6974 - val_accuracy: 0.7650\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 0.4833 - accuracy: 0.8612 - val_loss: 0.6979 - val_accuracy: 0.7600\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 0.4868 - accuracy: 0.8612 - val_loss: 0.6918 - val_accuracy: 0.7600\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.4453 - accuracy: 0.8675 - val_loss: 0.6842 - val_accuracy: 0.7750\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 0.4069 - accuracy: 0.8737 - val_loss: 0.6775 - val_accuracy: 0.7650\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.4103 - accuracy: 0.8825 - val_loss: 0.6776 - val_accuracy: 0.7650\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.3962 - accuracy: 0.8850 - val_loss: 0.6762 - val_accuracy: 0.7700\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.3907 - accuracy: 0.8888 - val_loss: 0.6915 - val_accuracy: 0.7700\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.4118 - accuracy: 0.8675 - val_loss: 0.6628 - val_accuracy: 0.7750\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.3669 - accuracy: 0.8863 - val_loss: 0.6451 - val_accuracy: 0.7800\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.3735 - accuracy: 0.8863 - val_loss: 0.6935 - val_accuracy: 0.7450\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.3488 - accuracy: 0.9013 - val_loss: 0.6432 - val_accuracy: 0.7800\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.3336 - accuracy: 0.8963 - val_loss: 0.6728 - val_accuracy: 0.7750\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.3253 - accuracy: 0.9038 - val_loss: 0.6663 - val_accuracy: 0.7750\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 0.3367 - accuracy: 0.8913 - val_loss: 0.6732 - val_accuracy: 0.7700\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.3162 - accuracy: 0.9075 - val_loss: 0.6296 - val_accuracy: 0.8000\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.3008 - accuracy: 0.9225 - val_loss: 0.6664 - val_accuracy: 0.7600\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 0.3156 - accuracy: 0.9125 - val_loss: 0.6369 - val_accuracy: 0.7950\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 0.3061 - accuracy: 0.9025 - val_loss: 0.6537 - val_accuracy: 0.7650\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2868 - accuracy: 0.9250 - val_loss: 0.6424 - val_accuracy: 0.8050\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2983 - accuracy: 0.9112 - val_loss: 0.6553 - val_accuracy: 0.7700\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 0.2930 - accuracy: 0.9100 - val_loss: 0.6368 - val_accuracy: 0.7650\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2497 - accuracy: 0.9212 - val_loss: 0.6591 - val_accuracy: 0.7850\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 0.2709 - accuracy: 0.9162 - val_loss: 0.6376 - val_accuracy: 0.7950\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 0.2578 - accuracy: 0.9275 - val_loss: 0.6465 - val_accuracy: 0.8000\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2859 - accuracy: 0.9125 - val_loss: 0.6506 - val_accuracy: 0.7800\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 0.2523 - accuracy: 0.9237 - val_loss: 0.6348 - val_accuracy: 0.7800\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2536 - accuracy: 0.9287 - val_loss: 0.6341 - val_accuracy: 0.8000\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2395 - accuracy: 0.9350 - val_loss: 0.6353 - val_accuracy: 0.7850\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2392 - accuracy: 0.9225 - val_loss: 0.6538 - val_accuracy: 0.7700\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2385 - accuracy: 0.9425 - val_loss: 0.6269 - val_accuracy: 0.8000\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2162 - accuracy: 0.9400 - val_loss: 0.6444 - val_accuracy: 0.7900\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2198 - accuracy: 0.9463 - val_loss: 0.6523 - val_accuracy: 0.7700\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2363 - accuracy: 0.9312 - val_loss: 0.6421 - val_accuracy: 0.7900\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2067 - accuracy: 0.9337 - val_loss: 0.6253 - val_accuracy: 0.8200\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2146 - accuracy: 0.9425 - val_loss: 0.6598 - val_accuracy: 0.7850\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2046 - accuracy: 0.9450 - val_loss: 0.6596 - val_accuracy: 0.7800\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2055 - accuracy: 0.9362 - val_loss: 0.6554 - val_accuracy: 0.7750\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 0.2014 - accuracy: 0.9388 - val_loss: 0.6507 - val_accuracy: 0.7900\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2067 - accuracy: 0.9488 - val_loss: 0.6292 - val_accuracy: 0.8000\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1865 - accuracy: 0.9500 - val_loss: 0.6538 - val_accuracy: 0.7950\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1774 - accuracy: 0.9463 - val_loss: 0.6503 - val_accuracy: 0.7950\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1846 - accuracy: 0.9500 - val_loss: 0.6560 - val_accuracy: 0.7850\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1733 - accuracy: 0.9513 - val_loss: 0.6397 - val_accuracy: 0.8050\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1658 - accuracy: 0.9538 - val_loss: 0.6581 - val_accuracy: 0.7950\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1822 - accuracy: 0.9388 - val_loss: 0.6700 - val_accuracy: 0.7850\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1472 - accuracy: 0.9625 - val_loss: 0.6529 - val_accuracy: 0.8000\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1685 - accuracy: 0.9500 - val_loss: 0.6632 - val_accuracy: 0.7750\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1807 - accuracy: 0.9475 - val_loss: 0.6501 - val_accuracy: 0.7800\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1638 - accuracy: 0.9588 - val_loss: 0.6609 - val_accuracy: 0.7900\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 0.1758 - accuracy: 0.9425 - val_loss: 0.6562 - val_accuracy: 0.7850\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1611 - accuracy: 0.9638 - val_loss: 0.6607 - val_accuracy: 0.7900\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1509 - accuracy: 0.9575 - val_loss: 0.6436 - val_accuracy: 0.7900\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1580 - accuracy: 0.9500 - val_loss: 0.6739 - val_accuracy: 0.7750\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1609 - accuracy: 0.9625 - val_loss: 0.6545 - val_accuracy: 0.8050\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1492 - accuracy: 0.9575 - val_loss: 0.6622 - val_accuracy: 0.8000\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1557 - accuracy: 0.9550 - val_loss: 0.6582 - val_accuracy: 0.7900\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1287 - accuracy: 0.9663 - val_loss: 0.6607 - val_accuracy: 0.8050\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1458 - accuracy: 0.9650 - val_loss: 0.6565 - val_accuracy: 0.7900\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1460 - accuracy: 0.9588 - val_loss: 0.6400 - val_accuracy: 0.8000\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1527 - accuracy: 0.9538 - val_loss: 0.6492 - val_accuracy: 0.8000\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1292 - accuracy: 0.9675 - val_loss: 0.6543 - val_accuracy: 0.7850\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1292 - accuracy: 0.9638 - val_loss: 0.6620 - val_accuracy: 0.7850\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1353 - accuracy: 0.9625 - val_loss: 0.6733 - val_accuracy: 0.7800\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1569 - accuracy: 0.9450 - val_loss: 0.6456 - val_accuracy: 0.7800\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 0.1391 - accuracy: 0.9613 - val_loss: 0.6555 - val_accuracy: 0.7900\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1485 - accuracy: 0.9563 - val_loss: 0.6620 - val_accuracy: 0.8050\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1260 - accuracy: 0.9613 - val_loss: 0.6657 - val_accuracy: 0.7800\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 0.1388 - accuracy: 0.9550 - val_loss: 0.6874 - val_accuracy: 0.8000\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1278 - accuracy: 0.9650 - val_loss: 0.6858 - val_accuracy: 0.7800\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1298 - accuracy: 0.9600 - val_loss: 0.6826 - val_accuracy: 0.7850\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1234 - accuracy: 0.9700 - val_loss: 0.6839 - val_accuracy: 0.7800\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1215 - accuracy: 0.9725 - val_loss: 0.6763 - val_accuracy: 0.7950\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1270 - accuracy: 0.9613 - val_loss: 0.6640 - val_accuracy: 0.7950\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1130 - accuracy: 0.9725 - val_loss: 0.6538 - val_accuracy: 0.8000\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1198 - accuracy: 0.9712 - val_loss: 0.6920 - val_accuracy: 0.8000\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1108 - accuracy: 0.9737 - val_loss: 0.6715 - val_accuracy: 0.7800\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1171 - accuracy: 0.9625 - val_loss: 0.6835 - val_accuracy: 0.7950\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1152 - accuracy: 0.9700 - val_loss: 0.6813 - val_accuracy: 0.7950\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 0.1185 - accuracy: 0.9700 - val_loss: 0.6789 - val_accuracy: 0.7950\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1188 - accuracy: 0.9737 - val_loss: 0.6634 - val_accuracy: 0.8100\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1213 - accuracy: 0.9625 - val_loss: 0.6612 - val_accuracy: 0.8050\n",
            "fold 2\n",
            "Model: \"functional_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_3 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_3 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_3 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "inception_v3 (Functional)    (None, 8, 8, 2048)        21802784  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_3 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               262272    \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 22,066,346\n",
            "Trainable params: 263,562\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 2.5270 - accuracy: 0.0850\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 2.3038 - accuracy: 0.1762 - val_loss: 2.0100 - val_accuracy: 0.3200\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 1.9554 - accuracy: 0.3300 - val_loss: 1.7105 - val_accuracy: 0.5650\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 1.6730 - accuracy: 0.4600 - val_loss: 1.4477 - val_accuracy: 0.6400\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 1.4440 - accuracy: 0.5525 - val_loss: 1.2397 - val_accuracy: 0.7100\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 1.2471 - accuracy: 0.6288 - val_loss: 1.0708 - val_accuracy: 0.7500\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 1.0931 - accuracy: 0.6737 - val_loss: 0.9815 - val_accuracy: 0.7700\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.9922 - accuracy: 0.7225 - val_loss: 0.9212 - val_accuracy: 0.7550\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.8891 - accuracy: 0.7337 - val_loss: 0.8351 - val_accuracy: 0.7700\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.8626 - accuracy: 0.7425 - val_loss: 0.7809 - val_accuracy: 0.7950\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.8015 - accuracy: 0.7788 - val_loss: 0.7299 - val_accuracy: 0.8050\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.7285 - accuracy: 0.7912 - val_loss: 0.6956 - val_accuracy: 0.8050\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.6751 - accuracy: 0.7962 - val_loss: 0.7165 - val_accuracy: 0.7800\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.6819 - accuracy: 0.7975 - val_loss: 0.6771 - val_accuracy: 0.7950\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.6500 - accuracy: 0.8213 - val_loss: 0.6594 - val_accuracy: 0.8100\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.6221 - accuracy: 0.8150 - val_loss: 0.6488 - val_accuracy: 0.8250\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.5454 - accuracy: 0.8537 - val_loss: 0.6281 - val_accuracy: 0.8000\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.5715 - accuracy: 0.8300 - val_loss: 0.5994 - val_accuracy: 0.8100\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.5437 - accuracy: 0.8263 - val_loss: 0.5915 - val_accuracy: 0.8100\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.5218 - accuracy: 0.8562 - val_loss: 0.6218 - val_accuracy: 0.7950\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.4966 - accuracy: 0.8512 - val_loss: 0.6077 - val_accuracy: 0.8200\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.4549 - accuracy: 0.8763 - val_loss: 0.5783 - val_accuracy: 0.8200\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.4518 - accuracy: 0.8750 - val_loss: 0.5677 - val_accuracy: 0.8250\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.4386 - accuracy: 0.8662 - val_loss: 0.5818 - val_accuracy: 0.8000\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.4359 - accuracy: 0.8662 - val_loss: 0.5633 - val_accuracy: 0.8150\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.4162 - accuracy: 0.8700 - val_loss: 0.5824 - val_accuracy: 0.8100\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.3833 - accuracy: 0.8825 - val_loss: 0.5836 - val_accuracy: 0.8000\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.4352 - accuracy: 0.8662 - val_loss: 0.5699 - val_accuracy: 0.8150\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.3924 - accuracy: 0.8813 - val_loss: 0.5615 - val_accuracy: 0.7950\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.3693 - accuracy: 0.8950 - val_loss: 0.5533 - val_accuracy: 0.8200\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 0.3391 - accuracy: 0.8950 - val_loss: 0.5456 - val_accuracy: 0.8200\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.3708 - accuracy: 0.8938 - val_loss: 0.5494 - val_accuracy: 0.8100\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.3644 - accuracy: 0.8838 - val_loss: 0.5670 - val_accuracy: 0.7950\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.3449 - accuracy: 0.8950 - val_loss: 0.5446 - val_accuracy: 0.8150\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.3303 - accuracy: 0.9038 - val_loss: 0.5564 - val_accuracy: 0.8250\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.3060 - accuracy: 0.9137 - val_loss: 0.5443 - val_accuracy: 0.8150\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.3068 - accuracy: 0.9125 - val_loss: 0.5417 - val_accuracy: 0.8200\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2681 - accuracy: 0.9312 - val_loss: 0.5376 - val_accuracy: 0.8200\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.3226 - accuracy: 0.9112 - val_loss: 0.5424 - val_accuracy: 0.8250\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.3071 - accuracy: 0.9025 - val_loss: 0.5528 - val_accuracy: 0.8050\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2762 - accuracy: 0.9200 - val_loss: 0.5317 - val_accuracy: 0.8250\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2792 - accuracy: 0.9212 - val_loss: 0.5360 - val_accuracy: 0.8450\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2666 - accuracy: 0.9237 - val_loss: 0.5309 - val_accuracy: 0.8150\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2645 - accuracy: 0.9162 - val_loss: 0.5426 - val_accuracy: 0.8000\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2492 - accuracy: 0.9362 - val_loss: 0.5191 - val_accuracy: 0.8200\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2584 - accuracy: 0.9212 - val_loss: 0.5257 - val_accuracy: 0.8100\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2633 - accuracy: 0.9250 - val_loss: 0.5297 - val_accuracy: 0.8400\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 0.2576 - accuracy: 0.9175 - val_loss: 0.5370 - val_accuracy: 0.8300\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2198 - accuracy: 0.9525 - val_loss: 0.5432 - val_accuracy: 0.8250\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2262 - accuracy: 0.9375 - val_loss: 0.5237 - val_accuracy: 0.8300\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2395 - accuracy: 0.9400 - val_loss: 0.5218 - val_accuracy: 0.8200\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2307 - accuracy: 0.9262 - val_loss: 0.5319 - val_accuracy: 0.8300\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2314 - accuracy: 0.9275 - val_loss: 0.5311 - val_accuracy: 0.8200\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2092 - accuracy: 0.9425 - val_loss: 0.5339 - val_accuracy: 0.8100\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 0.2228 - accuracy: 0.9375 - val_loss: 0.5470 - val_accuracy: 0.8200\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1955 - accuracy: 0.9488 - val_loss: 0.5457 - val_accuracy: 0.8150\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2311 - accuracy: 0.9212 - val_loss: 0.5278 - val_accuracy: 0.8100\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2202 - accuracy: 0.9375 - val_loss: 0.5532 - val_accuracy: 0.8200\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2003 - accuracy: 0.9525 - val_loss: 0.5335 - val_accuracy: 0.8200\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1816 - accuracy: 0.9513 - val_loss: 0.5417 - val_accuracy: 0.8000\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2058 - accuracy: 0.9388 - val_loss: 0.5275 - val_accuracy: 0.8200\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1831 - accuracy: 0.9425 - val_loss: 0.5350 - val_accuracy: 0.8150\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2099 - accuracy: 0.9413 - val_loss: 0.5397 - val_accuracy: 0.8150\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2015 - accuracy: 0.9400 - val_loss: 0.5444 - val_accuracy: 0.8100\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1756 - accuracy: 0.9563 - val_loss: 0.5638 - val_accuracy: 0.8150\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1699 - accuracy: 0.9563 - val_loss: 0.5397 - val_accuracy: 0.8000\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1480 - accuracy: 0.9650 - val_loss: 0.5477 - val_accuracy: 0.8200\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1775 - accuracy: 0.9488 - val_loss: 0.5354 - val_accuracy: 0.8000\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1755 - accuracy: 0.9475 - val_loss: 0.5458 - val_accuracy: 0.8200\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1716 - accuracy: 0.9563 - val_loss: 0.5374 - val_accuracy: 0.8050\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1537 - accuracy: 0.9613 - val_loss: 0.5483 - val_accuracy: 0.8150\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1758 - accuracy: 0.9488 - val_loss: 0.5230 - val_accuracy: 0.8100\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1626 - accuracy: 0.9538 - val_loss: 0.5376 - val_accuracy: 0.8100\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1614 - accuracy: 0.9538 - val_loss: 0.5667 - val_accuracy: 0.7900\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1407 - accuracy: 0.9625 - val_loss: 0.5225 - val_accuracy: 0.8300\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1600 - accuracy: 0.9588 - val_loss: 0.5343 - val_accuracy: 0.8200\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1446 - accuracy: 0.9613 - val_loss: 0.5329 - val_accuracy: 0.8200\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1384 - accuracy: 0.9625 - val_loss: 0.5415 - val_accuracy: 0.8150\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1390 - accuracy: 0.9650 - val_loss: 0.5161 - val_accuracy: 0.8300\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1404 - accuracy: 0.9638 - val_loss: 0.5547 - val_accuracy: 0.8050\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1357 - accuracy: 0.9688 - val_loss: 0.5479 - val_accuracy: 0.8150\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1429 - accuracy: 0.9588 - val_loss: 0.5666 - val_accuracy: 0.8150\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1410 - accuracy: 0.9675 - val_loss: 0.5536 - val_accuracy: 0.8250\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1516 - accuracy: 0.9513 - val_loss: 0.5567 - val_accuracy: 0.8100\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1372 - accuracy: 0.9600 - val_loss: 0.5608 - val_accuracy: 0.8150\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1247 - accuracy: 0.9675 - val_loss: 0.5532 - val_accuracy: 0.8250\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 0.1273 - accuracy: 0.9663 - val_loss: 0.5729 - val_accuracy: 0.8250\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1527 - accuracy: 0.9525 - val_loss: 0.5448 - val_accuracy: 0.8150\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1173 - accuracy: 0.9712 - val_loss: 0.5657 - val_accuracy: 0.8150\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1352 - accuracy: 0.9712 - val_loss: 0.5419 - val_accuracy: 0.8250\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1090 - accuracy: 0.9775 - val_loss: 0.5740 - val_accuracy: 0.7900\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1204 - accuracy: 0.9712 - val_loss: 0.5539 - val_accuracy: 0.8200\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1098 - accuracy: 0.9688 - val_loss: 0.5431 - val_accuracy: 0.8200\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 0.1354 - accuracy: 0.9650 - val_loss: 0.5434 - val_accuracy: 0.8400\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1151 - accuracy: 0.9700 - val_loss: 0.5275 - val_accuracy: 0.8400\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1069 - accuracy: 0.9725 - val_loss: 0.5799 - val_accuracy: 0.8000\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1174 - accuracy: 0.9638 - val_loss: 0.5507 - val_accuracy: 0.8100\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1135 - accuracy: 0.9725 - val_loss: 0.5480 - val_accuracy: 0.8150\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1180 - accuracy: 0.9712 - val_loss: 0.5547 - val_accuracy: 0.8050\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1137 - accuracy: 0.9650 - val_loss: 0.5763 - val_accuracy: 0.7950\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1095 - accuracy: 0.9725 - val_loss: 0.5435 - val_accuracy: 0.8000\n",
            "fold 3\n",
            "Model: \"functional_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_4 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_4 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_4 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "inception_v3 (Functional)    (None, 8, 8, 2048)        21802784  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_4 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               262272    \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 22,066,346\n",
            "Trainable params: 263,562\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 2.4282 - accuracy: 0.1050\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 2.3068 - accuracy: 0.1663 - val_loss: 2.0432 - val_accuracy: 0.3400\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 1.9279 - accuracy: 0.3550 - val_loss: 1.7464 - val_accuracy: 0.5050\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 1.6684 - accuracy: 0.4737 - val_loss: 1.4799 - val_accuracy: 0.6650\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 1.4598 - accuracy: 0.5650 - val_loss: 1.2729 - val_accuracy: 0.6900\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 1.2410 - accuracy: 0.6325 - val_loss: 1.0832 - val_accuracy: 0.7400\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 1.1292 - accuracy: 0.6612 - val_loss: 0.9868 - val_accuracy: 0.7650\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 1.0018 - accuracy: 0.6963 - val_loss: 0.8874 - val_accuracy: 0.7600\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.9411 - accuracy: 0.7250 - val_loss: 0.8389 - val_accuracy: 0.7950\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.8585 - accuracy: 0.7487 - val_loss: 0.7693 - val_accuracy: 0.8250\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.7718 - accuracy: 0.7800 - val_loss: 0.7373 - val_accuracy: 0.7950\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.7695 - accuracy: 0.7713 - val_loss: 0.7229 - val_accuracy: 0.8100\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.6718 - accuracy: 0.8050 - val_loss: 0.6743 - val_accuracy: 0.8100\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.6844 - accuracy: 0.7875 - val_loss: 0.6430 - val_accuracy: 0.8300\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.6144 - accuracy: 0.8125 - val_loss: 0.6215 - val_accuracy: 0.8100\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.6337 - accuracy: 0.8062 - val_loss: 0.6093 - val_accuracy: 0.8300\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.6241 - accuracy: 0.8050 - val_loss: 0.5930 - val_accuracy: 0.8150\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.5619 - accuracy: 0.8363 - val_loss: 0.5880 - val_accuracy: 0.8150\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.5474 - accuracy: 0.8475 - val_loss: 0.5804 - val_accuracy: 0.8200\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.5163 - accuracy: 0.8537 - val_loss: 0.5710 - val_accuracy: 0.8150\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.5006 - accuracy: 0.8375 - val_loss: 0.5570 - val_accuracy: 0.8200\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.4969 - accuracy: 0.8500 - val_loss: 0.5580 - val_accuracy: 0.8250\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.4746 - accuracy: 0.8525 - val_loss: 0.5378 - val_accuracy: 0.8300\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.4582 - accuracy: 0.8500 - val_loss: 0.5540 - val_accuracy: 0.8350\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.4396 - accuracy: 0.8562 - val_loss: 0.5270 - val_accuracy: 0.8350\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.3989 - accuracy: 0.8788 - val_loss: 0.5374 - val_accuracy: 0.8200\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.4230 - accuracy: 0.8863 - val_loss: 0.5497 - val_accuracy: 0.8300\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.3916 - accuracy: 0.8900 - val_loss: 0.5288 - val_accuracy: 0.8300\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.3753 - accuracy: 0.8925 - val_loss: 0.5273 - val_accuracy: 0.8200\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.3904 - accuracy: 0.8800 - val_loss: 0.5246 - val_accuracy: 0.8350\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.3473 - accuracy: 0.8913 - val_loss: 0.5087 - val_accuracy: 0.8350\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.3580 - accuracy: 0.8925 - val_loss: 0.5140 - val_accuracy: 0.8250\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.3439 - accuracy: 0.9050 - val_loss: 0.5145 - val_accuracy: 0.8300\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.3777 - accuracy: 0.8863 - val_loss: 0.5137 - val_accuracy: 0.8250\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.3607 - accuracy: 0.8963 - val_loss: 0.5160 - val_accuracy: 0.8450\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.3469 - accuracy: 0.8938 - val_loss: 0.5204 - val_accuracy: 0.8300\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.3316 - accuracy: 0.9075 - val_loss: 0.4959 - val_accuracy: 0.8300\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.3281 - accuracy: 0.9075 - val_loss: 0.4985 - val_accuracy: 0.8300\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2980 - accuracy: 0.9137 - val_loss: 0.4864 - val_accuracy: 0.8350\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.3037 - accuracy: 0.9075 - val_loss: 0.4847 - val_accuracy: 0.8350\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2884 - accuracy: 0.9212 - val_loss: 0.4937 - val_accuracy: 0.8400\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.3015 - accuracy: 0.9062 - val_loss: 0.4927 - val_accuracy: 0.8250\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2806 - accuracy: 0.9237 - val_loss: 0.4847 - val_accuracy: 0.8350\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2936 - accuracy: 0.9087 - val_loss: 0.4905 - val_accuracy: 0.8200\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2521 - accuracy: 0.9312 - val_loss: 0.4897 - val_accuracy: 0.8350\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2681 - accuracy: 0.9262 - val_loss: 0.4849 - val_accuracy: 0.8250\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2756 - accuracy: 0.9162 - val_loss: 0.4799 - val_accuracy: 0.8500\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2441 - accuracy: 0.9212 - val_loss: 0.5039 - val_accuracy: 0.8250\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2403 - accuracy: 0.9262 - val_loss: 0.4860 - val_accuracy: 0.8300\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2406 - accuracy: 0.9312 - val_loss: 0.4849 - val_accuracy: 0.8250\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2435 - accuracy: 0.9237 - val_loss: 0.4878 - val_accuracy: 0.8350\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2267 - accuracy: 0.9388 - val_loss: 0.4731 - val_accuracy: 0.8350\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2303 - accuracy: 0.9388 - val_loss: 0.4758 - val_accuracy: 0.8250\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.2026 - accuracy: 0.9438 - val_loss: 0.4849 - val_accuracy: 0.8350\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2369 - accuracy: 0.9275 - val_loss: 0.4828 - val_accuracy: 0.8350\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.2047 - accuracy: 0.9375 - val_loss: 0.4866 - val_accuracy: 0.8350\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2250 - accuracy: 0.9350 - val_loss: 0.4696 - val_accuracy: 0.8400\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2113 - accuracy: 0.9337 - val_loss: 0.4660 - val_accuracy: 0.8400\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1854 - accuracy: 0.9488 - val_loss: 0.4807 - val_accuracy: 0.8400\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2278 - accuracy: 0.9375 - val_loss: 0.4592 - val_accuracy: 0.8450\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1990 - accuracy: 0.9375 - val_loss: 0.4727 - val_accuracy: 0.8250\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1903 - accuracy: 0.9475 - val_loss: 0.4734 - val_accuracy: 0.8350\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2013 - accuracy: 0.9400 - val_loss: 0.4582 - val_accuracy: 0.8300\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1919 - accuracy: 0.9500 - val_loss: 0.5011 - val_accuracy: 0.8250\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1890 - accuracy: 0.9400 - val_loss: 0.4714 - val_accuracy: 0.8250\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1884 - accuracy: 0.9500 - val_loss: 0.4698 - val_accuracy: 0.8450\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1937 - accuracy: 0.9413 - val_loss: 0.4523 - val_accuracy: 0.8300\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1890 - accuracy: 0.9450 - val_loss: 0.4704 - val_accuracy: 0.8400\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1991 - accuracy: 0.9475 - val_loss: 0.4739 - val_accuracy: 0.8350\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1963 - accuracy: 0.9413 - val_loss: 0.4818 - val_accuracy: 0.8300\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1874 - accuracy: 0.9400 - val_loss: 0.4639 - val_accuracy: 0.8350\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1530 - accuracy: 0.9588 - val_loss: 0.4933 - val_accuracy: 0.8200\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1741 - accuracy: 0.9488 - val_loss: 0.4748 - val_accuracy: 0.8350\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1745 - accuracy: 0.9550 - val_loss: 0.4578 - val_accuracy: 0.8450\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1553 - accuracy: 0.9538 - val_loss: 0.4543 - val_accuracy: 0.8450\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1706 - accuracy: 0.9450 - val_loss: 0.4755 - val_accuracy: 0.8250\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1520 - accuracy: 0.9575 - val_loss: 0.4772 - val_accuracy: 0.8450\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1638 - accuracy: 0.9588 - val_loss: 0.4665 - val_accuracy: 0.8500\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1576 - accuracy: 0.9625 - val_loss: 0.4695 - val_accuracy: 0.8550\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1426 - accuracy: 0.9663 - val_loss: 0.4626 - val_accuracy: 0.8500\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1608 - accuracy: 0.9550 - val_loss: 0.4813 - val_accuracy: 0.8400\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1836 - accuracy: 0.9463 - val_loss: 0.4793 - val_accuracy: 0.8550\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1398 - accuracy: 0.9575 - val_loss: 0.4686 - val_accuracy: 0.8450\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1560 - accuracy: 0.9563 - val_loss: 0.4631 - val_accuracy: 0.8500\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1549 - accuracy: 0.9588 - val_loss: 0.4857 - val_accuracy: 0.8350\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1401 - accuracy: 0.9675 - val_loss: 0.4810 - val_accuracy: 0.8400\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1361 - accuracy: 0.9650 - val_loss: 0.4520 - val_accuracy: 0.8550\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1493 - accuracy: 0.9563 - val_loss: 0.4972 - val_accuracy: 0.8350\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1251 - accuracy: 0.9675 - val_loss: 0.4919 - val_accuracy: 0.8400\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1485 - accuracy: 0.9638 - val_loss: 0.4711 - val_accuracy: 0.8400\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1286 - accuracy: 0.9625 - val_loss: 0.4610 - val_accuracy: 0.8450\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1289 - accuracy: 0.9650 - val_loss: 0.4559 - val_accuracy: 0.8450\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1511 - accuracy: 0.9525 - val_loss: 0.4893 - val_accuracy: 0.8300\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1170 - accuracy: 0.9750 - val_loss: 0.4653 - val_accuracy: 0.8450\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1275 - accuracy: 0.9625 - val_loss: 0.4854 - val_accuracy: 0.8600\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1398 - accuracy: 0.9563 - val_loss: 0.4771 - val_accuracy: 0.8400\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1403 - accuracy: 0.9563 - val_loss: 0.4637 - val_accuracy: 0.8500\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1352 - accuracy: 0.9663 - val_loss: 0.4511 - val_accuracy: 0.8550\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1557 - accuracy: 0.9613 - val_loss: 0.4723 - val_accuracy: 0.8350\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1184 - accuracy: 0.9663 - val_loss: 0.4688 - val_accuracy: 0.8400\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1158 - accuracy: 0.9663 - val_loss: 0.4833 - val_accuracy: 0.8550\n",
            "fold 4\n",
            "Model: \"functional_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_5 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_5 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_5 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "inception_v3 (Functional)    (None, 8, 8, 2048)        21802784  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_5 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 128)               262272    \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 22,066,346\n",
            "Trainable params: 263,562\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 2.5802 - accuracy: 0.0750\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 2.3108 - accuracy: 0.1663 - val_loss: 2.0083 - val_accuracy: 0.3650\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 1.9480 - accuracy: 0.3388 - val_loss: 1.7168 - val_accuracy: 0.5550\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 1.6309 - accuracy: 0.4913 - val_loss: 1.4475 - val_accuracy: 0.6500\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 1.4000 - accuracy: 0.5612 - val_loss: 1.2237 - val_accuracy: 0.6800\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 1.1959 - accuracy: 0.6250 - val_loss: 1.0803 - val_accuracy: 0.7200\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 1.0518 - accuracy: 0.6900 - val_loss: 0.9750 - val_accuracy: 0.7600\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.9539 - accuracy: 0.7050 - val_loss: 0.9065 - val_accuracy: 0.7700\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.8870 - accuracy: 0.7475 - val_loss: 0.8242 - val_accuracy: 0.8050\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.8404 - accuracy: 0.7425 - val_loss: 0.7559 - val_accuracy: 0.8200\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.7589 - accuracy: 0.7812 - val_loss: 0.7644 - val_accuracy: 0.7850\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.7212 - accuracy: 0.7788 - val_loss: 0.7135 - val_accuracy: 0.8100\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.6960 - accuracy: 0.7825 - val_loss: 0.6616 - val_accuracy: 0.8150\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.6476 - accuracy: 0.8112 - val_loss: 0.6690 - val_accuracy: 0.8100\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.6113 - accuracy: 0.8062 - val_loss: 0.6378 - val_accuracy: 0.8100\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.5982 - accuracy: 0.8125 - val_loss: 0.6274 - val_accuracy: 0.8250\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.5481 - accuracy: 0.8525 - val_loss: 0.6091 - val_accuracy: 0.8200\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.4932 - accuracy: 0.8575 - val_loss: 0.5870 - val_accuracy: 0.8100\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.5143 - accuracy: 0.8537 - val_loss: 0.5953 - val_accuracy: 0.8250\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.4886 - accuracy: 0.8475 - val_loss: 0.6112 - val_accuracy: 0.8150\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.4584 - accuracy: 0.8562 - val_loss: 0.5843 - val_accuracy: 0.8200\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.4247 - accuracy: 0.8800 - val_loss: 0.5748 - val_accuracy: 0.8150\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.4291 - accuracy: 0.8788 - val_loss: 0.5686 - val_accuracy: 0.8200\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.4067 - accuracy: 0.8825 - val_loss: 0.5868 - val_accuracy: 0.8050\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.3881 - accuracy: 0.8900 - val_loss: 0.5653 - val_accuracy: 0.8200\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.3878 - accuracy: 0.8888 - val_loss: 0.5601 - val_accuracy: 0.8200\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.4012 - accuracy: 0.8775 - val_loss: 0.5624 - val_accuracy: 0.8250\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.3838 - accuracy: 0.8788 - val_loss: 0.5737 - val_accuracy: 0.8100\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.3379 - accuracy: 0.9025 - val_loss: 0.5568 - val_accuracy: 0.8250\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.3538 - accuracy: 0.9013 - val_loss: 0.5764 - val_accuracy: 0.8100\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.3528 - accuracy: 0.8913 - val_loss: 0.5522 - val_accuracy: 0.8100\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.3511 - accuracy: 0.9087 - val_loss: 0.5508 - val_accuracy: 0.8200\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.3093 - accuracy: 0.9075 - val_loss: 0.5596 - val_accuracy: 0.8200\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.3306 - accuracy: 0.8913 - val_loss: 0.5510 - val_accuracy: 0.8200\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.3029 - accuracy: 0.9112 - val_loss: 0.5386 - val_accuracy: 0.8350\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.3101 - accuracy: 0.9038 - val_loss: 0.5510 - val_accuracy: 0.8100\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.2954 - accuracy: 0.9112 - val_loss: 0.5590 - val_accuracy: 0.8100\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.3013 - accuracy: 0.9137 - val_loss: 0.5374 - val_accuracy: 0.8400\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2754 - accuracy: 0.9200 - val_loss: 0.5647 - val_accuracy: 0.8050\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2538 - accuracy: 0.9250 - val_loss: 0.5434 - val_accuracy: 0.8000\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.2621 - accuracy: 0.9200 - val_loss: 0.5477 - val_accuracy: 0.8200\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2515 - accuracy: 0.9275 - val_loss: 0.5323 - val_accuracy: 0.8450\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.2859 - accuracy: 0.9062 - val_loss: 0.5707 - val_accuracy: 0.8100\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.2426 - accuracy: 0.9312 - val_loss: 0.5489 - val_accuracy: 0.8250\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.2337 - accuracy: 0.9413 - val_loss: 0.5340 - val_accuracy: 0.8050\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.2381 - accuracy: 0.9250 - val_loss: 0.5594 - val_accuracy: 0.8150\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2208 - accuracy: 0.9375 - val_loss: 0.5488 - val_accuracy: 0.8150\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2336 - accuracy: 0.9237 - val_loss: 0.5222 - val_accuracy: 0.8100\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2127 - accuracy: 0.9413 - val_loss: 0.5374 - val_accuracy: 0.8200\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.2038 - accuracy: 0.9475 - val_loss: 0.5587 - val_accuracy: 0.8050\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2120 - accuracy: 0.9337 - val_loss: 0.5564 - val_accuracy: 0.8100\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2197 - accuracy: 0.9388 - val_loss: 0.5613 - val_accuracy: 0.8350\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2045 - accuracy: 0.9413 - val_loss: 0.5367 - val_accuracy: 0.8200\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.2007 - accuracy: 0.9425 - val_loss: 0.5519 - val_accuracy: 0.8000\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2143 - accuracy: 0.9287 - val_loss: 0.5501 - val_accuracy: 0.8100\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2099 - accuracy: 0.9350 - val_loss: 0.5682 - val_accuracy: 0.7900\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1859 - accuracy: 0.9538 - val_loss: 0.5632 - val_accuracy: 0.8100\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2019 - accuracy: 0.9350 - val_loss: 0.5576 - val_accuracy: 0.8150\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1782 - accuracy: 0.9550 - val_loss: 0.5782 - val_accuracy: 0.8000\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1567 - accuracy: 0.9625 - val_loss: 0.5634 - val_accuracy: 0.8250\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1714 - accuracy: 0.9588 - val_loss: 0.5395 - val_accuracy: 0.8300\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1800 - accuracy: 0.9500 - val_loss: 0.5911 - val_accuracy: 0.8000\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1679 - accuracy: 0.9550 - val_loss: 0.5601 - val_accuracy: 0.8250\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2170 - accuracy: 0.9450 - val_loss: 0.5674 - val_accuracy: 0.8250\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1649 - accuracy: 0.9538 - val_loss: 0.5666 - val_accuracy: 0.8150\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1747 - accuracy: 0.9513 - val_loss: 0.5998 - val_accuracy: 0.8100\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1589 - accuracy: 0.9588 - val_loss: 0.5606 - val_accuracy: 0.8100\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1572 - accuracy: 0.9538 - val_loss: 0.5562 - val_accuracy: 0.8250\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1684 - accuracy: 0.9488 - val_loss: 0.5753 - val_accuracy: 0.8200\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1589 - accuracy: 0.9575 - val_loss: 0.5591 - val_accuracy: 0.8100\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1582 - accuracy: 0.9488 - val_loss: 0.5654 - val_accuracy: 0.8250\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1669 - accuracy: 0.9538 - val_loss: 0.5483 - val_accuracy: 0.8250\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1423 - accuracy: 0.9650 - val_loss: 0.5502 - val_accuracy: 0.8150\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1619 - accuracy: 0.9513 - val_loss: 0.5729 - val_accuracy: 0.8100\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1619 - accuracy: 0.9538 - val_loss: 0.5346 - val_accuracy: 0.8100\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1520 - accuracy: 0.9600 - val_loss: 0.5807 - val_accuracy: 0.8050\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.1605 - accuracy: 0.9538 - val_loss: 0.5596 - val_accuracy: 0.8150\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1366 - accuracy: 0.9650 - val_loss: 0.5810 - val_accuracy: 0.8050\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1264 - accuracy: 0.9663 - val_loss: 0.5708 - val_accuracy: 0.8350\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1500 - accuracy: 0.9563 - val_loss: 0.5732 - val_accuracy: 0.8300\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1211 - accuracy: 0.9712 - val_loss: 0.6252 - val_accuracy: 0.8100\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1403 - accuracy: 0.9663 - val_loss: 0.5523 - val_accuracy: 0.7900\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1449 - accuracy: 0.9600 - val_loss: 0.5969 - val_accuracy: 0.8150\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1415 - accuracy: 0.9588 - val_loss: 0.5719 - val_accuracy: 0.8300\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1312 - accuracy: 0.9663 - val_loss: 0.5850 - val_accuracy: 0.8100\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1244 - accuracy: 0.9712 - val_loss: 0.5927 - val_accuracy: 0.8100\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1298 - accuracy: 0.9613 - val_loss: 0.5964 - val_accuracy: 0.8100\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1194 - accuracy: 0.9688 - val_loss: 0.6270 - val_accuracy: 0.7950\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1251 - accuracy: 0.9638 - val_loss: 0.5998 - val_accuracy: 0.8100\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1277 - accuracy: 0.9700 - val_loss: 0.5691 - val_accuracy: 0.8200\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1091 - accuracy: 0.9750 - val_loss: 0.6106 - val_accuracy: 0.8000\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1129 - accuracy: 0.9725 - val_loss: 0.5725 - val_accuracy: 0.8250\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1165 - accuracy: 0.9700 - val_loss: 0.6031 - val_accuracy: 0.8100\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1182 - accuracy: 0.9675 - val_loss: 0.6159 - val_accuracy: 0.8000\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1164 - accuracy: 0.9650 - val_loss: 0.5932 - val_accuracy: 0.8150\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 3s 67ms/step - loss: 0.1101 - accuracy: 0.9700 - val_loss: 0.6347 - val_accuracy: 0.7950\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 3s 67ms/step - loss: 0.1464 - accuracy: 0.9513 - val_loss: 0.6083 - val_accuracy: 0.8200\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 3s 67ms/step - loss: 0.0981 - accuracy: 0.9812 - val_loss: 0.6234 - val_accuracy: 0.8250\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1154 - accuracy: 0.9712 - val_loss: 0.6240 - val_accuracy: 0.8150\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1012 - accuracy: 0.9675 - val_loss: 0.6273 - val_accuracy: 0.8050\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1105 - accuracy: 0.9638 - val_loss: 0.6321 - val_accuracy: 0.8050\n",
            "fold 5\n",
            "Model: \"functional_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_6 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_6 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_6 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "inception_v3 (Functional)    (None, 8, 8, 2048)        21802784  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_6 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 128)               262272    \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 22,066,346\n",
            "Trainable params: 263,562\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 2.6400 - accuracy: 0.1000\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 2.3782 - accuracy: 0.1363 - val_loss: 2.0785 - val_accuracy: 0.2900\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 1.9610 - accuracy: 0.3575 - val_loss: 1.7981 - val_accuracy: 0.5000\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 1.7213 - accuracy: 0.4650 - val_loss: 1.5406 - val_accuracy: 0.5900\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 1.4239 - accuracy: 0.5562 - val_loss: 1.3552 - val_accuracy: 0.6600\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 1.2391 - accuracy: 0.6363 - val_loss: 1.1870 - val_accuracy: 0.6800\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 3s 67ms/step - loss: 1.1026 - accuracy: 0.6750 - val_loss: 1.0752 - val_accuracy: 0.7150\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 3s 67ms/step - loss: 0.9559 - accuracy: 0.7312 - val_loss: 0.9873 - val_accuracy: 0.7100\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.8971 - accuracy: 0.7225 - val_loss: 0.9037 - val_accuracy: 0.7450\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.8147 - accuracy: 0.7563 - val_loss: 0.8676 - val_accuracy: 0.7500\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.7796 - accuracy: 0.7750 - val_loss: 0.8188 - val_accuracy: 0.7650\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.7039 - accuracy: 0.8037 - val_loss: 0.7874 - val_accuracy: 0.7600\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.6653 - accuracy: 0.8037 - val_loss: 0.7691 - val_accuracy: 0.7600\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.6002 - accuracy: 0.8288 - val_loss: 0.7498 - val_accuracy: 0.7700\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.5968 - accuracy: 0.8263 - val_loss: 0.7087 - val_accuracy: 0.7950\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.6008 - accuracy: 0.8200 - val_loss: 0.7005 - val_accuracy: 0.7900\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.5640 - accuracy: 0.8425 - val_loss: 0.7003 - val_accuracy: 0.7750\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.5066 - accuracy: 0.8462 - val_loss: 0.6936 - val_accuracy: 0.7750\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.5336 - accuracy: 0.8475 - val_loss: 0.6870 - val_accuracy: 0.7750\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.4914 - accuracy: 0.8525 - val_loss: 0.6833 - val_accuracy: 0.7600\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.4800 - accuracy: 0.8537 - val_loss: 0.6445 - val_accuracy: 0.8000\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.4172 - accuracy: 0.8750 - val_loss: 0.6216 - val_accuracy: 0.8150\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.4027 - accuracy: 0.8800 - val_loss: 0.6330 - val_accuracy: 0.7950\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 3s 67ms/step - loss: 0.4203 - accuracy: 0.8800 - val_loss: 0.6121 - val_accuracy: 0.7950\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.4197 - accuracy: 0.8763 - val_loss: 0.6411 - val_accuracy: 0.8000\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.3915 - accuracy: 0.8950 - val_loss: 0.6312 - val_accuracy: 0.8000\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.3820 - accuracy: 0.8925 - val_loss: 0.6371 - val_accuracy: 0.7900\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.3736 - accuracy: 0.8900 - val_loss: 0.6161 - val_accuracy: 0.8050\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 3s 67ms/step - loss: 0.3571 - accuracy: 0.8950 - val_loss: 0.6465 - val_accuracy: 0.7800\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.3894 - accuracy: 0.8950 - val_loss: 0.6299 - val_accuracy: 0.8000\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.3507 - accuracy: 0.8900 - val_loss: 0.6197 - val_accuracy: 0.8050\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 3s 67ms/step - loss: 0.3248 - accuracy: 0.9075 - val_loss: 0.5998 - val_accuracy: 0.7900\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.3159 - accuracy: 0.9050 - val_loss: 0.6126 - val_accuracy: 0.8050\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.3106 - accuracy: 0.8963 - val_loss: 0.6062 - val_accuracy: 0.8050\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.3170 - accuracy: 0.8988 - val_loss: 0.6035 - val_accuracy: 0.7950\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.3285 - accuracy: 0.9087 - val_loss: 0.6208 - val_accuracy: 0.8050\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.2841 - accuracy: 0.9050 - val_loss: 0.5966 - val_accuracy: 0.8100\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 3s 67ms/step - loss: 0.2976 - accuracy: 0.9100 - val_loss: 0.5977 - val_accuracy: 0.8100\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 3s 67ms/step - loss: 0.2966 - accuracy: 0.9150 - val_loss: 0.5995 - val_accuracy: 0.7850\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.2841 - accuracy: 0.9137 - val_loss: 0.5994 - val_accuracy: 0.8000\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.2614 - accuracy: 0.9362 - val_loss: 0.6292 - val_accuracy: 0.7750\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.2639 - accuracy: 0.9212 - val_loss: 0.5959 - val_accuracy: 0.7900\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 3s 67ms/step - loss: 0.2734 - accuracy: 0.9275 - val_loss: 0.6163 - val_accuracy: 0.8050\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.2443 - accuracy: 0.9250 - val_loss: 0.5875 - val_accuracy: 0.8150\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2484 - accuracy: 0.9350 - val_loss: 0.5974 - val_accuracy: 0.8000\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.2511 - accuracy: 0.9212 - val_loss: 0.5760 - val_accuracy: 0.8200\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.2464 - accuracy: 0.9262 - val_loss: 0.6047 - val_accuracy: 0.8100\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.2410 - accuracy: 0.9237 - val_loss: 0.5989 - val_accuracy: 0.8100\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2185 - accuracy: 0.9388 - val_loss: 0.6140 - val_accuracy: 0.8000\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.2117 - accuracy: 0.9438 - val_loss: 0.6095 - val_accuracy: 0.8000\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2505 - accuracy: 0.9275 - val_loss: 0.6270 - val_accuracy: 0.7850\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.2153 - accuracy: 0.9287 - val_loss: 0.6245 - val_accuracy: 0.8000\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.2240 - accuracy: 0.9362 - val_loss: 0.5943 - val_accuracy: 0.8100\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1908 - accuracy: 0.9500 - val_loss: 0.5912 - val_accuracy: 0.8100\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.2175 - accuracy: 0.9362 - val_loss: 0.5828 - val_accuracy: 0.8000\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1844 - accuracy: 0.9550 - val_loss: 0.6071 - val_accuracy: 0.8050\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1870 - accuracy: 0.9500 - val_loss: 0.5887 - val_accuracy: 0.8100\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.2092 - accuracy: 0.9463 - val_loss: 0.5858 - val_accuracy: 0.8200\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 3s 67ms/step - loss: 0.2108 - accuracy: 0.9325 - val_loss: 0.5717 - val_accuracy: 0.8250\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1551 - accuracy: 0.9700 - val_loss: 0.6319 - val_accuracy: 0.7750\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1717 - accuracy: 0.9550 - val_loss: 0.5951 - val_accuracy: 0.8000\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1748 - accuracy: 0.9475 - val_loss: 0.6299 - val_accuracy: 0.7900\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1943 - accuracy: 0.9413 - val_loss: 0.6416 - val_accuracy: 0.7800\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1741 - accuracy: 0.9475 - val_loss: 0.6295 - val_accuracy: 0.7950\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1802 - accuracy: 0.9475 - val_loss: 0.6060 - val_accuracy: 0.7950\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1761 - accuracy: 0.9425 - val_loss: 0.6103 - val_accuracy: 0.7750\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1680 - accuracy: 0.9513 - val_loss: 0.6379 - val_accuracy: 0.7850\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1810 - accuracy: 0.9500 - val_loss: 0.6489 - val_accuracy: 0.7700\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1623 - accuracy: 0.9563 - val_loss: 0.6147 - val_accuracy: 0.8050\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1530 - accuracy: 0.9525 - val_loss: 0.5869 - val_accuracy: 0.8100\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1800 - accuracy: 0.9488 - val_loss: 0.5979 - val_accuracy: 0.8150\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1542 - accuracy: 0.9600 - val_loss: 0.6175 - val_accuracy: 0.7850\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1700 - accuracy: 0.9500 - val_loss: 0.5982 - val_accuracy: 0.8050\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 3s 67ms/step - loss: 0.1580 - accuracy: 0.9588 - val_loss: 0.6260 - val_accuracy: 0.7850\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1455 - accuracy: 0.9638 - val_loss: 0.6418 - val_accuracy: 0.7750\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1509 - accuracy: 0.9625 - val_loss: 0.6073 - val_accuracy: 0.8100\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1589 - accuracy: 0.9613 - val_loss: 0.6087 - val_accuracy: 0.7900\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 3s 67ms/step - loss: 0.1562 - accuracy: 0.9513 - val_loss: 0.5982 - val_accuracy: 0.8050\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1419 - accuracy: 0.9600 - val_loss: 0.6504 - val_accuracy: 0.7750\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1315 - accuracy: 0.9600 - val_loss: 0.6581 - val_accuracy: 0.7850\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 3s 67ms/step - loss: 0.1480 - accuracy: 0.9538 - val_loss: 0.6532 - val_accuracy: 0.7750\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1344 - accuracy: 0.9638 - val_loss: 0.6314 - val_accuracy: 0.7800\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 3s 67ms/step - loss: 0.1300 - accuracy: 0.9663 - val_loss: 0.6035 - val_accuracy: 0.8100\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 3s 67ms/step - loss: 0.1226 - accuracy: 0.9663 - val_loss: 0.6272 - val_accuracy: 0.8000\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 3s 67ms/step - loss: 0.1365 - accuracy: 0.9650 - val_loss: 0.6180 - val_accuracy: 0.7850\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 3s 67ms/step - loss: 0.1491 - accuracy: 0.9563 - val_loss: 0.6197 - val_accuracy: 0.8000\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 3s 68ms/step - loss: 0.1192 - accuracy: 0.9688 - val_loss: 0.6411 - val_accuracy: 0.7900\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1347 - accuracy: 0.9563 - val_loss: 0.6243 - val_accuracy: 0.8000\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1383 - accuracy: 0.9600 - val_loss: 0.6437 - val_accuracy: 0.7900\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1368 - accuracy: 0.9613 - val_loss: 0.6515 - val_accuracy: 0.7800\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1227 - accuracy: 0.9663 - val_loss: 0.6373 - val_accuracy: 0.7850\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1051 - accuracy: 0.9712 - val_loss: 0.6199 - val_accuracy: 0.8050\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1261 - accuracy: 0.9663 - val_loss: 0.6800 - val_accuracy: 0.7700\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1036 - accuracy: 0.9775 - val_loss: 0.6398 - val_accuracy: 0.7900\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.0938 - accuracy: 0.9787 - val_loss: 0.6582 - val_accuracy: 0.7850\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1284 - accuracy: 0.9650 - val_loss: 0.6623 - val_accuracy: 0.7700\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1159 - accuracy: 0.9725 - val_loss: 0.7093 - val_accuracy: 0.7750\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 0.1090 - accuracy: 0.9638 - val_loss: 0.6154 - val_accuracy: 0.8050\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 3s 67ms/step - loss: 0.1041 - accuracy: 0.9750 - val_loss: 0.6621 - val_accuracy: 0.7700\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1094 - accuracy: 0.9762 - val_loss: 0.6472 - val_accuracy: 0.7750\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 3s 66ms/step - loss: 0.1100 - accuracy: 0.9725 - val_loss: 0.6248 - val_accuracy: 0.7900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2E72C3O30fv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f81052b0-a916-4d51-c1e5-15a7078df988"
      },
      "source": [
        "# cross-validated accuracy for pre-trained model before training\n",
        "print(\"Base model accuracy:\", np.mean(base_model_acc_list))\n",
        "# cross-validated accuracy after training\n",
        "print(\"Final accuracy:\", np.mean(final_acc_list))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Base model accuracy: 0.09000000059604644\n",
            "Final accuracy: 0.8390000104904175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dtn-03T_ZaRX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "72c8e86f-3704-4a0f-caf8-a9bce3939012"
      },
      "source": [
        "# plot graph of cross-validated accuracies\n",
        "acc = np.mean(history_map['accuracy'], axis=0)\n",
        "val_acc = np.mean(history_map['val_accuracy'], axis=0)\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftCYvBkKZmGa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "f7c1e992-0b91-41eb-a907-b5402dba52b8"
      },
      "source": [
        "# plot graph of cross-validated losses\n",
        "loss = np.mean(history_map['loss'], axis=0)\n",
        "val_loss = np.mean(history_map['val_loss'], axis=0)\n",
        "\n",
        "plt.figure(2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "#plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}