{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "materials-resnet-balanced-modified-cross-validated.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMneHEKIGCrLM6WmAXG7lQZ"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERKJLyg2z-Uq"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fIbsvD01EQm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efad92a6-b8a4-4c89-fc2a-13107d464b45"
      },
      "source": [
        "import pathlib\n",
        "\n",
        "# set path to FMD image directory\n",
        "data_dir = pathlib.Path(\"image\")\n",
        "\n",
        "# total no. of images in FMD dataset\n",
        "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
        "print(image_count)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL3rBqoe3sK5"
      },
      "source": [
        "# set batch size for training\n",
        "batch_size = 16\n",
        "\n",
        "# set dimensions to change images into for training\n",
        "# 299x299 images is used to train for consistency between different pre-trained models\n",
        "img_height = 299\n",
        "img_width = 299"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFKuhNRxqxcB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b406f9a-c3e6-4015-8908-644025e2b3e6"
      },
      "source": [
        "# get class names from the folder names in data_dir\n",
        "class_names = np.array(sorted([item.name for item in data_dir.glob('*') if item.name != \"LICENSE.txt\"]))\n",
        "print(class_names)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['fabric' 'foliage' 'glass' 'leather' 'metal' 'paper' 'plastic' 'stone'\n",
            " 'water' 'wood']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap_FpvbEqWrS"
      },
      "source": [
        "# create list containing the dataset for each class\n",
        "ds_each_class = [tf.data.Dataset.list_files(str(data_dir/f'{class_name}/*.jpg'), shuffle=False) for class_name in class_names]\n",
        "\n",
        "# shuffle the 100 images in each class with the random seed value of 123 before training\n",
        "for index, ds in enumerate(ds_each_class):\n",
        "  ds_each_class[index] = ds.shuffle(image_count//10, seed=123, reshuffle_each_iteration=False)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty_LijJpqbEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5a00ee9-7147-4012-b8a3-d5cec5af0152"
      },
      "source": [
        "# display some samples from a class to verify each class dataset contains only the class images\n",
        "for f in ds_each_class[0].take(10):\n",
        "  print(f.numpy())\n",
        "\n",
        "for f in ds_each_class[1].take(10):\n",
        "  print(f.numpy())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'FMD/image/fabric/fabric_moderate_037_new.jpg'\n",
            "b'FMD/image/fabric/fabric_moderate_004_new.jpg'\n",
            "b'FMD/image/fabric/fabric_moderate_008_new.jpg'\n",
            "b'FMD/image/fabric/fabric_object_003_new.jpg'\n",
            "b'FMD/image/fabric/fabric_object_017_new.jpg'\n",
            "b'FMD/image/fabric/fabric_object_001_new.jpg'\n",
            "b'FMD/image/fabric/fabric_moderate_032_new.jpg'\n",
            "b'FMD/image/fabric/fabric_moderate_030_new.jpg'\n",
            "b'FMD/image/fabric/fabric_moderate_038_new.jpg'\n",
            "b'FMD/image/fabric/fabric_object_009_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_037_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_004_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_008_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_053_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_067_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_051_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_032_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_030_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_038_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_059_new.jpg'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACILObxurdXN"
      },
      "source": [
        "# split first class dataset into 5 equal sized partitions\n",
        "# then for remaining classes' datasets do the same and add to corresponding partition\n",
        "# for 5-fold cross validation\n",
        "A = ds_each_class[0].shard(num_shards=5, index=0)\n",
        "B = ds_each_class[0].shard(num_shards=5, index=1)\n",
        "C = ds_each_class[0].shard(num_shards=5, index=2)\n",
        "D = ds_each_class[0].shard(num_shards=5, index=3)\n",
        "E = ds_each_class[0].shard(num_shards=5, index=4)\n",
        "for i in range(1, 10):\n",
        "  A = A.concatenate(ds_each_class[i].shard(num_shards=5, index=0))\n",
        "  B = B.concatenate(ds_each_class[i].shard(num_shards=5, index=1))\n",
        "  C = C.concatenate(ds_each_class[i].shard(num_shards=5, index=2))\n",
        "  D = D.concatenate(ds_each_class[i].shard(num_shards=5, index=3))\n",
        "  E = E.concatenate(ds_each_class[i].shard(num_shards=5, index=4))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9oYdHkhrwdz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55b3e2d1-90dd-4857-c216-fe96efec1ae9"
      },
      "source": [
        "# check no. of samples in each partition is the same\n",
        "print(A.cardinality().numpy())\n",
        "print(B.cardinality().numpy())\n",
        "print(C.cardinality().numpy())\n",
        "print(D.cardinality().numpy())\n",
        "print(E.cardinality().numpy())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BdD3FMHtGRV"
      },
      "source": [
        "def get_label(file_path):\n",
        "  # convert the path to a list of path components\n",
        "  parts = tf.strings.split(file_path, os.path.sep)\n",
        "  # The second to last is the class-directory\n",
        "  one_hot = parts[-2] == class_names\n",
        "  # Integer encode the label\n",
        "  return tf.argmax(one_hot)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfWduaL4tKDV"
      },
      "source": [
        "def decode_img(img):\n",
        "  # convert the compressed string to a 3D uint8 tensor\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\n",
        "  # resize the image to the desired size\n",
        "  return tf.image.resize(img, [img_height, img_width])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bGGe0KltMTC"
      },
      "source": [
        "def process_path(file_path):\n",
        "  label = get_label(file_path)\n",
        "  # load the raw data from the file as a string\n",
        "  img = tf.io.read_file(file_path)\n",
        "  img = decode_img(img)\n",
        "  return img, label"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcyZLwRxt1dy"
      },
      "source": [
        "# prompt the tf.data runtime to tune the number of elements to prefetch dynamically at runtime\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkBqAQlHtblh"
      },
      "source": [
        "# use the path of image to load the image into each partition of the dataset\n",
        "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
        "A = A.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "B = B.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "C = C.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "D = D.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "E = E.map(process_path, num_parallel_calls=AUTOTUNE)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc9pmaQ5t9H5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ed977be-25e3-4b27-908c-53f9c74494a1"
      },
      "source": [
        "for image, label in A.take(1):\n",
        "  print(\"Image shape: \", image.numpy().shape)\n",
        "  print(\"Label: \", label.numpy())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image shape:  (299, 299, 3)\n",
            "Label:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_T2KNdGub1X"
      },
      "source": [
        "# shuffle, batch, and prefetch the dataset\n",
        "def configure_for_performance(ds):\n",
        "  ds = ds.cache()\n",
        "  ds = ds.shuffle(buffer_size=1000)\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
        "  return ds"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWcojoVRuok5"
      },
      "source": [
        "# map the dataset partitions to integers for building training/test sets during cross-validation\n",
        "ds_fold_dict = {0:A, 1:B, 2:C, 3:D, 4:E}"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx3xKoMZU9Yv"
      },
      "source": [
        "# normalise the input values to the pre-trained model's required range of values\n",
        "preprocess_input = keras.applications.resnet_v2.preprocess_input"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVOP5fIPwEx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bf47805-2ce2-4530-83f9-4da548bef8f9"
      },
      "source": [
        "# get pre-trained model\n",
        "base_model = keras.applications.ResNet50V2(include_top=False, input_shape=(img_height, img_width, 3))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94674944/94668760 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS2kAceGVW0e"
      },
      "source": [
        "# don't train base model weights\n",
        "base_model.trainable = False"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGkReMX60ScJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e0a25ab-14c7-4fb5-8a96-12245b966b51"
      },
      "source": [
        "base_model.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"resnet50v2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 299, 299, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 305, 305, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 150, 150, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 152, 152, 64) 0           conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 75, 75, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_preact_bn (BatchNo (None, 75, 75, 64)   256         pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_preact_relu (Activ (None, 75, 75, 64)   0           conv2_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 75, 75, 64)   4096        conv2_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 75, 75, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_pad (ZeroPadding (None, 77, 77, 64)   0           conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 75, 75, 64)   36864       conv2_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 75, 75, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 75, 75, 256)  16640       conv2_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 75, 75, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Add)          (None, 75, 75, 256)  0           conv2_block1_0_conv[0][0]        \n",
            "                                                                 conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_preact_bn (BatchNo (None, 75, 75, 256)  1024        conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_preact_relu (Activ (None, 75, 75, 256)  0           conv2_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 75, 75, 64)   16384       conv2_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 75, 75, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_pad (ZeroPadding (None, 77, 77, 64)   0           conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 75, 75, 64)   36864       conv2_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 75, 75, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 75, 75, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Add)          (None, 75, 75, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_preact_bn (BatchNo (None, 75, 75, 256)  1024        conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_preact_relu (Activ (None, 75, 75, 256)  0           conv2_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 75, 75, 64)   16384       conv2_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 75, 75, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_pad (ZeroPadding (None, 77, 77, 64)   0           conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 38, 38, 64)   36864       conv2_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 38, 38, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 38, 38, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 38, 38, 256)  0           conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 38, 38, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Add)          (None, 38, 38, 256)  0           max_pooling2d[0][0]              \n",
            "                                                                 conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_preact_bn (BatchNo (None, 38, 38, 256)  1024        conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_preact_relu (Activ (None, 38, 38, 256)  0           conv3_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 38, 38, 128)  32768       conv3_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 38, 38, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_pad (ZeroPadding (None, 40, 40, 128)  0           conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 38, 38, 128)  147456      conv3_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 38, 38, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 38, 38, 512)  131584      conv3_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Add)          (None, 38, 38, 512)  0           conv3_block1_0_conv[0][0]        \n",
            "                                                                 conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_preact_bn (BatchNo (None, 38, 38, 512)  2048        conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_preact_relu (Activ (None, 38, 38, 512)  0           conv3_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 38, 38, 128)  65536       conv3_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 38, 38, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_pad (ZeroPadding (None, 40, 40, 128)  0           conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 38, 38, 128)  147456      conv3_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 38, 38, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Add)          (None, 38, 38, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_preact_bn (BatchNo (None, 38, 38, 512)  2048        conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_preact_relu (Activ (None, 38, 38, 512)  0           conv3_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 38, 38, 128)  65536       conv3_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 38, 38, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_pad (ZeroPadding (None, 40, 40, 128)  0           conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 38, 38, 128)  147456      conv3_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 38, 38, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Add)          (None, 38, 38, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_preact_bn (BatchNo (None, 38, 38, 512)  2048        conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_preact_relu (Activ (None, 38, 38, 512)  0           conv3_block4_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 38, 38, 128)  65536       conv3_block4_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 38, 38, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_pad (ZeroPadding (None, 40, 40, 128)  0           conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 19, 19, 128)  147456      conv3_block4_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 19, 19, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 19, 19, 512)  0           conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 19, 19, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Add)          (None, 19, 19, 512)  0           max_pooling2d_1[0][0]            \n",
            "                                                                 conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_preact_bn (BatchNo (None, 19, 19, 512)  2048        conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_preact_relu (Activ (None, 19, 19, 512)  0           conv4_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 19, 19, 256)  131072      conv4_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 19, 19, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 19, 19, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 19, 19, 1024) 525312      conv4_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Add)          (None, 19, 19, 1024) 0           conv4_block1_0_conv[0][0]        \n",
            "                                                                 conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 19, 19, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 19, 19, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Add)          (None, 19, 19, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 19, 19, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 19, 19, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Add)          (None, 19, 19, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block4_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block4_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 19, 19, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block4_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 19, 19, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Add)          (None, 19, 19, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block5_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block5_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 19, 19, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block5_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 19, 19, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Add)          (None, 19, 19, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block6_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block6_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 19, 19, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 10, 10, 256)  589824      conv4_block6_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 10, 10, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 10, 10, 1024) 0           conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 10, 10, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Add)          (None, 10, 10, 1024) 0           max_pooling2d_2[0][0]            \n",
            "                                                                 conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_preact_bn (BatchNo (None, 10, 10, 1024) 4096        conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_preact_relu (Activ (None, 10, 10, 1024) 0           conv5_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 10, 10, 512)  524288      conv5_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 10, 10, 512)  0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_pad (ZeroPadding (None, 12, 12, 512)  0           conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 10, 10, 512)  2359296     conv5_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 10, 10, 512)  0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 10, 10, 2048) 2099200     conv5_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 10, 10, 2048) 1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Add)          (None, 10, 10, 2048) 0           conv5_block1_0_conv[0][0]        \n",
            "                                                                 conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_preact_bn (BatchNo (None, 10, 10, 2048) 8192        conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_preact_relu (Activ (None, 10, 10, 2048) 0           conv5_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 10, 10, 512)  1048576     conv5_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 10, 10, 512)  0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_pad (ZeroPadding (None, 12, 12, 512)  0           conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 10, 10, 512)  2359296     conv5_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 10, 10, 512)  0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 10, 10, 2048) 1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Add)          (None, 10, 10, 2048) 0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_preact_bn (BatchNo (None, 10, 10, 2048) 8192        conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_preact_relu (Activ (None, 10, 10, 2048) 0           conv5_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 10, 10, 512)  1048576     conv5_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 10, 10, 512)  0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_pad (ZeroPadding (None, 12, 12, 512)  0           conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 10, 10, 512)  2359296     conv5_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 10, 10, 512)  0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 10, 10, 2048) 1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Add)          (None, 10, 10, 2048) 0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "post_bn (BatchNormalization)    (None, 10, 10, 2048) 8192        conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "post_relu (Activation)          (None, 10, 10, 2048) 0           post_bn[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 23,564,800\n",
            "Trainable params: 0\n",
            "Non-trainable params: 23,564,800\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzhaWb2aX2if"
      },
      "source": [
        "# set a low learning rate to avoid overfitting too quickly\n",
        "base_learning_rate = 0.0001\n",
        "\n",
        "# create the model to train using the pre-trained model as base model\n",
        "def create_model():\n",
        "  # generate additional training data from input training data by augmenting them using random flip, rotation & zoom\n",
        "  data_augmentation = keras.Sequential(\n",
        "    [\n",
        "      layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n",
        "                                                  input_shape=(img_height, \n",
        "                                                                img_width,\n",
        "                                                                3)),\n",
        "      layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "      layers.experimental.preprocessing.RandomZoom(0.1),\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  # average over the spatial locations to convert the features to a single vector per image\n",
        "  global_average_layer = keras.layers.GlobalAveragePooling2D()\n",
        "  # increase network depth by adding additional fully connected layer of size 128 with ReLU activation\n",
        "  fully_connected_layer = keras.layers.Dense(128, activation='relu')\n",
        "  # convert these features into a single prediction per image\n",
        "  prediction_layer = keras.layers.Dense(10)\n",
        "\n",
        "  # Build a model by chaining together the layers using the Keras Functional API.\n",
        "  inputs = keras.Input(shape=(img_height, img_width, 3))\n",
        "  x = data_augmentation(inputs)\n",
        "  x = preprocess_input(x)\n",
        "  x = base_model(x, training=False) # use training=False as the base model contains a BatchNormalization layer\n",
        "  x = global_average_layer(x)\n",
        "  x = keras.layers.Dropout(0.2)(x) # add dropout to fully connected layer to reduce overfitting\n",
        "  x = fully_connected_layer(x)\n",
        "  x = keras.layers.Dropout(0.2)(x) # add dropout to fully connected layer to reduce overfitting\n",
        "  outputs = prediction_layer(x)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  optimizer = keras.optimizers.Adam(lr=base_learning_rate)\n",
        "  loss= keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  # compile model with Adam optimizer with specified learning rate\n",
        "  model.compile(optimizer=optimizer,\n",
        "                loss=loss,\n",
        "                metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-5TEZRgY2l_"
      },
      "source": [
        "no_epochs = 100"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya698zRbu7Ep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9c173eb-e88a-49b2-b3d7-5b4dd80ba97d"
      },
      "source": [
        "# store results to plot graphs and get cross-validated accuracies\n",
        "history_map = {}\n",
        "base_model_acc_list = []\n",
        "final_acc_list = []\n",
        "\n",
        "# do 5-fold cross-validation\n",
        "for i in range(5):\n",
        "  print('fold', i + 1)\n",
        "  temp_dict = ds_fold_dict.copy()\n",
        "  # get test set for this iteration\n",
        "  current_val_ds = temp_dict[i]\n",
        "\n",
        "  # get training set for this iteration from remaining data samples\n",
        "  del temp_dict[i]\n",
        "  current_train_ds = None\n",
        "  for ds_shard in temp_dict.values():\n",
        "    if current_train_ds is None:\n",
        "      current_train_ds = ds_shard\n",
        "    else:\n",
        "      current_train_ds = current_train_ds.concatenate(ds_shard)\n",
        "  \n",
        "  # configure both training and test sets to improve performance\n",
        "  current_train_ds = configure_for_performance(current_train_ds)\n",
        "  current_val_ds = configure_for_performance(current_val_ds)\n",
        "\n",
        "  # create a new model\n",
        "  model = create_model()\n",
        "  # get initial test accuracy\n",
        "  base_model_acc_list.append(model.evaluate(current_val_ds)[1])\n",
        "  # train for specified epochs\n",
        "  history = model.fit(current_train_ds,\n",
        "                    epochs=no_epochs,\n",
        "                    validation_data=current_val_ds)\n",
        "  \n",
        "  # save results\n",
        "  if i == 0:\n",
        "    history_map['accuracy'] = [history.history['accuracy']]\n",
        "    history_map['val_accuracy'] = [history.history['val_accuracy']]\n",
        "    history_map['loss'] = [history.history['loss']]\n",
        "    history_map['val_loss'] = [history.history['val_loss']]\n",
        "  else:\n",
        "    history_map['accuracy'].append(history.history['accuracy'])\n",
        "    history_map['val_accuracy'].append(history.history['val_accuracy'])\n",
        "    history_map['loss'].append(history.history['loss'])\n",
        "    history_map['val_loss'].append(history.history['val_loss'])\n",
        "  \n",
        "  # get final test accuracy by taking the max accuracy over the whole training\n",
        "  final_acc_list.append(np.amax(history.history['val_accuracy']))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fold 1\n",
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential (Sequential)      (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv (TensorF [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub (TensorFlowO [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "resnet50v2 (Functional)      (None, 10, 10, 2048)      23564800  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               262272    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 23,828,362\n",
            "Trainable params: 263,562\n",
            "Non-trainable params: 23,564,800\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 1s 107ms/step - loss: 2.5779 - accuracy: 0.0900\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 2.4782 - accuracy: 0.1287 - val_loss: 1.9133 - val_accuracy: 0.3850\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 1.9146 - accuracy: 0.3350 - val_loss: 1.5256 - val_accuracy: 0.5850\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 1.5803 - accuracy: 0.5025 - val_loss: 1.2332 - val_accuracy: 0.6900\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 1.3229 - accuracy: 0.5738 - val_loss: 1.0223 - val_accuracy: 0.7550\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 1.1029 - accuracy: 0.6425 - val_loss: 0.8765 - val_accuracy: 0.7700\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.9891 - accuracy: 0.7100 - val_loss: 0.7782 - val_accuracy: 0.7900\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.8685 - accuracy: 0.7163 - val_loss: 0.7243 - val_accuracy: 0.7950\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.7795 - accuracy: 0.7650 - val_loss: 0.6630 - val_accuracy: 0.8050\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.7261 - accuracy: 0.7825 - val_loss: 0.6381 - val_accuracy: 0.7900\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.6848 - accuracy: 0.7975 - val_loss: 0.6120 - val_accuracy: 0.8000\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.6349 - accuracy: 0.8075 - val_loss: 0.5969 - val_accuracy: 0.8000\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.6155 - accuracy: 0.7975 - val_loss: 0.5697 - val_accuracy: 0.7950\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.5672 - accuracy: 0.8200 - val_loss: 0.5649 - val_accuracy: 0.8100\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.5035 - accuracy: 0.8512 - val_loss: 0.5480 - val_accuracy: 0.8000\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.5329 - accuracy: 0.8413 - val_loss: 0.5385 - val_accuracy: 0.8050\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 6s 112ms/step - loss: 0.4491 - accuracy: 0.8450 - val_loss: 0.5366 - val_accuracy: 0.8050\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.4480 - accuracy: 0.8687 - val_loss: 0.5095 - val_accuracy: 0.8350\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.3966 - accuracy: 0.8813 - val_loss: 0.5113 - val_accuracy: 0.8250\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.4246 - accuracy: 0.8750 - val_loss: 0.5117 - val_accuracy: 0.8250\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.4189 - accuracy: 0.8687 - val_loss: 0.5065 - val_accuracy: 0.8300\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.3676 - accuracy: 0.8913 - val_loss: 0.5127 - val_accuracy: 0.8300\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.3591 - accuracy: 0.8900 - val_loss: 0.4987 - val_accuracy: 0.8250\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.3384 - accuracy: 0.8950 - val_loss: 0.4936 - val_accuracy: 0.8300\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.3280 - accuracy: 0.8950 - val_loss: 0.4968 - val_accuracy: 0.8300\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.3006 - accuracy: 0.9175 - val_loss: 0.4957 - val_accuracy: 0.8250\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.3008 - accuracy: 0.9013 - val_loss: 0.4961 - val_accuracy: 0.8350\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.2733 - accuracy: 0.9125 - val_loss: 0.4977 - val_accuracy: 0.8300\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.2798 - accuracy: 0.9200 - val_loss: 0.4977 - val_accuracy: 0.8300\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.2806 - accuracy: 0.9087 - val_loss: 0.4840 - val_accuracy: 0.8350\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.2458 - accuracy: 0.9325 - val_loss: 0.4931 - val_accuracy: 0.8350\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.2783 - accuracy: 0.9125 - val_loss: 0.4906 - val_accuracy: 0.8250\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.2496 - accuracy: 0.9212 - val_loss: 0.5002 - val_accuracy: 0.8250\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.2599 - accuracy: 0.9137 - val_loss: 0.5082 - val_accuracy: 0.8250\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.2499 - accuracy: 0.9250 - val_loss: 0.4994 - val_accuracy: 0.8250\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.2233 - accuracy: 0.9275 - val_loss: 0.5051 - val_accuracy: 0.8350\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.2289 - accuracy: 0.9300 - val_loss: 0.4981 - val_accuracy: 0.8200\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.2064 - accuracy: 0.9362 - val_loss: 0.4981 - val_accuracy: 0.8150\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.2104 - accuracy: 0.9488 - val_loss: 0.5098 - val_accuracy: 0.8250\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1985 - accuracy: 0.9413 - val_loss: 0.5058 - val_accuracy: 0.8200\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.2030 - accuracy: 0.9425 - val_loss: 0.5127 - val_accuracy: 0.8200\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1817 - accuracy: 0.9463 - val_loss: 0.5151 - val_accuracy: 0.8250\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1782 - accuracy: 0.9463 - val_loss: 0.5120 - val_accuracy: 0.8200\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1894 - accuracy: 0.9413 - val_loss: 0.5110 - val_accuracy: 0.8300\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1857 - accuracy: 0.9388 - val_loss: 0.5358 - val_accuracy: 0.8250\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1652 - accuracy: 0.9488 - val_loss: 0.5257 - val_accuracy: 0.8150\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1800 - accuracy: 0.9513 - val_loss: 0.5343 - val_accuracy: 0.8200\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1537 - accuracy: 0.9600 - val_loss: 0.5292 - val_accuracy: 0.8300\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1635 - accuracy: 0.9550 - val_loss: 0.5367 - val_accuracy: 0.8350\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1649 - accuracy: 0.9538 - val_loss: 0.5304 - val_accuracy: 0.8350\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1419 - accuracy: 0.9625 - val_loss: 0.5283 - val_accuracy: 0.8200\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1464 - accuracy: 0.9538 - val_loss: 0.5303 - val_accuracy: 0.8200\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1667 - accuracy: 0.9538 - val_loss: 0.5379 - val_accuracy: 0.8200\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1546 - accuracy: 0.9550 - val_loss: 0.5300 - val_accuracy: 0.8200\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1389 - accuracy: 0.9625 - val_loss: 0.5362 - val_accuracy: 0.8200\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1575 - accuracy: 0.9625 - val_loss: 0.5262 - val_accuracy: 0.8150\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1322 - accuracy: 0.9650 - val_loss: 0.5506 - val_accuracy: 0.8200\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1248 - accuracy: 0.9650 - val_loss: 0.5537 - val_accuracy: 0.8150\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1271 - accuracy: 0.9688 - val_loss: 0.5519 - val_accuracy: 0.8150\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1442 - accuracy: 0.9525 - val_loss: 0.5441 - val_accuracy: 0.8150\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1418 - accuracy: 0.9588 - val_loss: 0.5461 - val_accuracy: 0.8150\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1224 - accuracy: 0.9650 - val_loss: 0.5476 - val_accuracy: 0.8200\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1537 - accuracy: 0.9513 - val_loss: 0.5386 - val_accuracy: 0.8150\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1411 - accuracy: 0.9613 - val_loss: 0.5463 - val_accuracy: 0.8200\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1142 - accuracy: 0.9688 - val_loss: 0.5541 - val_accuracy: 0.8250\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1253 - accuracy: 0.9625 - val_loss: 0.5616 - val_accuracy: 0.8200\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1283 - accuracy: 0.9588 - val_loss: 0.5686 - val_accuracy: 0.8100\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1021 - accuracy: 0.9787 - val_loss: 0.5672 - val_accuracy: 0.8150\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1051 - accuracy: 0.9750 - val_loss: 0.5643 - val_accuracy: 0.8200\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1024 - accuracy: 0.9725 - val_loss: 0.5614 - val_accuracy: 0.8150\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1053 - accuracy: 0.9650 - val_loss: 0.5809 - val_accuracy: 0.8150\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0852 - accuracy: 0.9812 - val_loss: 0.5630 - val_accuracy: 0.8200\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0919 - accuracy: 0.9787 - val_loss: 0.5678 - val_accuracy: 0.8200\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 6s 112ms/step - loss: 0.0901 - accuracy: 0.9837 - val_loss: 0.5610 - val_accuracy: 0.8250\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 6s 112ms/step - loss: 0.0924 - accuracy: 0.9762 - val_loss: 0.5696 - val_accuracy: 0.8200\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0951 - accuracy: 0.9700 - val_loss: 0.5609 - val_accuracy: 0.8200\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0914 - accuracy: 0.9750 - val_loss: 0.5686 - val_accuracy: 0.8200\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0818 - accuracy: 0.9750 - val_loss: 0.5658 - val_accuracy: 0.8200\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1086 - accuracy: 0.9663 - val_loss: 0.5578 - val_accuracy: 0.8250\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.0858 - accuracy: 0.9762 - val_loss: 0.5803 - val_accuracy: 0.8250\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1060 - accuracy: 0.9787 - val_loss: 0.5831 - val_accuracy: 0.8200\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0865 - accuracy: 0.9737 - val_loss: 0.5707 - val_accuracy: 0.8250\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0912 - accuracy: 0.9775 - val_loss: 0.5729 - val_accuracy: 0.8250\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0788 - accuracy: 0.9787 - val_loss: 0.5746 - val_accuracy: 0.8250\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.0909 - accuracy: 0.9762 - val_loss: 0.5688 - val_accuracy: 0.8250\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1109 - accuracy: 0.9675 - val_loss: 0.5982 - val_accuracy: 0.8100\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0792 - accuracy: 0.9787 - val_loss: 0.5939 - val_accuracy: 0.8250\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0759 - accuracy: 0.9850 - val_loss: 0.5654 - val_accuracy: 0.8200\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0766 - accuracy: 0.9825 - val_loss: 0.5776 - val_accuracy: 0.8300\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0780 - accuracy: 0.9775 - val_loss: 0.5589 - val_accuracy: 0.8250\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0641 - accuracy: 0.9862 - val_loss: 0.5559 - val_accuracy: 0.8150\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0669 - accuracy: 0.9875 - val_loss: 0.5630 - val_accuracy: 0.8200\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0656 - accuracy: 0.9862 - val_loss: 0.5752 - val_accuracy: 0.8200\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0785 - accuracy: 0.9800 - val_loss: 0.5758 - val_accuracy: 0.8400\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0706 - accuracy: 0.9825 - val_loss: 0.5869 - val_accuracy: 0.8250\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0825 - accuracy: 0.9737 - val_loss: 0.5704 - val_accuracy: 0.8350\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0736 - accuracy: 0.9812 - val_loss: 0.5708 - val_accuracy: 0.8250\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0628 - accuracy: 0.9850 - val_loss: 0.5696 - val_accuracy: 0.8300\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 6s 112ms/step - loss: 0.0778 - accuracy: 0.9750 - val_loss: 0.5708 - val_accuracy: 0.8250\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 6s 112ms/step - loss: 0.0525 - accuracy: 0.9887 - val_loss: 0.5733 - val_accuracy: 0.8200\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 6s 112ms/step - loss: 0.0757 - accuracy: 0.9800 - val_loss: 0.5831 - val_accuracy: 0.8250\n",
            "fold 2\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_1 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_1 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_1 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "resnet50v2 (Functional)      (None, 10, 10, 2048)      23564800  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               262272    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 23,828,362\n",
            "Trainable params: 263,562\n",
            "Non-trainable params: 23,564,800\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 1s 79ms/step - loss: 2.4767 - accuracy: 0.1200\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 2.4019 - accuracy: 0.1650 - val_loss: 1.8164 - val_accuracy: 0.4550\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 6s 112ms/step - loss: 1.8766 - accuracy: 0.3625 - val_loss: 1.4499 - val_accuracy: 0.6300\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 1.5278 - accuracy: 0.5075 - val_loss: 1.1743 - val_accuracy: 0.6800\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 1.2472 - accuracy: 0.6050 - val_loss: 0.9732 - val_accuracy: 0.7250\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 1.1004 - accuracy: 0.6675 - val_loss: 0.8519 - val_accuracy: 0.7600\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.9496 - accuracy: 0.6975 - val_loss: 0.7860 - val_accuracy: 0.7700\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.8339 - accuracy: 0.7237 - val_loss: 0.7137 - val_accuracy: 0.7850\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.7840 - accuracy: 0.7387 - val_loss: 0.6702 - val_accuracy: 0.8050\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.7054 - accuracy: 0.7688 - val_loss: 0.6467 - val_accuracy: 0.8100\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.6480 - accuracy: 0.7975 - val_loss: 0.6186 - val_accuracy: 0.8000\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.6214 - accuracy: 0.8037 - val_loss: 0.6028 - val_accuracy: 0.8050\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 6s 112ms/step - loss: 0.5670 - accuracy: 0.8288 - val_loss: 0.5757 - val_accuracy: 0.8150\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.5596 - accuracy: 0.8075 - val_loss: 0.5642 - val_accuracy: 0.8150\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.5217 - accuracy: 0.8275 - val_loss: 0.5464 - val_accuracy: 0.8150\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.5327 - accuracy: 0.8338 - val_loss: 0.5375 - val_accuracy: 0.8150\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.4380 - accuracy: 0.8675 - val_loss: 0.5316 - val_accuracy: 0.8100\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.4422 - accuracy: 0.8687 - val_loss: 0.5251 - val_accuracy: 0.8150\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.4388 - accuracy: 0.8500 - val_loss: 0.5204 - val_accuracy: 0.8250\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.4000 - accuracy: 0.8750 - val_loss: 0.5152 - val_accuracy: 0.8150\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.3627 - accuracy: 0.8737 - val_loss: 0.5128 - val_accuracy: 0.8350\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.3844 - accuracy: 0.8775 - val_loss: 0.5096 - val_accuracy: 0.8350\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.3758 - accuracy: 0.8687 - val_loss: 0.5016 - val_accuracy: 0.8500\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.3302 - accuracy: 0.9075 - val_loss: 0.5014 - val_accuracy: 0.8500\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.3729 - accuracy: 0.8725 - val_loss: 0.5034 - val_accuracy: 0.8450\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.3000 - accuracy: 0.9150 - val_loss: 0.5023 - val_accuracy: 0.8350\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.2747 - accuracy: 0.9287 - val_loss: 0.5015 - val_accuracy: 0.8400\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.3105 - accuracy: 0.9038 - val_loss: 0.4948 - val_accuracy: 0.8400\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.2780 - accuracy: 0.9162 - val_loss: 0.4950 - val_accuracy: 0.8450\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 6s 112ms/step - loss: 0.2931 - accuracy: 0.9038 - val_loss: 0.4994 - val_accuracy: 0.8300\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.2498 - accuracy: 0.9262 - val_loss: 0.4952 - val_accuracy: 0.8450\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.2522 - accuracy: 0.9200 - val_loss: 0.4922 - val_accuracy: 0.8350\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.2583 - accuracy: 0.9212 - val_loss: 0.4926 - val_accuracy: 0.8450\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 6s 112ms/step - loss: 0.2382 - accuracy: 0.9262 - val_loss: 0.4949 - val_accuracy: 0.8450\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.2232 - accuracy: 0.9237 - val_loss: 0.4965 - val_accuracy: 0.8300\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.2252 - accuracy: 0.9362 - val_loss: 0.4972 - val_accuracy: 0.8300\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.2209 - accuracy: 0.9388 - val_loss: 0.4940 - val_accuracy: 0.8300\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.2308 - accuracy: 0.9275 - val_loss: 0.4917 - val_accuracy: 0.8350\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1954 - accuracy: 0.9513 - val_loss: 0.4908 - val_accuracy: 0.8350\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.2165 - accuracy: 0.9325 - val_loss: 0.4938 - val_accuracy: 0.8350\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1994 - accuracy: 0.9362 - val_loss: 0.5000 - val_accuracy: 0.8350\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 6s 112ms/step - loss: 0.1907 - accuracy: 0.9463 - val_loss: 0.5043 - val_accuracy: 0.8450\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1959 - accuracy: 0.9375 - val_loss: 0.4949 - val_accuracy: 0.8500\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.2158 - accuracy: 0.9300 - val_loss: 0.4916 - val_accuracy: 0.8400\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1936 - accuracy: 0.9538 - val_loss: 0.4917 - val_accuracy: 0.8450\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1604 - accuracy: 0.9613 - val_loss: 0.4911 - val_accuracy: 0.8350\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1680 - accuracy: 0.9600 - val_loss: 0.4875 - val_accuracy: 0.8500\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1788 - accuracy: 0.9525 - val_loss: 0.4832 - val_accuracy: 0.8550\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1543 - accuracy: 0.9600 - val_loss: 0.4948 - val_accuracy: 0.8550\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1517 - accuracy: 0.9575 - val_loss: 0.4893 - val_accuracy: 0.8550\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1626 - accuracy: 0.9513 - val_loss: 0.4882 - val_accuracy: 0.8250\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1562 - accuracy: 0.9488 - val_loss: 0.4862 - val_accuracy: 0.8500\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1607 - accuracy: 0.9525 - val_loss: 0.4857 - val_accuracy: 0.8500\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1341 - accuracy: 0.9663 - val_loss: 0.4867 - val_accuracy: 0.8450\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1449 - accuracy: 0.9625 - val_loss: 0.4894 - val_accuracy: 0.8450\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1559 - accuracy: 0.9488 - val_loss: 0.4882 - val_accuracy: 0.8350\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1197 - accuracy: 0.9725 - val_loss: 0.4930 - val_accuracy: 0.8250\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1213 - accuracy: 0.9750 - val_loss: 0.4867 - val_accuracy: 0.8400\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1248 - accuracy: 0.9663 - val_loss: 0.4961 - val_accuracy: 0.8250\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1067 - accuracy: 0.9800 - val_loss: 0.5026 - val_accuracy: 0.8400\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1548 - accuracy: 0.9563 - val_loss: 0.5046 - val_accuracy: 0.8350\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1055 - accuracy: 0.9762 - val_loss: 0.5123 - val_accuracy: 0.8450\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1049 - accuracy: 0.9725 - val_loss: 0.5077 - val_accuracy: 0.8550\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1105 - accuracy: 0.9700 - val_loss: 0.5087 - val_accuracy: 0.8400\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1129 - accuracy: 0.9712 - val_loss: 0.5136 - val_accuracy: 0.8300\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1163 - accuracy: 0.9700 - val_loss: 0.4972 - val_accuracy: 0.8350\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1192 - accuracy: 0.9600 - val_loss: 0.5039 - val_accuracy: 0.8300\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 6s 112ms/step - loss: 0.0777 - accuracy: 0.9912 - val_loss: 0.5134 - val_accuracy: 0.8300\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 6s 112ms/step - loss: 0.1199 - accuracy: 0.9675 - val_loss: 0.5100 - val_accuracy: 0.8300\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1120 - accuracy: 0.9688 - val_loss: 0.5036 - val_accuracy: 0.8200\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0985 - accuracy: 0.9725 - val_loss: 0.5151 - val_accuracy: 0.8350\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1286 - accuracy: 0.9588 - val_loss: 0.5064 - val_accuracy: 0.8300\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0929 - accuracy: 0.9787 - val_loss: 0.5155 - val_accuracy: 0.8300\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1013 - accuracy: 0.9688 - val_loss: 0.5099 - val_accuracy: 0.8300\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1004 - accuracy: 0.9737 - val_loss: 0.5108 - val_accuracy: 0.8300\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0988 - accuracy: 0.9737 - val_loss: 0.5037 - val_accuracy: 0.8300\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1065 - accuracy: 0.9663 - val_loss: 0.5019 - val_accuracy: 0.8400\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.0965 - accuracy: 0.9775 - val_loss: 0.5072 - val_accuracy: 0.8400\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0889 - accuracy: 0.9787 - val_loss: 0.5073 - val_accuracy: 0.8300\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1002 - accuracy: 0.9737 - val_loss: 0.5206 - val_accuracy: 0.8300\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.0942 - accuracy: 0.9737 - val_loss: 0.5109 - val_accuracy: 0.8250\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1066 - accuracy: 0.9712 - val_loss: 0.5183 - val_accuracy: 0.8400\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0732 - accuracy: 0.9825 - val_loss: 0.5221 - val_accuracy: 0.8450\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0735 - accuracy: 0.9825 - val_loss: 0.5249 - val_accuracy: 0.8450\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.1051 - accuracy: 0.9675 - val_loss: 0.5272 - val_accuracy: 0.8500\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0894 - accuracy: 0.9737 - val_loss: 0.5317 - val_accuracy: 0.8200\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0797 - accuracy: 0.9825 - val_loss: 0.5287 - val_accuracy: 0.8300\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0738 - accuracy: 0.9812 - val_loss: 0.5297 - val_accuracy: 0.8400\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0886 - accuracy: 0.9762 - val_loss: 0.5336 - val_accuracy: 0.8100\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0997 - accuracy: 0.9712 - val_loss: 0.5339 - val_accuracy: 0.8250\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0692 - accuracy: 0.9862 - val_loss: 0.5422 - val_accuracy: 0.8250\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0927 - accuracy: 0.9725 - val_loss: 0.5427 - val_accuracy: 0.8300\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0823 - accuracy: 0.9812 - val_loss: 0.5364 - val_accuracy: 0.8300\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0709 - accuracy: 0.9812 - val_loss: 0.5512 - val_accuracy: 0.8400\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0640 - accuracy: 0.9837 - val_loss: 0.5459 - val_accuracy: 0.8300\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0800 - accuracy: 0.9750 - val_loss: 0.5496 - val_accuracy: 0.8300\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0736 - accuracy: 0.9800 - val_loss: 0.5427 - val_accuracy: 0.8450\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.0712 - accuracy: 0.9875 - val_loss: 0.5416 - val_accuracy: 0.8450\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.0778 - accuracy: 0.9775 - val_loss: 0.5311 - val_accuracy: 0.8400\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0804 - accuracy: 0.9800 - val_loss: 0.5370 - val_accuracy: 0.8350\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.0577 - accuracy: 0.9900 - val_loss: 0.5351 - val_accuracy: 0.8250\n",
            "fold 3\n",
            "Model: \"functional_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_2 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_2 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_2 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "resnet50v2 (Functional)      (None, 10, 10, 2048)      23564800  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               262272    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 23,828,362\n",
            "Trainable params: 263,562\n",
            "Non-trainable params: 23,564,800\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 1s 77ms/step - loss: 2.7053 - accuracy: 0.0850\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 2.4100 - accuracy: 0.1725 - val_loss: 1.9873 - val_accuracy: 0.3350\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 1.8787 - accuracy: 0.3625 - val_loss: 1.6147 - val_accuracy: 0.5400\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 1.5244 - accuracy: 0.4950 - val_loss: 1.3506 - val_accuracy: 0.6200\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 1.3060 - accuracy: 0.6012 - val_loss: 1.1480 - val_accuracy: 0.6850\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 1.0997 - accuracy: 0.6675 - val_loss: 1.0065 - val_accuracy: 0.7100\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.9096 - accuracy: 0.7312 - val_loss: 0.9129 - val_accuracy: 0.7300\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.8467 - accuracy: 0.7225 - val_loss: 0.8422 - val_accuracy: 0.7500\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.7769 - accuracy: 0.7775 - val_loss: 0.7844 - val_accuracy: 0.7650\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.7006 - accuracy: 0.7937 - val_loss: 0.7491 - val_accuracy: 0.7600\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.6564 - accuracy: 0.7987 - val_loss: 0.7216 - val_accuracy: 0.7550\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.6388 - accuracy: 0.8012 - val_loss: 0.7102 - val_accuracy: 0.7600\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.5614 - accuracy: 0.8188 - val_loss: 0.6833 - val_accuracy: 0.7600\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.4970 - accuracy: 0.8462 - val_loss: 0.6821 - val_accuracy: 0.7500\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.5319 - accuracy: 0.8400 - val_loss: 0.6816 - val_accuracy: 0.7650\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.4544 - accuracy: 0.8650 - val_loss: 0.6735 - val_accuracy: 0.7600\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.4578 - accuracy: 0.8500 - val_loss: 0.6644 - val_accuracy: 0.7650\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.4624 - accuracy: 0.8537 - val_loss: 0.6503 - val_accuracy: 0.7550\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.3885 - accuracy: 0.8863 - val_loss: 0.6497 - val_accuracy: 0.7550\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.4108 - accuracy: 0.8700 - val_loss: 0.6413 - val_accuracy: 0.7500\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.3944 - accuracy: 0.8813 - val_loss: 0.6533 - val_accuracy: 0.7500\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.3488 - accuracy: 0.9000 - val_loss: 0.6420 - val_accuracy: 0.7550\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.3227 - accuracy: 0.8950 - val_loss: 0.6416 - val_accuracy: 0.7700\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.3112 - accuracy: 0.9112 - val_loss: 0.6269 - val_accuracy: 0.7750\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.3240 - accuracy: 0.9038 - val_loss: 0.6380 - val_accuracy: 0.7700\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.3418 - accuracy: 0.8925 - val_loss: 0.6120 - val_accuracy: 0.7800\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.2952 - accuracy: 0.9137 - val_loss: 0.6126 - val_accuracy: 0.7750\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.3044 - accuracy: 0.9000 - val_loss: 0.6072 - val_accuracy: 0.7850\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.3035 - accuracy: 0.9087 - val_loss: 0.6121 - val_accuracy: 0.7800\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.2752 - accuracy: 0.9100 - val_loss: 0.6164 - val_accuracy: 0.8000\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.2561 - accuracy: 0.9262 - val_loss: 0.6196 - val_accuracy: 0.7950\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.2620 - accuracy: 0.9187 - val_loss: 0.6171 - val_accuracy: 0.7850\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.2504 - accuracy: 0.9325 - val_loss: 0.6185 - val_accuracy: 0.7900\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.2503 - accuracy: 0.9200 - val_loss: 0.6268 - val_accuracy: 0.7850\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.2199 - accuracy: 0.9300 - val_loss: 0.6248 - val_accuracy: 0.7950\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.2375 - accuracy: 0.9300 - val_loss: 0.6227 - val_accuracy: 0.7950\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.2088 - accuracy: 0.9438 - val_loss: 0.6218 - val_accuracy: 0.7850\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.2032 - accuracy: 0.9425 - val_loss: 0.6142 - val_accuracy: 0.7950\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1976 - accuracy: 0.9463 - val_loss: 0.6175 - val_accuracy: 0.7850\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.2280 - accuracy: 0.9237 - val_loss: 0.6163 - val_accuracy: 0.7900\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1862 - accuracy: 0.9513 - val_loss: 0.6323 - val_accuracy: 0.7900\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1916 - accuracy: 0.9413 - val_loss: 0.6212 - val_accuracy: 0.7800\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1921 - accuracy: 0.9413 - val_loss: 0.6270 - val_accuracy: 0.7900\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1928 - accuracy: 0.9400 - val_loss: 0.6313 - val_accuracy: 0.7900\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1830 - accuracy: 0.9388 - val_loss: 0.6257 - val_accuracy: 0.7900\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1896 - accuracy: 0.9425 - val_loss: 0.6236 - val_accuracy: 0.7900\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1394 - accuracy: 0.9663 - val_loss: 0.6436 - val_accuracy: 0.7750\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1551 - accuracy: 0.9525 - val_loss: 0.6437 - val_accuracy: 0.7900\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1692 - accuracy: 0.9525 - val_loss: 0.6519 - val_accuracy: 0.7850\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1593 - accuracy: 0.9513 - val_loss: 0.6479 - val_accuracy: 0.7900\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1573 - accuracy: 0.9513 - val_loss: 0.6614 - val_accuracy: 0.7750\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1506 - accuracy: 0.9613 - val_loss: 0.6535 - val_accuracy: 0.7850\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1514 - accuracy: 0.9538 - val_loss: 0.6493 - val_accuracy: 0.7850\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1453 - accuracy: 0.9575 - val_loss: 0.6438 - val_accuracy: 0.7850\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1294 - accuracy: 0.9638 - val_loss: 0.6630 - val_accuracy: 0.7850\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1567 - accuracy: 0.9475 - val_loss: 0.6329 - val_accuracy: 0.7850\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1386 - accuracy: 0.9625 - val_loss: 0.6392 - val_accuracy: 0.7850\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1254 - accuracy: 0.9688 - val_loss: 0.6432 - val_accuracy: 0.7900\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1204 - accuracy: 0.9712 - val_loss: 0.6461 - val_accuracy: 0.7900\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1327 - accuracy: 0.9600 - val_loss: 0.6392 - val_accuracy: 0.7950\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1391 - accuracy: 0.9550 - val_loss: 0.6396 - val_accuracy: 0.7950\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.1032 - accuracy: 0.9787 - val_loss: 0.6430 - val_accuracy: 0.7800\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1271 - accuracy: 0.9688 - val_loss: 0.6518 - val_accuracy: 0.7900\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1154 - accuracy: 0.9675 - val_loss: 0.6526 - val_accuracy: 0.7950\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1118 - accuracy: 0.9663 - val_loss: 0.6407 - val_accuracy: 0.7900\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1055 - accuracy: 0.9750 - val_loss: 0.6575 - val_accuracy: 0.7750\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1111 - accuracy: 0.9675 - val_loss: 0.6571 - val_accuracy: 0.7900\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.0993 - accuracy: 0.9737 - val_loss: 0.6519 - val_accuracy: 0.7900\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1056 - accuracy: 0.9712 - val_loss: 0.6674 - val_accuracy: 0.7850\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1047 - accuracy: 0.9750 - val_loss: 0.6602 - val_accuracy: 0.7850\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1014 - accuracy: 0.9750 - val_loss: 0.6600 - val_accuracy: 0.7850\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1001 - accuracy: 0.9775 - val_loss: 0.6601 - val_accuracy: 0.7950\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0987 - accuracy: 0.9762 - val_loss: 0.6705 - val_accuracy: 0.7900\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0874 - accuracy: 0.9800 - val_loss: 0.6703 - val_accuracy: 0.7900\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0993 - accuracy: 0.9737 - val_loss: 0.6537 - val_accuracy: 0.7850\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.0974 - accuracy: 0.9737 - val_loss: 0.6626 - val_accuracy: 0.7900\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0869 - accuracy: 0.9775 - val_loss: 0.6535 - val_accuracy: 0.7900\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0872 - accuracy: 0.9737 - val_loss: 0.6644 - val_accuracy: 0.7950\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.0881 - accuracy: 0.9812 - val_loss: 0.6522 - val_accuracy: 0.7900\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0807 - accuracy: 0.9762 - val_loss: 0.6642 - val_accuracy: 0.7900\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0805 - accuracy: 0.9775 - val_loss: 0.6561 - val_accuracy: 0.7900\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0784 - accuracy: 0.9800 - val_loss: 0.6773 - val_accuracy: 0.7900\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0810 - accuracy: 0.9800 - val_loss: 0.6645 - val_accuracy: 0.7900\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0705 - accuracy: 0.9850 - val_loss: 0.6629 - val_accuracy: 0.7950\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0973 - accuracy: 0.9712 - val_loss: 0.6662 - val_accuracy: 0.7950\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0811 - accuracy: 0.9837 - val_loss: 0.6650 - val_accuracy: 0.7900\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0672 - accuracy: 0.9850 - val_loss: 0.6734 - val_accuracy: 0.7800\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0805 - accuracy: 0.9800 - val_loss: 0.6694 - val_accuracy: 0.7850\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0685 - accuracy: 0.9812 - val_loss: 0.6717 - val_accuracy: 0.7850\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.0817 - accuracy: 0.9825 - val_loss: 0.6706 - val_accuracy: 0.7900\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0796 - accuracy: 0.9812 - val_loss: 0.6856 - val_accuracy: 0.7850\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0730 - accuracy: 0.9825 - val_loss: 0.6711 - val_accuracy: 0.7850\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0780 - accuracy: 0.9825 - val_loss: 0.6686 - val_accuracy: 0.7800\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0957 - accuracy: 0.9725 - val_loss: 0.6677 - val_accuracy: 0.7900\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0641 - accuracy: 0.9862 - val_loss: 0.6694 - val_accuracy: 0.7950\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0842 - accuracy: 0.9812 - val_loss: 0.6723 - val_accuracy: 0.7800\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0644 - accuracy: 0.9825 - val_loss: 0.6668 - val_accuracy: 0.7850\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0724 - accuracy: 0.9800 - val_loss: 0.6574 - val_accuracy: 0.7800\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0764 - accuracy: 0.9787 - val_loss: 0.6695 - val_accuracy: 0.7900\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0676 - accuracy: 0.9800 - val_loss: 0.6911 - val_accuracy: 0.7750\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0639 - accuracy: 0.9850 - val_loss: 0.6887 - val_accuracy: 0.7800\n",
            "fold 4\n",
            "Model: \"functional_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_3 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_3 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_3 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "resnet50v2 (Functional)      (None, 10, 10, 2048)      23564800  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_3 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               262272    \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 23,828,362\n",
            "Trainable params: 263,562\n",
            "Non-trainable params: 23,564,800\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 1s 79ms/step - loss: 2.4941 - accuracy: 0.1200\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 2.3724 - accuracy: 0.1887 - val_loss: 1.8788 - val_accuracy: 0.3300\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 1.8540 - accuracy: 0.3837 - val_loss: 1.4915 - val_accuracy: 0.5500\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 1.5025 - accuracy: 0.5362 - val_loss: 1.2026 - val_accuracy: 0.6750\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 1.2607 - accuracy: 0.6000 - val_loss: 0.9879 - val_accuracy: 0.7550\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 1.0868 - accuracy: 0.6513 - val_loss: 0.8567 - val_accuracy: 0.7700\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.9267 - accuracy: 0.7300 - val_loss: 0.7605 - val_accuracy: 0.7900\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.8669 - accuracy: 0.7287 - val_loss: 0.6970 - val_accuracy: 0.7950\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.6948 - accuracy: 0.8000 - val_loss: 0.6407 - val_accuracy: 0.7850\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.6844 - accuracy: 0.7837 - val_loss: 0.5993 - val_accuracy: 0.8000\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.6695 - accuracy: 0.8050 - val_loss: 0.5670 - val_accuracy: 0.8150\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.6056 - accuracy: 0.8087 - val_loss: 0.5597 - val_accuracy: 0.8250\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.5518 - accuracy: 0.8363 - val_loss: 0.5476 - val_accuracy: 0.8400\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.5405 - accuracy: 0.8100 - val_loss: 0.5275 - val_accuracy: 0.8350\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 6s 114ms/step - loss: 0.4854 - accuracy: 0.8487 - val_loss: 0.5169 - val_accuracy: 0.8250\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.5166 - accuracy: 0.8338 - val_loss: 0.5104 - val_accuracy: 0.8450\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.4664 - accuracy: 0.8525 - val_loss: 0.5015 - val_accuracy: 0.8400\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.4354 - accuracy: 0.8662 - val_loss: 0.4827 - val_accuracy: 0.8550\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.3885 - accuracy: 0.8800 - val_loss: 0.4820 - val_accuracy: 0.8500\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.3799 - accuracy: 0.8900 - val_loss: 0.4788 - val_accuracy: 0.8550\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.3984 - accuracy: 0.8687 - val_loss: 0.4764 - val_accuracy: 0.8600\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.3525 - accuracy: 0.8913 - val_loss: 0.4605 - val_accuracy: 0.8600\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.3378 - accuracy: 0.8938 - val_loss: 0.4580 - val_accuracy: 0.8650\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.3059 - accuracy: 0.9187 - val_loss: 0.4526 - val_accuracy: 0.8650\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.3431 - accuracy: 0.8863 - val_loss: 0.4386 - val_accuracy: 0.8700\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.3109 - accuracy: 0.9050 - val_loss: 0.4325 - val_accuracy: 0.8600\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.2954 - accuracy: 0.9100 - val_loss: 0.4264 - val_accuracy: 0.8650\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.2847 - accuracy: 0.9125 - val_loss: 0.4281 - val_accuracy: 0.8700\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.2866 - accuracy: 0.9175 - val_loss: 0.4265 - val_accuracy: 0.8600\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.2533 - accuracy: 0.9137 - val_loss: 0.4391 - val_accuracy: 0.8450\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.2702 - accuracy: 0.9112 - val_loss: 0.4222 - val_accuracy: 0.8600\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.2395 - accuracy: 0.9237 - val_loss: 0.4183 - val_accuracy: 0.8700\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.2421 - accuracy: 0.9300 - val_loss: 0.4285 - val_accuracy: 0.8750\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.2330 - accuracy: 0.9312 - val_loss: 0.4209 - val_accuracy: 0.8800\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.2324 - accuracy: 0.9300 - val_loss: 0.4104 - val_accuracy: 0.8550\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1975 - accuracy: 0.9500 - val_loss: 0.4218 - val_accuracy: 0.8600\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.2451 - accuracy: 0.9250 - val_loss: 0.4283 - val_accuracy: 0.8750\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.2018 - accuracy: 0.9488 - val_loss: 0.4156 - val_accuracy: 0.8800\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1980 - accuracy: 0.9438 - val_loss: 0.4095 - val_accuracy: 0.8800\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.2031 - accuracy: 0.9337 - val_loss: 0.4014 - val_accuracy: 0.8800\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1950 - accuracy: 0.9513 - val_loss: 0.4026 - val_accuracy: 0.8750\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1839 - accuracy: 0.9513 - val_loss: 0.4070 - val_accuracy: 0.8700\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1716 - accuracy: 0.9463 - val_loss: 0.4153 - val_accuracy: 0.8650\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1860 - accuracy: 0.9525 - val_loss: 0.4130 - val_accuracy: 0.8700\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1774 - accuracy: 0.9438 - val_loss: 0.4093 - val_accuracy: 0.8700\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1703 - accuracy: 0.9513 - val_loss: 0.4096 - val_accuracy: 0.8650\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1810 - accuracy: 0.9450 - val_loss: 0.4125 - val_accuracy: 0.8650\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1752 - accuracy: 0.9563 - val_loss: 0.4359 - val_accuracy: 0.8700\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1283 - accuracy: 0.9688 - val_loss: 0.4254 - val_accuracy: 0.8700\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1878 - accuracy: 0.9413 - val_loss: 0.4170 - val_accuracy: 0.8700\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1458 - accuracy: 0.9688 - val_loss: 0.4138 - val_accuracy: 0.8700\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1547 - accuracy: 0.9575 - val_loss: 0.4199 - val_accuracy: 0.8700\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1278 - accuracy: 0.9750 - val_loss: 0.4098 - val_accuracy: 0.8700\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1141 - accuracy: 0.9750 - val_loss: 0.4173 - val_accuracy: 0.8700\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1490 - accuracy: 0.9538 - val_loss: 0.4307 - val_accuracy: 0.8650\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1367 - accuracy: 0.9638 - val_loss: 0.4228 - val_accuracy: 0.8550\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1237 - accuracy: 0.9688 - val_loss: 0.4339 - val_accuracy: 0.8650\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1320 - accuracy: 0.9588 - val_loss: 0.4261 - val_accuracy: 0.8750\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1329 - accuracy: 0.9625 - val_loss: 0.4319 - val_accuracy: 0.8700\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1258 - accuracy: 0.9600 - val_loss: 0.4487 - val_accuracy: 0.8500\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1177 - accuracy: 0.9688 - val_loss: 0.4414 - val_accuracy: 0.8650\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1312 - accuracy: 0.9625 - val_loss: 0.4373 - val_accuracy: 0.8550\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1117 - accuracy: 0.9712 - val_loss: 0.4435 - val_accuracy: 0.8600\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1249 - accuracy: 0.9638 - val_loss: 0.4361 - val_accuracy: 0.8600\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1115 - accuracy: 0.9700 - val_loss: 0.4338 - val_accuracy: 0.8600\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1080 - accuracy: 0.9725 - val_loss: 0.4290 - val_accuracy: 0.8700\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1363 - accuracy: 0.9575 - val_loss: 0.4458 - val_accuracy: 0.8600\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1151 - accuracy: 0.9712 - val_loss: 0.4471 - val_accuracy: 0.8650\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 6s 117ms/step - loss: 0.1153 - accuracy: 0.9688 - val_loss: 0.4424 - val_accuracy: 0.8650\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1011 - accuracy: 0.9762 - val_loss: 0.4525 - val_accuracy: 0.8650\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0994 - accuracy: 0.9712 - val_loss: 0.4383 - val_accuracy: 0.8600\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1062 - accuracy: 0.9725 - val_loss: 0.4448 - val_accuracy: 0.8550\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1003 - accuracy: 0.9650 - val_loss: 0.4441 - val_accuracy: 0.8650\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0894 - accuracy: 0.9850 - val_loss: 0.4593 - val_accuracy: 0.8550\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1154 - accuracy: 0.9650 - val_loss: 0.4589 - val_accuracy: 0.8650\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 6s 117ms/step - loss: 0.1022 - accuracy: 0.9750 - val_loss: 0.4622 - val_accuracy: 0.8550\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0799 - accuracy: 0.9800 - val_loss: 0.4539 - val_accuracy: 0.8550\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1034 - accuracy: 0.9750 - val_loss: 0.4650 - val_accuracy: 0.8550\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0865 - accuracy: 0.9812 - val_loss: 0.4809 - val_accuracy: 0.8500\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1032 - accuracy: 0.9700 - val_loss: 0.4716 - val_accuracy: 0.8550\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0901 - accuracy: 0.9812 - val_loss: 0.4765 - val_accuracy: 0.8500\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0942 - accuracy: 0.9712 - val_loss: 0.4668 - val_accuracy: 0.8550\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0904 - accuracy: 0.9762 - val_loss: 0.4569 - val_accuracy: 0.8450\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0916 - accuracy: 0.9762 - val_loss: 0.4581 - val_accuracy: 0.8600\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0824 - accuracy: 0.9787 - val_loss: 0.4779 - val_accuracy: 0.8550\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0788 - accuracy: 0.9787 - val_loss: 0.4695 - val_accuracy: 0.8600\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0901 - accuracy: 0.9737 - val_loss: 0.4649 - val_accuracy: 0.8550\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0672 - accuracy: 0.9862 - val_loss: 0.4885 - val_accuracy: 0.8500\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0745 - accuracy: 0.9812 - val_loss: 0.4768 - val_accuracy: 0.8550\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0734 - accuracy: 0.9787 - val_loss: 0.4802 - val_accuracy: 0.8550\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0660 - accuracy: 0.9900 - val_loss: 0.4648 - val_accuracy: 0.8650\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0655 - accuracy: 0.9862 - val_loss: 0.4872 - val_accuracy: 0.8600\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0669 - accuracy: 0.9775 - val_loss: 0.4513 - val_accuracy: 0.8700\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0661 - accuracy: 0.9837 - val_loss: 0.4713 - val_accuracy: 0.8650\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0563 - accuracy: 0.9912 - val_loss: 0.4672 - val_accuracy: 0.8700\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0692 - accuracy: 0.9825 - val_loss: 0.4719 - val_accuracy: 0.8700\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0709 - accuracy: 0.9800 - val_loss: 0.4785 - val_accuracy: 0.8600\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0960 - accuracy: 0.9737 - val_loss: 0.4815 - val_accuracy: 0.8600\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0742 - accuracy: 0.9800 - val_loss: 0.4760 - val_accuracy: 0.8450\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0743 - accuracy: 0.9837 - val_loss: 0.4662 - val_accuracy: 0.8750\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0781 - accuracy: 0.9750 - val_loss: 0.4854 - val_accuracy: 0.8600\n",
            "fold 5\n",
            "Model: \"functional_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_4 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_4 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_4 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "resnet50v2 (Functional)      (None, 10, 10, 2048)      23564800  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_4 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               262272    \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 23,828,362\n",
            "Trainable params: 263,562\n",
            "Non-trainable params: 23,564,800\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 1s 80ms/step - loss: 2.7829 - accuracy: 0.1050\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 2.5166 - accuracy: 0.1562 - val_loss: 2.0671 - val_accuracy: 0.2900\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 1.9877 - accuracy: 0.3100 - val_loss: 1.6804 - val_accuracy: 0.5050\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 1.6283 - accuracy: 0.4938 - val_loss: 1.3639 - val_accuracy: 0.6700\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 1.3331 - accuracy: 0.6037 - val_loss: 1.1355 - val_accuracy: 0.7350\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 1.1300 - accuracy: 0.6538 - val_loss: 0.9801 - val_accuracy: 0.7450\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 6s 117ms/step - loss: 0.9941 - accuracy: 0.6988 - val_loss: 0.8661 - val_accuracy: 0.7750\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.8998 - accuracy: 0.7100 - val_loss: 0.7877 - val_accuracy: 0.7750\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.8220 - accuracy: 0.7487 - val_loss: 0.7177 - val_accuracy: 0.8000\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.7306 - accuracy: 0.7875 - val_loss: 0.6724 - val_accuracy: 0.8000\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.6899 - accuracy: 0.7725 - val_loss: 0.6184 - val_accuracy: 0.8300\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.6391 - accuracy: 0.8087 - val_loss: 0.6065 - val_accuracy: 0.8200\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.5492 - accuracy: 0.8363 - val_loss: 0.5739 - val_accuracy: 0.8300\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.5914 - accuracy: 0.8150 - val_loss: 0.5581 - val_accuracy: 0.8550\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.5110 - accuracy: 0.8438 - val_loss: 0.5493 - val_accuracy: 0.8500\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.4943 - accuracy: 0.8462 - val_loss: 0.5372 - val_accuracy: 0.8500\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.4954 - accuracy: 0.8375 - val_loss: 0.5288 - val_accuracy: 0.8500\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.4574 - accuracy: 0.8637 - val_loss: 0.5202 - val_accuracy: 0.8500\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.4376 - accuracy: 0.8575 - val_loss: 0.5066 - val_accuracy: 0.8600\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 6s 117ms/step - loss: 0.4108 - accuracy: 0.8788 - val_loss: 0.5010 - val_accuracy: 0.8650\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 6s 117ms/step - loss: 0.4003 - accuracy: 0.8700 - val_loss: 0.4931 - val_accuracy: 0.8600\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.3813 - accuracy: 0.8763 - val_loss: 0.4912 - val_accuracy: 0.8600\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.3552 - accuracy: 0.8875 - val_loss: 0.4763 - val_accuracy: 0.8600\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.3730 - accuracy: 0.8763 - val_loss: 0.4745 - val_accuracy: 0.8650\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.3298 - accuracy: 0.8988 - val_loss: 0.4745 - val_accuracy: 0.8700\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.3343 - accuracy: 0.9075 - val_loss: 0.4698 - val_accuracy: 0.8700\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.3190 - accuracy: 0.9075 - val_loss: 0.4881 - val_accuracy: 0.8550\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.2852 - accuracy: 0.9212 - val_loss: 0.4855 - val_accuracy: 0.8750\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.2625 - accuracy: 0.9250 - val_loss: 0.4846 - val_accuracy: 0.8650\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.2735 - accuracy: 0.9162 - val_loss: 0.4796 - val_accuracy: 0.8500\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.2838 - accuracy: 0.9100 - val_loss: 0.4657 - val_accuracy: 0.8650\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.2526 - accuracy: 0.9312 - val_loss: 0.4658 - val_accuracy: 0.8750\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.2576 - accuracy: 0.9225 - val_loss: 0.4684 - val_accuracy: 0.8450\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.2592 - accuracy: 0.9225 - val_loss: 0.4646 - val_accuracy: 0.8400\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.2468 - accuracy: 0.9300 - val_loss: 0.4607 - val_accuracy: 0.8650\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.2517 - accuracy: 0.9250 - val_loss: 0.4678 - val_accuracy: 0.8550\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.2240 - accuracy: 0.9325 - val_loss: 0.4597 - val_accuracy: 0.8550\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1923 - accuracy: 0.9488 - val_loss: 0.4461 - val_accuracy: 0.8700\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.2300 - accuracy: 0.9250 - val_loss: 0.4517 - val_accuracy: 0.8600\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.2062 - accuracy: 0.9450 - val_loss: 0.4525 - val_accuracy: 0.8600\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1745 - accuracy: 0.9475 - val_loss: 0.4525 - val_accuracy: 0.8600\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1986 - accuracy: 0.9438 - val_loss: 0.4497 - val_accuracy: 0.8650\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.2076 - accuracy: 0.9375 - val_loss: 0.4566 - val_accuracy: 0.8450\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1714 - accuracy: 0.9525 - val_loss: 0.4446 - val_accuracy: 0.8500\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1972 - accuracy: 0.9463 - val_loss: 0.4367 - val_accuracy: 0.8550\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1674 - accuracy: 0.9550 - val_loss: 0.4395 - val_accuracy: 0.8550\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1936 - accuracy: 0.9475 - val_loss: 0.4546 - val_accuracy: 0.8600\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1835 - accuracy: 0.9375 - val_loss: 0.4483 - val_accuracy: 0.8550\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 6s 117ms/step - loss: 0.1501 - accuracy: 0.9588 - val_loss: 0.4457 - val_accuracy: 0.8550\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1594 - accuracy: 0.9513 - val_loss: 0.4508 - val_accuracy: 0.8550\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1825 - accuracy: 0.9375 - val_loss: 0.4442 - val_accuracy: 0.8700\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1518 - accuracy: 0.9600 - val_loss: 0.4362 - val_accuracy: 0.8500\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1580 - accuracy: 0.9475 - val_loss: 0.4361 - val_accuracy: 0.8650\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1380 - accuracy: 0.9638 - val_loss: 0.4493 - val_accuracy: 0.8600\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1422 - accuracy: 0.9613 - val_loss: 0.4459 - val_accuracy: 0.8550\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1364 - accuracy: 0.9625 - val_loss: 0.4513 - val_accuracy: 0.8550\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1349 - accuracy: 0.9588 - val_loss: 0.4400 - val_accuracy: 0.8500\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1252 - accuracy: 0.9675 - val_loss: 0.4365 - val_accuracy: 0.8600\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1260 - accuracy: 0.9663 - val_loss: 0.4382 - val_accuracy: 0.8500\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1392 - accuracy: 0.9550 - val_loss: 0.4477 - val_accuracy: 0.8550\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1487 - accuracy: 0.9525 - val_loss: 0.4487 - val_accuracy: 0.8600\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1452 - accuracy: 0.9475 - val_loss: 0.4418 - val_accuracy: 0.8600\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1363 - accuracy: 0.9650 - val_loss: 0.4442 - val_accuracy: 0.8500\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1354 - accuracy: 0.9613 - val_loss: 0.4585 - val_accuracy: 0.8450\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 6s 117ms/step - loss: 0.1247 - accuracy: 0.9613 - val_loss: 0.4700 - val_accuracy: 0.8550\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1317 - accuracy: 0.9600 - val_loss: 0.4694 - val_accuracy: 0.8600\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1014 - accuracy: 0.9737 - val_loss: 0.4690 - val_accuracy: 0.8500\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1348 - accuracy: 0.9575 - val_loss: 0.4672 - val_accuracy: 0.8600\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0995 - accuracy: 0.9762 - val_loss: 0.4557 - val_accuracy: 0.8600\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1122 - accuracy: 0.9700 - val_loss: 0.4735 - val_accuracy: 0.8500\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1137 - accuracy: 0.9712 - val_loss: 0.4657 - val_accuracy: 0.8600\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 6s 117ms/step - loss: 0.0880 - accuracy: 0.9850 - val_loss: 0.4838 - val_accuracy: 0.8500\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 6s 118ms/step - loss: 0.1015 - accuracy: 0.9737 - val_loss: 0.4737 - val_accuracy: 0.8600\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1094 - accuracy: 0.9750 - val_loss: 0.4733 - val_accuracy: 0.8600\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0989 - accuracy: 0.9700 - val_loss: 0.4763 - val_accuracy: 0.8600\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0896 - accuracy: 0.9750 - val_loss: 0.4708 - val_accuracy: 0.8450\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0998 - accuracy: 0.9750 - val_loss: 0.4842 - val_accuracy: 0.8500\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0973 - accuracy: 0.9737 - val_loss: 0.4759 - val_accuracy: 0.8500\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1156 - accuracy: 0.9638 - val_loss: 0.4808 - val_accuracy: 0.8500\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0759 - accuracy: 0.9850 - val_loss: 0.4821 - val_accuracy: 0.8500\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 6s 117ms/step - loss: 0.0866 - accuracy: 0.9800 - val_loss: 0.4866 - val_accuracy: 0.8600\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.1008 - accuracy: 0.9700 - val_loss: 0.4855 - val_accuracy: 0.8600\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0719 - accuracy: 0.9837 - val_loss: 0.4881 - val_accuracy: 0.8600\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0787 - accuracy: 0.9825 - val_loss: 0.4945 - val_accuracy: 0.8350\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0884 - accuracy: 0.9775 - val_loss: 0.4889 - val_accuracy: 0.8500\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0860 - accuracy: 0.9825 - val_loss: 0.4902 - val_accuracy: 0.8500\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0844 - accuracy: 0.9787 - val_loss: 0.5001 - val_accuracy: 0.8300\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0739 - accuracy: 0.9837 - val_loss: 0.5021 - val_accuracy: 0.8300\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.1085 - accuracy: 0.9787 - val_loss: 0.4880 - val_accuracy: 0.8450\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0567 - accuracy: 0.9875 - val_loss: 0.4823 - val_accuracy: 0.8500\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 6s 117ms/step - loss: 0.0627 - accuracy: 0.9862 - val_loss: 0.4797 - val_accuracy: 0.8400\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0800 - accuracy: 0.9800 - val_loss: 0.4849 - val_accuracy: 0.8300\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0834 - accuracy: 0.9762 - val_loss: 0.5044 - val_accuracy: 0.8350\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0799 - accuracy: 0.9787 - val_loss: 0.4790 - val_accuracy: 0.8650\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.0668 - accuracy: 0.9875 - val_loss: 0.4695 - val_accuracy: 0.8400\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0585 - accuracy: 0.9875 - val_loss: 0.4903 - val_accuracy: 0.8400\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 6s 117ms/step - loss: 0.0720 - accuracy: 0.9812 - val_loss: 0.4943 - val_accuracy: 0.8650\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 6s 117ms/step - loss: 0.0748 - accuracy: 0.9775 - val_loss: 0.4903 - val_accuracy: 0.8600\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0742 - accuracy: 0.9837 - val_loss: 0.4716 - val_accuracy: 0.8600\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0694 - accuracy: 0.9850 - val_loss: 0.4684 - val_accuracy: 0.8650\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 0.0706 - accuracy: 0.9837 - val_loss: 0.4762 - val_accuracy: 0.8600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2E72C3O30fv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb9b96ca-b37e-4b9a-d476-9f67cce39c63"
      },
      "source": [
        "# cross-validated accuracy for pre-trained model before training\n",
        "print(\"Base model accuracy:\", np.mean(base_model_acc_list))\n",
        "# cross-validated accuracy after training\n",
        "print(\"Final accuracy:\", np.mean(final_acc_list))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Base model accuracy: 0.10399999916553497\n",
            "Final accuracy: 0.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dtn-03T_ZaRX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "66b2a680-c21f-418b-b130-6c7c2ecd5801"
      },
      "source": [
        "# plot graph of cross-validated accuracies\n",
        "acc = np.mean(history_map['accuracy'], axis=0)\n",
        "val_acc = np.mean(history_map['val_accuracy'], axis=0)\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEICAYAAABS0fM3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd5xU5bnA8d+zs72xbKHtLkU6KF1QsGBLsEFiiaJGSNHojVExJtcYY4gmV2/iTbtRc4m9RGIPGtSIiqhYWIpIr8uy1O29zcxz/zhnl9nKAjssu/N8P5/57Jz+nDmz55n3fc95j6gqxhhjQldYZwdgjDGmc1kiMMaYEGeJwBhjQpwlAmOMCXGWCIwxJsRZIjDGmBBnicA0IiJvicicjp63M4lItoicH4T1LhWR77vvrxWRf7dn3qPYTn8RKRcRz9HGakxbLBF0A+5Jov7lF5GqgOFrj2Rdqnqhqj7d0fOeiETkLhFZ1sL4VBGpFZGT27suVX1eVb/WQXE1SlyqmqOq8arq64j1t7A9EZEdIrIhGOs3Jz5LBN2Ae5KIV9V4IAe4NGDc8/XziUh450V5QnoOmCoig5qMvxr4SlXXdUJMneEsoBdwkoicejw3bN/JE4Mlgm5MRKaLSK6I/KeI7AeeFJGeIvKmiOSJSJH7PiNgmcDqjrki8rGIPOTOu1NELjzKeQeJyDIRKRORJSLysIg810rc7YnxfhH5xF3fv0UkNWD6t0Vkl4gUiMjPW/t8VDUXeB/4dpNJ1wPPHC6OJjHPFZGPA4YvEJFNIlIiIn8BJGDaYBF5340vX0SeF5Ekd9qzQH/gDbdE91MRGSgiWn/SFJF+IrJIRApFZJuI3BCw7vki8qKIPON+NutFZFJrn4FrDvBPYLH7PnC/RovIu+62DojI3e54j4jcLSLb3e2sFJHMprG68zb9nnwiIn8QkQJgflufh7tMpoi86h6HAhH5i4hEujGdEjBfLxGpFJG0w+yvacISQffXB0gGBgA34hzzJ93h/kAV8Jc2lp8CbAZSgd8Cj4uIHMW8fwe+AFKA+TQ/+QZqT4zXAN/B+SUbCdwJICKjgEfd9fdzt9fiydv1dGAsIjIcGOfGe6SfVf06UoFXgXtwPovtwLTAWYAH3PhGApk4nwmq+m0al+p+28ImFgK57vJXAP8lIucGTJ/pzpMELGorZhGJddfxvPu6WkQi3WkJwBLgbXdbQ4D33EXvAGYDFwGJwHeByjY/mEOmADuA3sBv2vo8xGkXeRPYBQwE0oGFqlrr7uN1AeudDbynqnntjMPUU1V7daMXkA2c776fDtQC0W3MPw4oChheCnzffT8X2BYwLRZQoM+RzItzEvUCsQHTnwOea+c+tRTjPQHD/wG87b6/F+dEUT8tzv0Mzm9l3bFAKTDVHf4N8M+j/Kw+dt9fD3wWMJ/gnLi/38p6vwGsbukYusMD3c8yHOck6QMSAqY/ADzlvp8PLAmYNgqoauOzvQ7Ic9cdDZQA33SnzQ6Mq8lym4FZLYxviLWNzynnMMe74fMATq+Pr4X5puAkTXGHs4Bvdeb/X1d9WYmg+8tT1er6ARGJFZH/c6tOSoFlQJK0fkXK/vo3qlr/iy/+COftBxQGjAPY3VrA7Yxxf8D7yoCY+gWuW1UrgILWtuXG9BJwvVt6uRZ45gjiaEnTGDRwWER6i8hCEdnjrvc5nJJDe9R/lmUB43bh/FKu1/SziZbW6+LnAC+qqtf9nrzCoeqhTJzSTEvamnY4jY79YT6PTGCXqnqbrkRVP8fZv+kiMgKnxLLoKGMKaZYIur+m3cv+GBgOTFHVRJyGQgioww6CfUCyWw1RL7ON+Y8lxn2B63a3mXKYZZ4GvgVcACQAbxxjHE1jEBrv73/hHJdT3PVe12SdbXUJvBfns0wIGNcf2HOYmJpx2zvOBa4Tkf3itCNdAVzkVm/tBk5qZfHdwOAWxle4fwOPdZ8m8zTdv7Y+j91A/zYS2dPu/N8GXg780WPazxJB6EnAqesuFpFk4JfB3qCq7sIpts93G/lOBy4NUowvA5eIyBluXfd9HP57/hFQDCzgUP3zscTxL2C0iFzmnsBupfHJMAEoB0pEJB34SZPlD9DKCVhVdwPLgQdEJFpExgDfw/kVfaS+DWzBSXbj3NcwnGqs2Th1831F5HYRiRKRBBGZ4i77GHC/iAwVxxgRSVGnfn4PTnLxiMh3aTlhBGrr8/gCJ7E+KCJx7j4Htrc8B3wTJxk8cxSfgcESQSj6IxAD5AOf4TQEHg/X4tT3FgC/Bv4B1LQy71HHqKrrgR/iNPbuA4pwTmxtLaM4J5EBND6ZHFUcqpoPXAk8iLO/Q4FPAmb5FTABpz7+XzgNy4EeAO4RkWIRubOFTczGqYvfC7wG/FJVl7QntibmAI+o6v7AF/BXYI5b/XQBTtLeD2wFznGX/T3wIvBvnDaWx3E+K4AbcE7mBcBonMTVllY/D3XunbgUp9onB+dYXhUwfTewCqdE8dGRfwQGDjWyGHNcicg/gE2qGvQSieneROQJYK+q3tPZsXRVlgjMcSHOjUqFwE7ga8DrwOmqurpTAzNdmogMBNYA41V1Z+dG03UFrWpIRJ4QkYMi0uLdmW694p/FuSFmrYhMCFYs5oTQB+cywnLgz8DNlgTMsRCR+4F1wO8sCRyboJUIROQsnH/6Z1S1WZ8tInIR8COcG1KmAH9S1SlN5zPGGBNcQSsRqOoynKqA1szCSRKqqp/hXJ/dN1jxGGOMaVlndviUTuMbS3LdcfuazigiN+J0j0BcXNzEESNGHJcAjTGmu1i5cmW+qrbYD1OX6PlPVRfgXOPNpEmTNCsrq5MjMsaYrkVEdrU2rTPvI9hD47stMziKuyONMcYcm84sESwCbhGRhTiNxSWq2qxayBhjgs3r87P5QBnpSTEkxUY2m66qLN2Sx8PvbwPg4jF9ufiUvvRKjG51ndsOlvPgWxvZerCcOacP5Jop/YmOaP9D5qrrfGRlFxEmkBgTQUJ0OGkJUcRGdvxpO5hXDb2A0/tlKs4t878EIgBU9a9u/yt/AWbgdBz1HVU9bJ2PVQ0Z03X4/IonLJjdWB2yu7CSxV/to6CiloLyWhTlO1MHcUpGj2bzen1+tuWV8+XuYpZtzefjrfmUVNUhAqP7JTJtcCoZPWOIjvDgCRNe+CKHFdlFZCbHEBcZzqb9ZYjAoNQ4UuIiSY6LpFdCNP2SYkjvGcPK7EKe+zyH2AgPQ3vHsyqnmF4JUVw5KYNar5/CijrKquvwhAnhnjBiIsLI6BnLgJRYEqMjeGf9fv711T7Kqhv3tXf/rNF8+/SBR/X5iMhKVW3x2RRd7oYySwTGBF+dz0+Ep301xz6/svVgGWEixER48KuydHMeb6/bzxfZhUwbkspdM0Ywql9ii9vJK6uhstZLnU+p8/mJDA8jITqCxOhwymu8bDtY3vDaerCc7QfLEYHLJ2Qwe3J/EmMi+N/3t/LcZ7uo8ylR4WGkxEVSXuOltNrLzLH9uHn6YPaVVPH5zkKysotYv7eE6jo/AL0TozhraBqnD05hd2EVn2zPZ3VOEXW+Q+fGXglR3HreUL41KZPI8DC2HSzjzbX72HKgjMKKWgorajlQWkNJVR0AYQLXTOnPvPOHkRIfxafbC/jDki18sbOwIb6E6Ah8qnh9fiprfRwsO9TjSmykhxkn9+HSMf2IifRQWlVHWbWXcf2TGJzWWue/bbNEYIxpl8paL7ctXMN7Gw8wvE8i4/snMSQtnspaL2XVXmq8fvr2iCa9ZwyRnjA+2HyQf68/QEFFbbN1DekVz6kDk1n81T5Kq+v4xrh0+iVFk11Qya6CCvaXVFNQUUt7T0E9YiIY0iueIWnxFFbW8v6mg/j8SnREGLVeP9+alMmt5w2lb49oRISy6jr+78MdPPbxjoaTfoRHGJORxNiMJMZk9ODk9B4MTouj6bOWarw+yqq9VNX6qK7zkZkc265qnfIaL3uKqoiL8pDRM7bZ9Bqvj6jwltdTXecjp7CSvLIaxvdP6vAqIEsExnQhXp+f/aXVHCyrqX8ACxGeMEb363FM1Sxl1XWszS1hX0k1VbVequp89E6M5vyRvYmLCie/vIbvPbWCr/aUcNWp/cktqmRNTjFlNU71RFR4GBGeMMprDlVXxEV6OHdkb84ZnkZkeBhVtT68fuXUgT0Z0svpKbukso5Hlm7jyeXZ+P1KZnIs/ZNj6ZcUQ+/EKHolRBMfHU6EW01S6/VTVl1HaXUdMZHhDEmLZ0iveFLjIxudsA+UVvPiit3sKa7ie2cMYmjvBFpyoLSad9bvZ2ivBMb3TzqievruxBKBMZ0st6iSrQfKOSWjB6nxUc2mqyrPfZ7DgmXb2Vtcjc/f/P9yQEos35k6kCsnZRIXFd5o2cVf7ee/Fm8kr7ym4YSaEB1OSlwkPeMi2VdczZaDZS3++o6J8PD10b1ZvbuYA6XV/O/sCVwwqjcAfr9SXFVHXJSn4ZdsWXUde4qrKK3yMiajR7tPrFW1PiI8Tmzm+LNEYEwHyM6vIC0hqtFJ+HBKq+t4+P1tPPlJNrU+p3qif3Iskwb25NwRvThrWBoVNV5++vJaPtqaz+RByUwemEx6zxj6JEY3lAAKKmp47rMcVu4qIiE6nLOGpTFtcCrD+yTwl/e38sHmPEb3S+SMIanU+RSv309ZtZeCiloKK2pIjotiQv8kJvTvycCUOGIiPcREeti4r5RXV+3hX2v3EuEJ429zJjGhf8+gfH6mc1kiMOYYHCyr5ndvb+allbn07RHNb755MueO6N0wvdbrZ0e+25h5oJzCilq8fj+1XuWDzQcpqqzligkZfGN8Ouv3lrBqVzGf7SyguLKOCI8Q4QlDFX5+8UiundK/WX11oNU5RTz/eQ4fbc3jQKnTuBgX6eGOrw1nzukDjvrXdq3XT5hgv9a7MUsExrShvMbLiyt289WeEjbuK2V7Xjm9E6MZ0SeRfknRvLpqDzVeH9dM7s/y7QVsPVjOzLH9GNk3keXb81mRXdjQGCniNGpGeMKICBOG9E7gp18fzsnpjS9h9PmVVTlFLNl4gLzSGm49bygDU+PaHbOqsj2vgrW5xZx2Ugr9kmIOv5AJaZYIjGmBqvLm2n38+l8bOFBaQ5/EaEb2TeCktHgOlFazcV8p2QWVnDU0lV9cMoqT0uKp8fp45IPtPLJ0G3U+ZVjveKYOTnWurukVz+C0+JBtjDQntrYSQZfoa8gYcC6v23KgjJ6xkWQmH7o0r9br55eL1vPqqtyGp6InRodz7oheXHhyX6aclExBeS25RVXsL62ioNy57nvlriI+31nI6H6JPHLtRCYOaF437vcrYQFX6kSFe5h3wTCundIfBHoltH5nqTFdhSUCc0LLK6vh4Q+2sWxrHtn5FfjVuRb8B2cN5pZzh1BV6+MHz63ki52FXD4hg7QE54qcvcVVvPXVfl7MavlxxeFhQu/EaO6bNZprpwxo9bLMsFbGt9W1gDFdjSUCc8JQVXx+pc6nVNZ6efazXSxYtoNar5/pw3tx6Zh+DO+TwHsbD/KXD7bxxtq9CLC3pJo/XT2OWePSG62vxutj+fYCvsotoXdiFOlJsfRLiiYlPorE6PA2G2WNCSXWRmA6XUWNl79/nsNjH+9ouBKm3kWn9OEnXx/BoCYNqcu35fPz19dRWlXHgusnMnFA8vEM2Zgux9oIzAnpYGk1L3yxm6eW76Soso6pg1O4bopzCWSERzh1YDJjM5NaXHbqkFTenXcWdT4lJtIaZ405FpYITNDUev18si2fN9bu5aOt+aQnxTChf0+G94nng015LNl4AK9fOWd4GrecO7TFxtq2hHvCaKXbFmPMEbBEYDrMm2v38sDiTdR4fQBU1vqorPWRGB3O2cN7caCkmr9/sYvqOj89YyP47hmDuPrUTE46yt4UjTEdwxKBOSL55TU8+NYmVu4q4jvTBnLN5P54woS/friD/357E2MyenCKe/NUhCeMM4emcuZQp0MycLod3plfwYCU2FZ7YTTGHF+WCEyL9hRX8crKXHIKKxnWO56RfRPZkVfBQ//eTHWdj+F9Erj3n+t55tNdjOiTwJtr9zFzbD9+e8WYNm+oivCEMayVXiKNMZ0jqIlARGYAfwI8wGOq+mCT6QOAJ4A0oBC4TlVbvvDbHBcrdxXxxyVb+HhbPqqQGh/JyysPHZIzhqTyq1mjOSk1jiUbD/Jfizfy5tp93HLOEO64YFir190bY05cQUsEIuIBHgYuAHKBFSKySFU3BMz2EPCMqj4tIucCDwDfDlZM5hCvz48nTBpdS59TUMl3nvyCmEgPt547lCsmZpCZHEthRS2b9pcCcPpJKQ3LXDCqN2cPSyOnsKKh73ljTNcTzBLBZGCbqu4AcB9SPwsITASjgDvc9x8ArwcxHuMqrqzlyr9+SmykhwXXT6J3YjTVdT7+4+8rAXj5pqmNunBIjotk6uDUFtcVGR5mScCYLi6Yfc6mA7sDhnPdcYG+BC5z338TSBCRlKYrEpEbRSRLRLLy8vKCEmyoqPP5+Y/nV7GroJKtB8uZ9ZdPWLenhPve3MC6PaX8/lvjGiUBY0z319mdj98JnC0iq4GzgT2Ar+lMqrpAVSep6qS0tLTjHWO3oarc+8/1LN9ewAOXncLLN00lTOCyR5fz989zuOnswZw/qvfhV2SM6VaCWTW0B8gMGM5wxzVQ1b24JQIRiQcuV9XiIMYUct74ci/Z+RWEe8LILarkhS9y+OE5g7l8YgYAr98yjVueX01MpIc7vzask6M1xnSGYCaCFcBQERmEkwCuBq4JnEFEUoFCVfUDP8O5gsh0kGc/zeYX/1zfaNyscf348QXDG4Z7JUTz4k2no6rWCZsxISpoiUBVvSJyC/AOzuWjT6jqehG5D8hS1UXAdOABEVFgGfDDYMXT3ZVW1xEfGd5w+eayLXnMf2MD547oxSPXTkAVvH4/CdERLS5vScCY0GW9j3Zxuwsr+Z9/b+b1NXs5KS2O608bwNjMJK5//AvSe8bw8s1TiT+Ch60bY7on6320GyqqqOUvH2zj2U93IQLXnz6AL3NLmP+Gc3Vuanwkj82ZZEnAGHNYdpboYqrrfDzxyU4eXbqdihovV0zMYN4Fw+jbw3l4+Zrdxby+eg+XT8ggo6ddBmqMOTxLBF3I+r0lfP/pLPaVVHPeiF78dMYIhvdpfDPXuMwkxrXSh78xxrTEEkEXkVNQyZwnVhDpEf5x42lMOanZfXfGGHNULBF0AfnlNVz/xOd4/X4W3ni6delgjOlQnX1nsTmMgvIavvPkCvaXVvP4nFMtCRhjOpyVCE5QpdV1PLZsB49/vJMar999QPuRPcrRGGPawxLBCaKixsvra/awM6+C7IJKVmQXUlJVx8Wn9GXeBcMY0sse53hC8dZAeFRnR2FMh7BEcIL46Str+dfafUSFhzEwJY6zh6Vx41kncbL72McO56uD2vJDw5EJ4Gnl61B2AD79XziwHoacDyMvhaT+jedRhfWvwoonIKE3pA6HtGGQNgKSB0N45NHFWVcFXy6EijxIHQZpwyG+N9TfCe2Jgsgml8n6vOD3QkR06+v1+8FXAxExrU/f8hZ8/n9QVejuI1BTAhUFUFcBA6bBN/8PkjJbWN4H+7+C6pLmMZvu7dNHIHcFXPCr5v8nJyi7s/gEsCqniMseWR78p3ztXgHb3oVdy50vqrf60LSIWMic7JzcUoc5Jy1VyP4YVj0D/jpIPgkKtjnz9xvvJISRM51l/3UHbHkbUoY4SaY4B+fMCYgHeg6EhL4QmwxxqRCbArGpznBdFVTmQ2UhxPR0TpwpQ2Dru7D8f6HiYNv7FR7jrNMTCZUFUF0MEgZ9xx7an6JsyN8ChTucpFJZCOqHwefApO/CsBmAQNFOyM1ytntwPSQNgN6jD20rKsGJOzwSvvgbhHng0j/D8Ath72rY9Ynz+eZ8DrVlAcv1cBPjcCdJ9hsHA86AsBaa6fw+WP0sbPin8/mOu6bl0kdlIexdBTHJzj5GxTsJrDQX8rdCZLyzzZjjXKVYUw7h0a3/sOhIpftg35fOPsamQFyK81nXf67FOc7xKNoFE+c6P1KC6dOH4Z27AXH+L867Fybf4HxP6hVsh1VPw85lEJXofHfj+0DGROf7mtDH+R8q3OHE32+8M88xauvOYksEnUxVufKvn7KrsJKld04nrqPvBFaF7e/DsocgZ7lzguwzBgZMhR6Zh074RTudf5gD62k4gQOERcC42TDtdkgZ7HyJN74BGxfBnpXuPOHOSfjce2DKTc6XvrYSCrZC3hbI3+ychCvynVdlPlQVOSfiQOHRjZMTwEnT4cw7IX2Cc3LL3+Kc7OvVVTnDlQVOdU1sivNP46t1Tsa5K5xf/mHhTskkZQjE93Lm83vhq5egdA9E93Bi9tc5600dDmf+GE6+vPUTWuEOePl7zsnYE+VsB5xS0ICpzj91bIob92bIc1/1iS1pAEyc42wjIs4Zt38t/PsXThKK6+XMm9APptzonNgrC6Bsn5PUDzbuUJCEfk4SrKtsPD7O3d+WJPaDERfDiEucRLJtiXN89687NE9MTxjzLTjlCicR1lY48+V87gzHpTqJas9K5zuUvwUQiElykpTHLQ2KuD8gZjrH1V8Hu7+AnE+dE15FvrN/sSmHPr+EPk7Cq8x3EmT98S3cCSufhM1vgTbpuV48znwSBuX7Az6HNPjGX2Ho+S1/Fq3xut+fsNafxQ1A1pPw5u0wahac/ytY/BPnh1fPQdBzgBNTRZ6TAMQD/U93vqeV+U5C81Y564nv4+6v192fMOg/FUbNdD67xL5HFr/LEsEJ7O11+7jpuVU8cNkpzJ7cAcXIAxtg7cJD/1ile5wTVmI6TL3VOalHt1HdVFUEpXsPDcf1gvhWngFRkgsb34TC7XD6D51f/e3l9znVJpUFTgKITXGqeGrKnBNJ/lZIHQrpE9u/zpZ4a5zPoEcmeFrocM/ndf5ZN77pnGDShjuvvuNb/rXebP218NnDUJ7nnLz6n+78Km1LVZGTnLOehOyPmk9PGgAX3OecUHYsdZL4ro8PTY9JdksUUyFjMtSUOgmmYJtz0k4d5nx2tRXO+PzNUF3aQiDqJP7CHYA4J3NvtbP+/qc5Jz9w1ntwg5OI0ifC7s+d+QKTHzi/xPufBhmnOiex+lJe/QnNVwu7PnWq1yJinWOjPuek2CPDLSUmO9+/gxuaRdtMbCqMvxaGX+RUc1YUuNsscL773hon3gFTnST0yg1O8pxys5PY6ktR4HwPqosbx7rzIycpbn/f+Wz6n+asK22EG2uKs928zbB3DXz2CAy9AK563ikxqsJXL8O6V9wfK/nOvo65CsZf1/iE7vPC/i+dz2f/V9Aj3fkxktjXjWMR5G2Cix5yShhHwRLBCeT11XtYtjWP6cN7MW1wCpc/upwITxhv3XYm4Z7DnHi8NbDpX84vwop854vbUFURBetehd2fOb/AkgYcqoIZegGMnW2Nmyei/G2w88NDpaPoHm51W5P2jeLdzvGLSe7YKhdV56S78Q2oKnaquAZMa7wNVadklfUk7Mlyfs2PvNT5lQpOYqstc75zh/vV7K11fhFvedspMdQns6gmF0NUFjolhaoi5/sdl+r8Mq4sdE6qkXEw9GtH1vZUV+WUtlb87dC4+D5OMqsqanmZHplOiclb48STt6nl+cTjVC9e8Xjr7U7HKm+L+z+dfFSLWyI4QWzcV8qsv3yCXxWv/9Dn/sTcSZw74jB1lzmfwaIfucVunF9rUYnOL+j66oyUIU496NhrDv+r1JhQVbjDKQnlbXKqmCJiDrVXNZQaxWlj6je+cSN/RQEU7zpU6oiIdi+IOOmE/6FlvY+eAKrrfNy2cDU9YiNYfOuZ5BRW8N6GvcT5KjhnWEDVS3kebP6X0/AY3cP5ghZuh5VPQY/+MHuhU0SNTjpUv19T5lQPJKbblSnGHE7ySc5r5KVHvmxcSrf8kWWJ4Dh58K1NbDlQztPfnUya7wBp259h4vpnncasdUlOvbSEOfWv6ndO9HWVTl0l4tRrnntP8yK0CEQnOi9jjDkKQU0EIjID+BPOE8oeU9UHm0zvDzwNJLnz3KWqi4MZU2f4YPNBnlqezd3jqjh71e1OPT84dZwDf+hcsZO32Tnxn/VT55dK/SWLteVOA9bxvgTQGBMygpYIRMQDPAxcAOQCK0RkkaoGXg5wD/Ciqj4qIqOAxcDAYMV03HlrWL0mizcW/ZuX4pdx6qZVTnXPmT926vJbuhGpqSjrW8gYE1zBLBFMBrap6g4AEVkIzAICE4EC9XUaPYC9dAf522Dxj/Hv/Ijx6mN8GPgjUuDsX8Kp37dqHGPMCSWYiSAd2B0wnAtMaTLPfODfIvIjIA5o8U4PEbkRuBGgf/8T+JZtXx0s/zMs/W9qw6J43HsxFUkj+MFlM0jIPPmEv6rAGBOaOruxeDbwlKr+j4icDjwrIierNr7lVFUXAAvAuXy0E+JsWek+ePHbhy7p9HmhroIdaedz1e5vMvikwTw251R7brAx5oQWzDPUHiCwEjzDHRfoe8AMAFX9VESigVTgMJ3LnACKc+Dpmc4t4+OuAQnDr8rC/MHcvSGDi8f05X+uHEt0xGFusDHGmE4WzESwAhgqIoNwEsDVwDVN5skBzgOeEpGRQDSQF8SYOkbhDicJVJfCt1+HzFNRVeb9Yw3/3LCX750xiJ9fNDJ4nccZY0wHCloiUFWviNwCvINzaegTqrpeRO4DslR1EfBj4G8iMg+n4Xiunui3Ou/4EF69wWkPmLPI6fMFeO7zHP65Zi/zzh/GbecP7eQgjTGm/YJaee3eE7C4ybh7A95vAKYFM4YO46uDD/4LPv6D05XDVc9Cr5EAbN5fxq/f3MDZw9L40blDOjlQY4w5MtaK2R61lfDsN5y7fidcDzMedDq9wuk64tYXVpMQHc5DV4616iBjTJdjiaA9lv3OSQKX/c3pvjbAg29tYvOBMp76zqmkJdjlocaYrqcdHa6HuAMbnHsDxl3bLAmUVdfx3Ge7mD25P9OH9+qkAI0x5thYImiL3w9vznO6e77g/maTP9tRiNevzBzbrxOCM8aYjnBirdQAACAASURBVGFVQ21Z/azzoJdZD7fY9exHW/OIjfQwYUBSJwRnjDEdw0oErSnPg3fvdZ7WNO7aFmf5eGs+UwYlExVuN40ZY7ouSwSt+fc9zjNfL/lDiw97yS2qZEd+BWcObeV5vsYY00VYImjJjg+dB8BPu815YEwLPt6aD8CZQ1OPZ2TGGNPhLBE05a2Bf90BPQfBWXe2OttHW/PpkxjNkF7xrc5jjDFdgTUWN/XxH6BgG1z3qvNQ6xb4/Mon2/M5f2RvxJ4RbIzp4qxEEKgkFz76Hzj5chhyXquzrdtTQnFlnVULGWO6BUsEgbI/dh4Wf2brVUIAH29z2gemDbFEYIzp+iwRBNr3JYTHtNpAXG/ZljxG90skNd66lDDGdH2WCALt+xL6nAxhLd8XsK+kip+89CVfZBcyfbhdNmqM6R6ssbie3w/7v2rWnxCAqvLn97bxyNJtqML3pg3iP6Zbd9PGmO4hqIlARGYAf8J5MM1jqvpgk+l/AM5xB2OBXqraOf01FO2EmlLoO7bZpO155fxhyRYuGNWbX146ioyesZ0QoDHGBEfQEoGIeICHgQuAXGCFiCxyH0YDgKrOC5j/R8D4YMVzWPu+dP62kAhW5RQD8J8zRlgSMMZ0O8FsI5gMbFPVHapaCywEZrUx/2zghSDG07Z9X0JYBKSNbDZpdU4xidHhnJQa1wmBGWNMcAUzEaQDuwOGc91xzYjIAGAQ8H4Q42nb/rXOoyfDI5tNWp1TxLj+Pe3pY8aYbulEuWroauBlVfW1NFFEbhSRLBHJysvL6/itqzolghaqhcprvGw5UMb4TOtq2hjTPQUzEewBMgOGM9xxLbmaNqqFVHWBqk5S1UlpaUG4bLN0D1QWtJgI1uYW41cY398SgTGmewpmIlgBDBWRQSISiXOyX9R0JhEZAfQEPg1iLG1ro6F4zW6noXiclQiMMd1U0BKBqnqBW4B3gI3Ai6q6XkTuE5GZAbNeDSxUVQ1WLIe1by1IGPQe3WzS6pxiTkqNIym2eduBMcZ0B0G9j0BVFwOLm4y7t8nw/GDG0C77voTUYRDZ+KogVWV1TjFnDbM+hYwx3deJ0ljcufZ9CX3GNBudW1RFfnkN4/v37ISgjDHm+LBEUJ4HZXtbbB9Y7bYP2BVDxpjuzBLB/rXO377NSwSrc4qIjghjRJ+E4xyUMcYcP5YIirKdvynNO5FbnVPMmPQkwj32MRljui87w5XkQlg4xPduNLrG62PD3lK7f8AY0+1ZIijZDYnpzZ5BsGFvKbU+v90/YIzp9iwRlORCj8xmo9fmlgAw1hKBMaabs0RQkgs9MpqN/jK3mNT4KPr2iO6EoIwx5vgJ7UTg80Lp3pYTwe5ixmb0QMR6HDXGdG+HTQQicqmIdM+EUbYP1AdJjauGyqrr2JFfYdVCxpiQ0J4T/FXAVhH5rdtBXPdRkuv8bVIi+GpPCaowJqNHJwRljDHH12ETgapeh/MIye3AUyLyqft8gK5/l1VDImhcIqhvKB6TYSUCY0z3164qH1UtBV7GedxkX+CbwCr3OcNdV0mO87dJiWBtbjGZyTEkx1mPo8aY7q89bQQzReQ1YCkQAUxW1QuBscCPgxtekJXkQkxys15Hv9xdwlgrDRhjQkR7uqG+HPiDqi4LHKmqlSLyveCEdZy0cOlofnkNe4qrmDt1YOfEZIwxx1l7qobmA1/UD4hIjIgMBFDV94IS1fFSvBuS+jcatTbX6XHUGoqNMaGiPYngJcAfMOxzxx2WiMwQkc0isk1E7mplnm+JyAYRWS8if2/PejuEqtO9RJMSwZe7SwgTODndEoExJjS0p2ooXFVr6wdUtdZ9BnGbRMQDPAxcAOQCK0RkkapuCJhnKPAzYJqqFolIryPeg6NVXQK15S02FA/tlUBcVFAf3maMMSeM9pQI8gKfMSwis4D8diw3GdimqjvcRLIQmNVknhuAh1W1CEBVD7Yv7A7QwqWjqsqXuSVWLWSMCSnt+dl7E/C8iPwFEGA3cH07lkt3562XC0xpMs8wABH5BPAA81X17aYrEpEbgRsB+vfv33Ty0SlxQwtIBLlFVRRW1DLG7ig2xoSQwyYCVd0OnCYi8e5weQdvfygwHcgAlonIKapa3CSGBcACgEmTJmmHbLmFu4q3HCgDYFTfxA7ZhDHGdAXtqggXkYuB0UB0fSdsqnrfYRbbAwTespvhjguUC3yuqnXAThHZgpMYVrQnrmNSshs8URCX1jBqV0ElAINS41pbyhhjup323FD2V5z+hn6EUzV0JTCgHeteAQwVkUFu4/LVwKIm87yOUxpARFJxqop2tDf4Y1KSCz3SIezQR7CroIKEqHB6xkYclxCMMeZE0J7G4qmqej1QpKq/Ak7Hrdtvi6p6gVuAd4CNwIuqul5E7gtofH4HKBCRDcAHwE9UteBoduSIFTe/dHRXYSUDUmOt62ljTEhpT9VQtfu3UkT6AQU4/Q0dlqouBhY3GXdvwHsF7nBfx1dJLgw+t9GoXQWV1j5gjAk57SkRvCEiScDvgFVANnD8bvwKBl+d8yyCgBKB1+cnt6iS/imxnRiYMcYcf22WCNwH0rznXsXzioi8CUSraslxiS5YSvcA2igR7Cupps6nDLREYIwJMW2WCFTVj3N3cP1wTZdPAtDipaP1Vwz1T7YrhowxoaU9VUPvicjl0p1aUEvcq1gDbibLLqgAYGCqlQiMMaGlPYngBzidzNWISKmIlIlIaZDjCq6qQudvXErDqJzCSiLDw+idEN1JQRljTOdoz53FXf+RlE1Vu7VbUYeuENpVUMGA5FjCwrpPwccYY9rjsIlARM5qaXzTB9V0KdWlEJkAYZ6GUbsKKhlgDcXGmBDUnvsIfhLwPhqnV9GVwLktz94FVJdA9KEeRlWVXQWVTB2c2olBGWNM52hP1dClgcMikgn8MWgRHQ/VxRB9qFoor6yGqjqfNRQbY0JSexqLm8oFRnZ0IMdVTWmjEsGuwvpLRy0RGGNCT3vaCP4XqO/6OQwYh3OHcddVXQIJh3rJyM53Lx1NsXsIjDGhpz1tBFkB773AC6r6SZDiOT6qSyB1eMNgTmElnjAhvWdMJwZljDGdoz2J4GWgWlV94DyLWERiVbUyuKEFUXXjqqHsgkrSk2KI8BxNTZkxxnRt7bqzGAj8qRwDLAlOOMeBarOrhnIKKuzSUWNMyGpPIogOfDyl+77rnjVrK0B9ja4ayi6otIZiY0zIak8iqBCRCfUDIjIRqApeSEFW4/aO4ZYIiitrKamqs4ZiY0zIak8iuB14SUQ+EpGPgX/gPHnssERkhohsFpFtInJXC9PnikieiKxxX98/svCPQn33Em4iaOh11KqGjDEhqj03lK0QkRFA/WU2m92HzbdJRDw4XVhfgHPvwQoRWaSqG5rM+g9VbVdi6RBN+hnaU+wUbjJ7WiIwxoSm9jy8/odAnKquU9V1QLyI/Ec71j0Z2KaqO1S1FlgIzDq2cDtAdX3VUBIAhRW1AKTGR3ZWRMYY06naUzV0g/uEMgBUtQi4oR3LpQO7A4Zz3XFNXS4ia0XkZbf7imZE5EYRyRKRrLy8vHZsug1NqobqE0FSrCUCY0xoak8i8AQ+lMat8umos+YbwEBVHQO8Czzd0kyqukBVJ6nqpLS0tGPbYrWb09yrhgorakmICicy3O4hMMaEpvac/d4G/iEi54nIecALwFvtWG4PEPgLP8Md10BVC1S1xh18DJjYjvUem/qrhtw2gqLKWpKtWsgYE8Lakwj+E3gfuMl9fUXjG8xaswIYKiKDRCQSuBpYFDiDiPQNGJwJbGxP0MekugTCoyHCeRJZYUUtPa1ayBgTwtpz1ZBfRD4HBgPfAlKBV9qxnFdEbgHeATzAE6q6XkTuA7JUdRFwq4jMxOnDqBCYe9R70l7VJY2eTFZYUUvvRHs8pTEmdLWaCERkGDDbfeXj3D+Aqp7T3pWr6mJgcZNx9wa8/xnwsyML+Rg16WeoqKKWkX0T21jAGGO6t7ZKBJuAj4BLVHUbgIjMOy5RBVNAP0OqSkFFLclxVjVkjAldbbURXAbsAz4Qkb+5DcVd/8nu1SUNVwxV1fmo8fqtjcAYE9JaTQSq+rqqXg2MAD7A6Wqil4g8KiJfO14BdriAp5PV30OQYiUCY0wIO+xVQ6paoap/d59dnAGsxrmSqGsKaCyuTwQ9LREYY0LYEd1FpapF7s1d5wUroKALaCOoTwTJcRGdGZExxnSq0Lqd1lsD3uqGRFBUWZ8IojozKmOM6VShlQiqGz+LoKDcTQTWWGyMCWEhlggadzhXVFmLJ0xIjGnPo5uNMaZ7Cq1EUNO859GesZEE9KlnjDEhJ7QSQZOH0hRW1FpDsTEm5IVmIqivGqqos7uKjTEhL8QSQZPG4ooaSwTGmJAXYomgvkRQ/yyCOutewhgT8kIvEUgYRMbj8yvFlbXWvYQxJuSFViKo72dIhJKqOvxq3UsYY0xoJYIW+hmyNgJjTKgLaiIQkRkisllEtonIXW3Md7mIqIhMCmY8gf0MHepewhKBMSa0BS0RiIgHeBi4EBgFzBaRUS3MlwDcBnwerFgaBDydrL57CWssNsaEumCWCCYD21R1h6rWAguBWS3Mdz/w30B1EGNxWInAGGOaCWYiSAd2BwznuuMaiMgEIFNV/9XWikTkRhHJEpGsvLy8o4+oxS6oLREYY0JbpzUWi0gY8Hvgx4eb130GwiRVnZSWlnb0G23ydLLYSA/REZ6jX58xxnQDwUwEe4DMgOEMd1y9BOBkYKmIZAOnAYuC1mDs9zmJwL1qqMjtcM4YY0JdMBPBCmCoiAwSkUjgamBR/URVLVHVVFUdqKoDgc+AmaqaFZRoahp3L1FYWUtKvCUCY4wJWiJQVS9wC/AOsBF4UVXXi8h9IjIzWNttVXXLXVAbY0yoC+oTWVR1MbC4ybh7W5l3ejBjOdTh3KEbyganxQd1k8YY0xWEzp3FzbqgrrUrhowxhhBNBNV1PipqfZYIjDGGUEoE9Y3FUYkNN5NZG4ExxoRSIggoEdjNZMYYc0joJIIeGTBsBkQlWiIwxpgAQb1q6IQy8lLnhXUvYYwxgUKnRBCguLIOgKTYiE6OxBhjOl9IJoKSKicR9IixRGCMMSGZCEqr6oiN9BDhCcndN8aYRkLyTFhSVWelAWOMcVkiMMaYEBeyiSAx2hKBMcZAKCcCKxEYYwwQoomg1KqGjDGmQUgmAmsjMMaYQ4KaCERkhohsFpFtInJXC9NvEpGvRGSNiHwsIqOCGQ9Anc9PRa3PEoExxriClghExAM8DFwIjAJmt3Ci/7uqnqKq44Df4jzMPqhKG24mC53eNYwxpi3BLBFMBrap6g5VrQUWArMCZ1DV0oDBOECDGA8ApdVeAHpY9xLGGAMEt9O5dGB3wHAuMKXpTCLyQ+AOIBI4N4jxANa9hDHGNNXpjcWq+rCqDgb+E7inpXlE5EYRyRKRrLy8vGPaXn0isPsIjDHGEcxEsAfIDBjOcMe1ZiHwjZYmqOoCVZ2kqpPS0tKOKSgrERhjTGPBTAQrgKEiMkhEIoGrgUWBM4jI0IDBi4GtQYwHsERgjDFNBa2NQFW9InIL8A7gAZ5Q1fUich+QpaqLgFtE5HygDigC5gQrnnr1Vw3ZncXGGOMI6jWUqroYWNxk3L0B728L5vZbUlJVR1R4GNERnuO9aWOMOSF1emPx8VZSaXcVG2NMoJC7q6q02hKB6R7q6urIzc2lurq6s0MxJ5Do6GgyMjKIiGj/eS7kEoH1M2S6i9zcXBISEhg4cCAi0tnhmBOAqlJQUEBubi6DBg1q93KhVzVkicB0E9XV1aSkpFgSMA1EhJSUlCMuJYZkIrArhkx3YUnANHU034mQTARWIjDGmENCKhH4/EpZtddKBMZ0gIKCAsaNG8e4cePo06cP6enpDcO1tbVtLpuVlcWtt9562G1MnTq1o8IF4Pbbbyc9PR2/39+h6+3qQqqxuKza7io2pqOkpKSwZs0aAObPn098fDx33nlnw3Sv10t4eMunmEmTJjFp0qTDbmP58uUdEyzg9/t57bXXyMzM5MMPP+Scc87psHUHamu/T1RdK9pjZN1LmO7qV2+sZ8Pe0sPPeARG9Uvkl5eOPqJl5s6dS3R0NKtXr2batGlcffXV3HbbbVRXVxMTE8OTTz7J8OHDWbp0KQ899BBvvvkm8+fPJycnhx07dpCTk8Ptt9/eUFqIj4+nvLycpUuXMn/+fFJTU1m3bh0TJ07kueeeQ0RYvHgxd9xxB3FxcUybNo0dO3bw5ptvNott6dKljB49mquuuooXXnihIREcOHCAm266iR07dgDw6KOPMnXqVJ555hkeeughRIQxY8bw7LPPMnfuXC655BKuuOKKZvH94he/oGfPnmzatIktW7bwjW98g927d1NdXc1tt93GjTfeCMDbb7/N3Xffjc/nIzU1lXfffZfhw4ezfPly0tLS8Pv9DBs2jE8//ZRj7VutvSwRGGM6VG5uLsuXL8fj8VBaWspHH31EeHg4S5Ys4e677+aVV15ptsymTZv44IMPKCsrY/jw4dx8883NroNfvXo169evp1+/fkybNo1PPvmESZMm8YMf/IBly5YxaNAgZs+e3WpcL7zwArNnz2bWrFncfffd1NXVERERwa233srZZ5/Na6+9hs/no7y8nPXr1/PrX/+a5cuXk5qaSmFh4WH3e9WqVaxbt67hss0nnniC5ORkqqqqOPXUU7n88svx+/3ccMMNDfEWFhYSFhbGddddx/PPP8/tt9/OkiVLGDt27HFLAhBiiaC0yn0ojSUC080c6S/3YLryyivxeJwuXEpKSpgzZw5bt25FRKirq2txmYsvvpioqCiioqLo1asXBw4cICMjo9E8kydPbhg3btw4srOziY+P56STTmo4+c6ePZsFCxY0W39tbS2LFy/m97//PQkJCUyZMoV33nmHSy65hPfff59nnnkGAI/HQ48ePXjmmWe48sorSU1NBSA5Ofmw+z158uRG1+7/+c9/5rXXXgNg9+7dbN26lby8PM4666yG+erX+93vfpdZs2Zx++2388QTT/Cd73znsNvrSCGVCKxEYEzwxcXFNbz/xS9+wTnnnMNrr71GdnY206dPb3GZqKiohvcejwev13tU87TmnXfeobi4mFNOOQWAyspKYmJiuOSSS9q9DoDw8PCGhma/39+oUTxwv5cuXcqSJUv49NNPiY2NZfr06W1e25+ZmUnv3r15//33+eKLL3j++eePKK5jFVJXDVkiMOb4KikpIT09HYCnnnqqw9c/fPhwduzYQXZ2NgD/+Mc/WpzvhRde4LHHHiM7O5vs7Gx27tzJu+++S2VlJeeddx6PPvooAD6fj5KSEs4991xeeuklCgoKABqqhgYOHMjKlSsBWLRoUaslnJKSEnr27ElsbCybNm3is88+A+C0005j2bJl7Ny5s9F6Ab7//e9z3XXXNSpRHS8hmQgS7cH1xhwXP/3pT/nZz37G+PHjj+gXfHvFxMTwyCOPMGPGDCZOnEhCQgI9evRoNE9lZSVvv/02F198ccO4uLg4zjjjDN544w3+9Kc/8cEHH3DKKacwceJENmzYwOjRo/n5z3/O2WefzdixY7njjjsAuOGGG/jwww8ZO3Ysn376aaNSQKAZM2bg9XoZOXIkd911F6eddhoAaWlpLFiwgMsuu4yxY8dy1VVXNSwzc+ZMysvLj3u1EICoBv158R1q0qRJmpWVdVTLPvjWJh7/eAdbfn2h3ZFpuryNGzcycuTIzg6j05WXlxMfH4+q8sMf/pChQ4cyb968zg7riGVlZTFv3jw++uijY15XS98NEVmpqi1esxtyJYIeMRGWBIzpRv72t78xbtw4Ro8eTUlJCT/4wQ86O6Qj9uCDD3L55ZfzwAMPdMr2g5oIRGSGiGwWkW0iclcL0+8QkQ0islZE3hORAcGMp9T6GTKm25k3bx5r1qxhw4YNPP/888TGxnZ2SEfsrrvuYteuXZxxxhmdsv2gJQIR8QAPAxcCo4DZIjKqyWyrgUmqOgZ4GfhtsOIB62fIGGNaEswSwWRgm6ruUNVaYCEwK3AGVf1AVSvdwc+ADILIHkpjjDHNBTMRpAO7A4Zz3XGt+R7wVksTRORGEckSkay8vLyjDshKBMYY09wJ0VgsItcBk4DftTRdVReo6iRVnXQst11bIjDGmOaCmQj2AJkBwxnuuEZE5Hzg58BMVa0JVjB+v1JqicCYDnPOOefwzjvvNBr3xz/+kZtvvrnVZaZPn0795d8XXXQRxcXFzeaZP38+Dz30UJvbfv3119mwYUPD8L333suSJUuOJPw2hVp31cFMBCuAoSIySEQigauBRYEziMh44P9wksDBIMZCea0Xv0JitCUCYzrC7NmzWbhwYaNxCxcubLPjt0CLFy8mKSnpqLbdNBHcd999nH/++Ue1rqaadlcdLMG4we5oBe0WW1X1isgtwDuAB3hCVdeLyH1AlqouwqkKigdecq/tz1HVmcGIp6TSupcw3dhbd8H+rzp2nX1OgQsfbHXyFVdcwT333ENtbS2RkZFkZ2ezd+9ezjzzTG6++WZWrFhBVVUVV1xxBb/61a+aLT9w4ECysrJITU3lN7/5DU8//TS9evUiMzOTiRMnAs49AgsWLKC2tpYhQ4bw7LPPsmbNGhYtWsSHH37Ir3/9a1555RXuv//+hu6h33vvPe688068Xi+nnnoqjz76KFFRUQwcOJA5c+bwxhtvUFdXx0svvcSIESOaxRWK3VUHta8FVV0MLG4y7t6A9x2TwtvhUPcSlgiM6QjJyclMnjyZt956i1mzZrFw4UK+9a1vISL85je/ITk5GZ/Px3nnncfatWsZM2ZMi+tZuXIlCxcuZM2aNXi9XiZMmNCQCC677DJuuOEGAO655x4ef/xxfvSjHzFz5sxGJ9p61dXVzJ07l/fee49hw4Zx/fXX8+ijj3L77bcDkJqayqpVq3jkkUd46KGHeOyxx5rFE4rdVYdMpzul1uGc6c7a+OUeTPXVQ/WJ4PHHHwfgxRdfZMGCBXi9Xvbt28eGDRtaTQQfffQR3/zmNxtuBJs581ClwLp167jnnnsoLi6mvLycr3/9623Gs3nzZgYNGsSwYcMAmDNnDg8//HBDIrjssssAmDhxIq+++mqz5UO1u+qQSQTW86gxHW/WrFnMmzePVatWUVlZycSJE9m5cycPPfQQK1asoGfPnsydO7fNLpjbMnfuXF5//XXGjh3LU089xdKlS48p3vqurFvrxjpUu6s+IS4fPR5K659XHGuJwJiOEh8fzznnnMN3v/vdhkbi0tJS4uLi6NGjBwcOHOCtt1q8PajBWWedxeuvv05VVRVlZWW88cYbDdPKysro27cvdXV1jU56CQkJlJWVNVvX8OHDyc7OZtu2bQA8++yznH322e3en1DtrjpkEoGVCIwJjtmzZ/Pll182JIKxY8cyfvx4RowYwTXXXMO0adPaXH7ChAlcddVVjB07lgsvvJBTTz21Ydr999/PlClTmDZtWqOG3auvvprf/e53jB8/nu3btzeMj46O5sknn+TKK6/klFNOISwsjJtuuqld+xHK3VWHTDfUWw+UsWZ3MVdMzLDeR023YN1Qh6b2dFd9pN1Qh0wbwdDeCQztndDZYRhjzFF78MEHefTRRzv8UZYhUzVkjDFdXbC6q7ZEYEwX1tWqdk3wHc13whKBMV1UdHQ0BQUFlgxMA1WloKCA6OjoI1ouZNoIjOluMjIyyM3N5Vi6ZjfdT3R0NBkZR/ZoF0sExnRRERERje5QNeZoWdWQMcaEOEsExhgT4iwRGGNMiOtydxaLSB6w6ygXTwXyOzCcriIU9zsU9xlCc79DcZ/hyPd7gKq22Gd1l0sEx0JEslq7xbo7C8X9DsV9htDc71DcZ+jY/baqIWOMCXGWCIwxJsSFWiJY0NkBdJJQ3O9Q3GcIzf0OxX2GDtzvkGojMMYY01yolQiMMcY0YYnAGGNCXMgkAhGZISKbRWSbiNzV2fEEg4hkisgHIrJBRNaLyG3u+GQReVdEtrp/e3Z2rB1NRDwislpE3nSHB4nI5+7x/oeIRHZ2jB1NRJJE5GUR2SQiG0Xk9BA51vPc7/c6EXlBRKK72/EWkSdE5KCIrAsY1+KxFcef3X1fKyITjnR7IZEIRMQDPAxcCIwCZovIqM6NKii8wI9VdRRwGvBDdz/vAt5T1aHAe+5wd3MbsDFg+L+BP6jqEKAI+F6nRBVcfwLeVtURwFic/e/Wx1pE0oFbgUmqejLgAa6m+x3vp4AZTca1dmwvBIa6rxuBR490YyGRCIDJwDZV3aGqtcBCYFYnx9ThVHWfqq5y35fhnBjScfb1aXe2p4FvdE6EwSEiGcDFwGPusADnAi+7s3THfe4BnAU8DqCqtapaTDc/1q5wIEZEwoFYYB/d7Hir6jKgsMno1o7tLOAZdXwGJIlI3yPZXqgkgnRgd8Bwrjuu2xKRgcB44HOgt6rucyftB3p3UljB8kfgp4DfHU4BilXV6w53x+M9CMgDnnSrxB4TkTi6+bFW1T3AQ0AOTgIoAVbS/Y83tH5sj/n8FiqJIKSISDzwCnC7qpYGTlPneuFuc82wiFwCHFTVlZ0dy3EWDkwAHlXV8UAFTaqButuxBnDrxWfhJMJ+QBzNq1C6vY4+tqGSCPYAmQHDGe64bkdEInCSwPOq+qo7+kB9UdH9e7Cz4guCacBMEcnGqfI7F6fuPMmtOoDuebxzgVxV/dwdfhknMXTnYw1wPrBTVfNUtQ54Fec70N2PN7R+bI/5/BYqiWAFMNS9siASp3FpUSfH1OHcuvHHgY2q+vuASYuAOe77OcA/j3dswaKqP1PVDFUdiHNc31fVa4EPgCvc2brVPgOo6n5gt4gMd0edB2ygGx9rVw5wmojEut/3+v3u1sfb1dqxiZbpuAAAALZJREFUXQRc7149dBpQElCF1D6qGhIv4CJgC7Ad+HlnxxOkfTwDp7i4Fljjvi7CqTN/D9gKLAGSOzvWIO3/dOBN9/1JwBfANuAlIKqz4wvC/o4Dstzj/TrQMxSONfArYBOwDngWiOpuxxt4AacNpA6n9Pe91o4tIDhXRW4HvsK5ouqItmddTBhj/r8dO6ABAABAENa/tQlMwF+CDeIqawiAQwgA4oQAIE4IAOKEACBOCADihAAgbjiULifEr80wAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftCYvBkKZmGa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "27a3fc92-78d9-4e6b-ac42-6d05e8267fa0"
      },
      "source": [
        "# plot graph of cross-validated losses\n",
        "loss = np.mean(history_map['loss'], axis=0)\n",
        "val_loss = np.mean(history_map['val_loss'], axis=0)\n",
        "\n",
        "plt.figure(2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "#plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}