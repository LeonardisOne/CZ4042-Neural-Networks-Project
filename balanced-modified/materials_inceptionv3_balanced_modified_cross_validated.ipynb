{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "materials-inceptionv3-balanced-modified-cross-validated.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMLomdup7Sb+r20sVoFbRov"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERKJLyg2z-Uq"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fIbsvD01EQm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21526b43-5b1f-4ee9-e00c-6ba0ba40ee46"
      },
      "source": [
        "import pathlib\n",
        "\n",
        "# set path to FMD image directory\n",
        "data_dir = pathlib.Path(\"image\")\n",
        "\n",
        "# total no. of images in FMD dataset\n",
        "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
        "print(image_count)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL3rBqoe3sK5"
      },
      "source": [
        "# set batch size for training\n",
        "batch_size = 16\n",
        "\n",
        "# set dimensions to change images into for training\n",
        "# 299x299 images is required for pre-trained models like Inception V3\n",
        "img_height = 299\n",
        "img_width = 299"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFKuhNRxqxcB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7015588a-d435-4855-ce02-9d085ff9c4d1"
      },
      "source": [
        "# get class names from the folder names in data_dir\n",
        "class_names = np.array(sorted([item.name for item in data_dir.glob('*') if item.name != \"LICENSE.txt\"]))\n",
        "print(class_names)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['fabric' 'foliage' 'glass' 'leather' 'metal' 'paper' 'plastic' 'stone'\n",
            " 'water' 'wood']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap_FpvbEqWrS"
      },
      "source": [
        "# create list containing the dataset for each class\n",
        "ds_each_class = [tf.data.Dataset.list_files(str(data_dir/f'{class_name}/*.jpg'), shuffle=False) for class_name in class_names]\n",
        "\n",
        "# shuffle the 100 images in each class with the random seed value of 123 before training\n",
        "for index, ds in enumerate(ds_each_class):\n",
        "  ds_each_class[index] = ds.shuffle(image_count//10, seed=123, reshuffle_each_iteration=False)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty_LijJpqbEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8cc0a27-dcb7-41e5-d4c0-5e62a7d57d84"
      },
      "source": [
        "# display some samples from a class to verify each class dataset contains only the class images\n",
        "for f in ds_each_class[0].take(10):\n",
        "  print(f.numpy())\n",
        "\n",
        "for f in ds_each_class[1].take(10):\n",
        "  print(f.numpy())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'FMD/image/fabric/fabric_moderate_037_new.jpg'\n",
            "b'FMD/image/fabric/fabric_moderate_004_new.jpg'\n",
            "b'FMD/image/fabric/fabric_moderate_008_new.jpg'\n",
            "b'FMD/image/fabric/fabric_object_003_new.jpg'\n",
            "b'FMD/image/fabric/fabric_object_017_new.jpg'\n",
            "b'FMD/image/fabric/fabric_object_001_new.jpg'\n",
            "b'FMD/image/fabric/fabric_moderate_032_new.jpg'\n",
            "b'FMD/image/fabric/fabric_moderate_030_new.jpg'\n",
            "b'FMD/image/fabric/fabric_moderate_038_new.jpg'\n",
            "b'FMD/image/fabric/fabric_object_009_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_037_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_004_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_008_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_053_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_067_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_051_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_032_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_030_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_038_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_059_new.jpg'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACILObxurdXN"
      },
      "source": [
        "# split first class dataset into 5 equal sized partitions\n",
        "# then for remaining classes' datasets do the same and add to corresponding partition\n",
        "# for 5-fold cross validation\n",
        "A = ds_each_class[0].shard(num_shards=5, index=0)\n",
        "B = ds_each_class[0].shard(num_shards=5, index=1)\n",
        "C = ds_each_class[0].shard(num_shards=5, index=2)\n",
        "D = ds_each_class[0].shard(num_shards=5, index=3)\n",
        "E = ds_each_class[0].shard(num_shards=5, index=4)\n",
        "for i in range(1, 10):\n",
        "  A = A.concatenate(ds_each_class[i].shard(num_shards=5, index=0))\n",
        "  B = B.concatenate(ds_each_class[i].shard(num_shards=5, index=1))\n",
        "  C = C.concatenate(ds_each_class[i].shard(num_shards=5, index=2))\n",
        "  D = D.concatenate(ds_each_class[i].shard(num_shards=5, index=3))\n",
        "  E = E.concatenate(ds_each_class[i].shard(num_shards=5, index=4))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9oYdHkhrwdz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c2747e5-ea0b-4a17-c95a-993091210109"
      },
      "source": [
        "# check no. of samples in each partition is the same\n",
        "print(A.cardinality().numpy())\n",
        "print(B.cardinality().numpy())\n",
        "print(C.cardinality().numpy())\n",
        "print(D.cardinality().numpy())\n",
        "print(E.cardinality().numpy())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BdD3FMHtGRV"
      },
      "source": [
        "def get_label(file_path):\n",
        "  # convert the path to a list of path components\n",
        "  parts = tf.strings.split(file_path, os.path.sep)\n",
        "  # The second to last is the class-directory\n",
        "  one_hot = parts[-2] == class_names\n",
        "  # Integer encode the label\n",
        "  return tf.argmax(one_hot)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfWduaL4tKDV"
      },
      "source": [
        "def decode_img(img):\n",
        "  # convert the compressed string to a 3D uint8 tensor\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\n",
        "  # resize the image to the desired size\n",
        "  return tf.image.resize(img, [img_height, img_width])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bGGe0KltMTC"
      },
      "source": [
        "def process_path(file_path):\n",
        "  label = get_label(file_path)\n",
        "  # load the raw data from the file as a string\n",
        "  img = tf.io.read_file(file_path)\n",
        "  img = decode_img(img)\n",
        "  return img, label"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcyZLwRxt1dy"
      },
      "source": [
        "# prompt the tf.data runtime to tune the number of elements to prefetch dynamically at runtime\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkBqAQlHtblh"
      },
      "source": [
        "# use the path of image to load the image into each partition of the dataset\n",
        "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
        "A = A.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "B = B.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "C = C.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "D = D.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "E = E.map(process_path, num_parallel_calls=AUTOTUNE)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc9pmaQ5t9H5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b3d8027-496e-449f-d9cd-9c831e752906"
      },
      "source": [
        "for image, label in A.take(1):\n",
        "  print(\"Image shape: \", image.numpy().shape)\n",
        "  print(\"Label: \", label.numpy())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image shape:  (299, 299, 3)\n",
            "Label:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_T2KNdGub1X"
      },
      "source": [
        "# shuffle, batch, and prefetch the dataset\n",
        "def configure_for_performance(ds):\n",
        "  ds = ds.cache()\n",
        "  ds = ds.shuffle(buffer_size=1000)\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
        "  return ds"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWcojoVRuok5"
      },
      "source": [
        "# map the dataset partitions to integers for building training/test sets during cross-validation\n",
        "ds_fold_dict = {0:A, 1:B, 2:C, 3:D, 4:E}"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx3xKoMZU9Yv"
      },
      "source": [
        "# normalise the input values to the pre-trained model's required range of values\n",
        "preprocess_input = keras.applications.inception_v3.preprocess_input"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVOP5fIPwEx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f2b3fa2-4292-47b6-af64-4a968b1aaf90"
      },
      "source": [
        "# get pre-trained model\n",
        "base_model = keras.applications.InceptionV3(include_top=False, input_shape=(img_height, img_width, 3))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS2kAceGVW0e"
      },
      "source": [
        "# don't train base model weights\n",
        "base_model.trainable = False"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGkReMX60ScJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83a6dfb5-4f88-49f0-a5fd-562018f17bc8"
      },
      "source": [
        "base_model.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"inception_v3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 299, 299, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 149, 149, 32) 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 149, 149, 32) 96          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 149, 149, 32) 0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 147, 147, 32) 9216        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 147, 147, 32) 96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 147, 147, 32) 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 147, 147, 64) 18432       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 147, 147, 64) 192         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 147, 147, 64) 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 73, 73, 64)   0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 73, 73, 80)   240         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 73, 73, 80)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 71, 71, 192)  138240      activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 71, 71, 192)  576         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 71, 71, 192)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 35, 35, 96)   55296       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 35, 35, 48)   144         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 35, 35, 96)   288         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 35, 35, 48)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 35, 35, 96)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 35, 35, 192)  0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 35, 35, 64)   76800       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 35, 35, 96)   82944       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 35, 35, 64)   192         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 35, 35, 64)   192         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 35, 35, 32)   96          conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 35, 35, 64)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 35, 35, 64)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 35, 35, 32)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_5[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "                                                                 activation_10[0][0]              \n",
            "                                                                 activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 35, 35, 96)   55296       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 35, 35, 48)   144         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 35, 35, 96)   288         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 35, 35, 48)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 35, 35, 96)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 35, 35, 64)   76800       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 35, 35, 96)   82944       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 35, 35, 64)   192         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 35, 35, 64)   192         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 35, 35, 64)   192         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 35, 35, 64)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 35, 35, 64)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 35, 35, 64)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_12[0][0]              \n",
            "                                                                 activation_14[0][0]              \n",
            "                                                                 activation_17[0][0]              \n",
            "                                                                 activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 35, 35, 96)   55296       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 35, 35, 48)   144         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 35, 35, 96)   288         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 35, 35, 48)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 35, 35, 96)   0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 35, 35, 64)   76800       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 35, 35, 96)   82944       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 35, 35, 64)   192         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 35, 35, 64)   192         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 35, 35, 64)   0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 35, 35, 64)   0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_19[0][0]              \n",
            "                                                                 activation_21[0][0]              \n",
            "                                                                 activation_24[0][0]              \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 35, 35, 64)   192         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 35, 35, 64)   0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 35, 35, 96)   55296       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 35, 35, 96)   288         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 35, 35, 96)   0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 17, 17, 96)   82944       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 17, 17, 384)  1152        conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 17, 17, 96)   288         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 17, 17, 384)  0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 17, 17, 96)   0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_26[0][0]              \n",
            "                                                                 activation_29[0][0]              \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 17, 17, 128)  384         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 17, 17, 128)  0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 17, 17, 128)  114688      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 17, 17, 128)  384         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 17, 17, 128)  0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 17, 17, 128)  114688      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 17, 17, 192)  172032      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 17, 17, 192)  172032      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 17, 17, 192)  576         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 17, 17, 192)  576         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 17, 17, 192)  576         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 17, 17, 192)  0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 17, 17, 192)  0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 17, 17, 192)  0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_30[0][0]              \n",
            "                                                                 activation_33[0][0]              \n",
            "                                                                 activation_38[0][0]              \n",
            "                                                                 activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 17, 17, 160)  480         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 17, 17, 160)  0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 17, 17, 160)  179200      activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 17, 17, 160)  480         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 17, 17, 160)  0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 17, 17, 160)  179200      activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 17, 17, 192)  215040      activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 17, 17, 192)  215040      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 17, 17, 192)  576         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 17, 17, 192)  576         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 17, 17, 192)  0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 17, 17, 192)  0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_40[0][0]              \n",
            "                                                                 activation_43[0][0]              \n",
            "                                                                 activation_48[0][0]              \n",
            "                                                                 activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 17, 17, 160)  480         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 17, 17, 160)  0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 17, 17, 160)  179200      activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 17, 17, 160)  480         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 17, 17, 160)  0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 17, 17, 160)  179200      activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 17, 17, 192)  215040      activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 17, 17, 192)  215040      activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 17, 17, 192)  576         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 17, 17, 192)  576         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 17, 17, 192)  0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 17, 17, 192)  0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_50[0][0]              \n",
            "                                                                 activation_53[0][0]              \n",
            "                                                                 activation_58[0][0]              \n",
            "                                                                 activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 17, 17, 192)  258048      activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 17, 17, 192)  258048      activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_60[0][0]              \n",
            "                                                                 activation_63[0][0]              \n",
            "                                                                 activation_68[0][0]              \n",
            "                                                                 activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 17, 17, 192)  576         conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 17, 17, 192)  0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 17, 17, 192)  258048      activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 8, 8, 320)    552960      activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 8, 8, 192)    331776      activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 8, 8, 320)    960         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 8, 8, 192)    576         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 8, 8, 320)    0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 8, 8, 192)    0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_71[0][0]              \n",
            "                                                                 activation_75[0][0]              \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 8, 8, 448)    1344        conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 8, 8, 448)    0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 8, 8, 384)    1548288     activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 8, 8, 384)    1152        conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 8, 8, 384)    1152        conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 8, 8, 384)    0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 8, 8, 384)    0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 8, 8, 384)    442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 8, 8, 384)    442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 8, 8, 320)    960         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 8, 8, 192)    576         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 8, 8, 320)    0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_78[0][0]              \n",
            "                                                                 activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 8, 8, 768)    0           activation_82[0][0]              \n",
            "                                                                 activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 8, 8, 192)    0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_76[0][0]              \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate[0][0]                \n",
            "                                                                 activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, 8, 8, 448)    1344        conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 8, 8, 448)    0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 8, 8, 384)    1548288     activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, 8, 8, 384)    1152        conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, 8, 8, 384)    1152        conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 8, 8, 384)    0           batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 8, 8, 384)    0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 8, 8, 384)    442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 8, 8, 384)    442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_8 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 8, 8, 320)    960         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, 8, 8, 192)    576         conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 8, 8, 320)    0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_87[0][0]              \n",
            "                                                                 activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_91[0][0]              \n",
            "                                                                 activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 8, 8, 192)    0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_85[0][0]              \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_1[0][0]              \n",
            "                                                                 activation_93[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 21,802,784\n",
            "Trainable params: 0\n",
            "Non-trainable params: 21,802,784\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzhaWb2aX2if"
      },
      "source": [
        "# set a low learning rate to avoid overfitting too quickly\n",
        "base_learning_rate = 0.0001\n",
        "\n",
        "# create the model to train using the pre-trained model as base model\n",
        "def create_model():\n",
        "  # generate additional training data from input training data by augmenting them using random flip, rotation & zoom\n",
        "  data_augmentation = keras.Sequential(\n",
        "    [\n",
        "      layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n",
        "                                                  input_shape=(img_height, \n",
        "                                                                img_width,\n",
        "                                                                3)),\n",
        "      layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "      layers.experimental.preprocessing.RandomZoom(0.1),\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  # average over the spatial locations to convert the features to a single vector per image\n",
        "  global_average_layer = keras.layers.GlobalAveragePooling2D()\n",
        "  # increase network depth by adding additional fully connected layer of size 128 with ReLU activation\n",
        "  fully_connected_layer = keras.layers.Dense(128, activation='relu')\n",
        "  # convert these features into a single prediction per image\n",
        "  prediction_layer = keras.layers.Dense(10)\n",
        "\n",
        "  # Build a model by chaining together the layers using the Keras Functional API.\n",
        "  inputs = keras.Input(shape=(img_height, img_width, 3))\n",
        "  x = data_augmentation(inputs)\n",
        "  x = preprocess_input(x)\n",
        "  x = base_model(x, training=False) # use training=False as the base model contains a BatchNormalization layer\n",
        "  x = global_average_layer(x)\n",
        "  x = keras.layers.Dropout(0.2)(x) # add dropout to fully connected layer to reduce overfitting\n",
        "  x = fully_connected_layer(x)\n",
        "  x = keras.layers.Dropout(0.2)(x) # add dropout to fully connected layer to reduce overfitting\n",
        "  outputs = prediction_layer(x)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  optimizer = keras.optimizers.Adam(lr=base_learning_rate)\n",
        "  loss= keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  # compile model with Adam optimizer with specified learning rate\n",
        "  model.compile(optimizer=optimizer,\n",
        "                loss=loss,\n",
        "                metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-5TEZRgY2l_"
      },
      "source": [
        "no_epochs = 100"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya698zRbu7Ep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db3641b5-86bc-4d93-ea9d-6539311c9b8a"
      },
      "source": [
        "# store results to plot graphs and get cross-validated accuracies\n",
        "history_map = {}\n",
        "base_model_acc_list = []\n",
        "final_acc_list = []\n",
        "\n",
        "# do 5-fold cross-validation\n",
        "for i in range(5):\n",
        "  print('fold', i + 1)\n",
        "  temp_dict = ds_fold_dict.copy()\n",
        "  # get test set for this iteration\n",
        "  current_val_ds = temp_dict[i]\n",
        "\n",
        "  # get training set for this iteration from remaining data samples\n",
        "  del temp_dict[i]\n",
        "  current_train_ds = None\n",
        "  for ds_shard in temp_dict.values():\n",
        "    if current_train_ds is None:\n",
        "      current_train_ds = ds_shard\n",
        "    else:\n",
        "      current_train_ds = current_train_ds.concatenate(ds_shard)\n",
        "  \n",
        "  # configure both training and test sets to improve performance\n",
        "  current_train_ds = configure_for_performance(current_train_ds)\n",
        "  current_val_ds = configure_for_performance(current_val_ds)\n",
        "\n",
        "  # create a new model\n",
        "  model = create_model()\n",
        "  # get initial test accuracy\n",
        "  base_model_acc_list.append(model.evaluate(current_val_ds)[1])\n",
        "  # train for specified epochs\n",
        "  history = model.fit(current_train_ds,\n",
        "                    epochs=no_epochs,\n",
        "                    validation_data=current_val_ds)\n",
        "  \n",
        "  # save results\n",
        "  if i == 0:\n",
        "    history_map['accuracy'] = [history.history['accuracy']]\n",
        "    history_map['val_accuracy'] = [history.history['val_accuracy']]\n",
        "    history_map['loss'] = [history.history['loss']]\n",
        "    history_map['val_loss'] = [history.history['val_loss']]\n",
        "  else:\n",
        "    history_map['accuracy'].append(history.history['accuracy'])\n",
        "    history_map['val_accuracy'].append(history.history['val_accuracy'])\n",
        "    history_map['loss'].append(history.history['loss'])\n",
        "    history_map['val_loss'].append(history.history['val_loss'])\n",
        "  \n",
        "  # get final test accuracy by taking the max accuracy over the whole training\n",
        "  final_acc_list.append(np.amax(history.history['val_accuracy']))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fold 1\n",
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential (Sequential)      (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv (TensorF [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub (TensorFlowO [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "inception_v3 (Functional)    (None, 8, 8, 2048)        21802784  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               262272    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 22,066,346\n",
            "Trainable params: 263,562\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 1s 95ms/step - loss: 2.4700 - accuracy: 0.1050\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 4s 88ms/step - loss: 2.3345 - accuracy: 0.1612 - val_loss: 2.0271 - val_accuracy: 0.3250\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 4s 87ms/step - loss: 1.9591 - accuracy: 0.3400 - val_loss: 1.7477 - val_accuracy: 0.5500\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 4s 87ms/step - loss: 1.6956 - accuracy: 0.4737 - val_loss: 1.4796 - val_accuracy: 0.6650\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 4s 87ms/step - loss: 1.5013 - accuracy: 0.5450 - val_loss: 1.2705 - val_accuracy: 0.7400\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 4s 87ms/step - loss: 1.2790 - accuracy: 0.6212 - val_loss: 1.0912 - val_accuracy: 0.7900\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 4s 87ms/step - loss: 1.1346 - accuracy: 0.6725 - val_loss: 0.9732 - val_accuracy: 0.7800\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 4s 88ms/step - loss: 1.0406 - accuracy: 0.6762 - val_loss: 0.8868 - val_accuracy: 0.7900\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 4s 88ms/step - loss: 0.9389 - accuracy: 0.7225 - val_loss: 0.8245 - val_accuracy: 0.8050\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 4s 89ms/step - loss: 0.8988 - accuracy: 0.7038 - val_loss: 0.7730 - val_accuracy: 0.8250\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 4s 88ms/step - loss: 0.8114 - accuracy: 0.7600 - val_loss: 0.7380 - val_accuracy: 0.8150\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 4s 89ms/step - loss: 0.7595 - accuracy: 0.7713 - val_loss: 0.7090 - val_accuracy: 0.8350\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 4s 89ms/step - loss: 0.6814 - accuracy: 0.8037 - val_loss: 0.6749 - val_accuracy: 0.8400\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 4s 89ms/step - loss: 0.6846 - accuracy: 0.7975 - val_loss: 0.6556 - val_accuracy: 0.8500\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 5s 90ms/step - loss: 0.6409 - accuracy: 0.8150 - val_loss: 0.6320 - val_accuracy: 0.8200\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 4s 90ms/step - loss: 0.6143 - accuracy: 0.8175 - val_loss: 0.6144 - val_accuracy: 0.8250\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 5s 91ms/step - loss: 0.5704 - accuracy: 0.8388 - val_loss: 0.6229 - val_accuracy: 0.8400\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 5s 91ms/step - loss: 0.5652 - accuracy: 0.8363 - val_loss: 0.5943 - val_accuracy: 0.8300\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 5s 91ms/step - loss: 0.5336 - accuracy: 0.8450 - val_loss: 0.5855 - val_accuracy: 0.8300\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 5s 91ms/step - loss: 0.5076 - accuracy: 0.8450 - val_loss: 0.5776 - val_accuracy: 0.8200\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 5s 91ms/step - loss: 0.4880 - accuracy: 0.8625 - val_loss: 0.5721 - val_accuracy: 0.8400\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 5s 92ms/step - loss: 0.4854 - accuracy: 0.8600 - val_loss: 0.5640 - val_accuracy: 0.8300\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 5s 92ms/step - loss: 0.4424 - accuracy: 0.8625 - val_loss: 0.5607 - val_accuracy: 0.8350\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 5s 92ms/step - loss: 0.4636 - accuracy: 0.8587 - val_loss: 0.5584 - val_accuracy: 0.8400\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 5s 93ms/step - loss: 0.4421 - accuracy: 0.8712 - val_loss: 0.5457 - val_accuracy: 0.8350\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 5s 93ms/step - loss: 0.4286 - accuracy: 0.8675 - val_loss: 0.5580 - val_accuracy: 0.8250\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 5s 93ms/step - loss: 0.4250 - accuracy: 0.8788 - val_loss: 0.5494 - val_accuracy: 0.8200\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 5s 93ms/step - loss: 0.4056 - accuracy: 0.8687 - val_loss: 0.5406 - val_accuracy: 0.8250\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 5s 93ms/step - loss: 0.4093 - accuracy: 0.8838 - val_loss: 0.5491 - val_accuracy: 0.8350\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 5s 93ms/step - loss: 0.3918 - accuracy: 0.8775 - val_loss: 0.5324 - val_accuracy: 0.8300\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.3680 - accuracy: 0.8850 - val_loss: 0.5452 - val_accuracy: 0.8300\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.3359 - accuracy: 0.9075 - val_loss: 0.5474 - val_accuracy: 0.8400\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.3349 - accuracy: 0.9025 - val_loss: 0.5428 - val_accuracy: 0.8400\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.3548 - accuracy: 0.8900 - val_loss: 0.5460 - val_accuracy: 0.8200\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.3154 - accuracy: 0.9125 - val_loss: 0.5797 - val_accuracy: 0.8350\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.3376 - accuracy: 0.9075 - val_loss: 0.5277 - val_accuracy: 0.8350\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.3236 - accuracy: 0.9087 - val_loss: 0.5354 - val_accuracy: 0.8300\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 5s 93ms/step - loss: 0.3293 - accuracy: 0.8988 - val_loss: 0.5321 - val_accuracy: 0.8200\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 5s 93ms/step - loss: 0.2984 - accuracy: 0.9175 - val_loss: 0.5436 - val_accuracy: 0.8200\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 5s 93ms/step - loss: 0.3122 - accuracy: 0.9175 - val_loss: 0.5311 - val_accuracy: 0.8100\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 5s 93ms/step - loss: 0.2895 - accuracy: 0.9175 - val_loss: 0.5310 - val_accuracy: 0.8300\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 5s 93ms/step - loss: 0.2955 - accuracy: 0.9087 - val_loss: 0.5260 - val_accuracy: 0.8350\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 5s 93ms/step - loss: 0.2707 - accuracy: 0.9237 - val_loss: 0.5185 - val_accuracy: 0.8200\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.2682 - accuracy: 0.9187 - val_loss: 0.5240 - val_accuracy: 0.8250\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.2437 - accuracy: 0.9300 - val_loss: 0.5333 - val_accuracy: 0.8150\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2525 - accuracy: 0.9250 - val_loss: 0.5326 - val_accuracy: 0.8300\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2359 - accuracy: 0.9300 - val_loss: 0.5289 - val_accuracy: 0.8250\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.2343 - accuracy: 0.9388 - val_loss: 0.5353 - val_accuracy: 0.8350\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2383 - accuracy: 0.9287 - val_loss: 0.5352 - val_accuracy: 0.8300\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.2300 - accuracy: 0.9375 - val_loss: 0.5519 - val_accuracy: 0.8200\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 5s 93ms/step - loss: 0.2245 - accuracy: 0.9300 - val_loss: 0.5311 - val_accuracy: 0.8300\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.2410 - accuracy: 0.9275 - val_loss: 0.5198 - val_accuracy: 0.8300\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.2146 - accuracy: 0.9300 - val_loss: 0.5289 - val_accuracy: 0.8150\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.2291 - accuracy: 0.9375 - val_loss: 0.5405 - val_accuracy: 0.8300\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.2124 - accuracy: 0.9388 - val_loss: 0.5297 - val_accuracy: 0.8050\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.2211 - accuracy: 0.9337 - val_loss: 0.5295 - val_accuracy: 0.8100\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1996 - accuracy: 0.9438 - val_loss: 0.5390 - val_accuracy: 0.8200\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.2290 - accuracy: 0.9350 - val_loss: 0.5444 - val_accuracy: 0.8000\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.2085 - accuracy: 0.9375 - val_loss: 0.5593 - val_accuracy: 0.8250\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.2019 - accuracy: 0.9450 - val_loss: 0.5253 - val_accuracy: 0.8300\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1864 - accuracy: 0.9563 - val_loss: 0.5358 - val_accuracy: 0.8200\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 5s 93ms/step - loss: 0.1921 - accuracy: 0.9438 - val_loss: 0.5571 - val_accuracy: 0.8150\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.2030 - accuracy: 0.9488 - val_loss: 0.5473 - val_accuracy: 0.8250\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.2055 - accuracy: 0.9400 - val_loss: 0.5475 - val_accuracy: 0.8150\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1992 - accuracy: 0.9450 - val_loss: 0.5403 - val_accuracy: 0.8250\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1854 - accuracy: 0.9475 - val_loss: 0.5362 - val_accuracy: 0.8100\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 5s 93ms/step - loss: 0.1651 - accuracy: 0.9563 - val_loss: 0.5493 - val_accuracy: 0.8350\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 5s 93ms/step - loss: 0.1646 - accuracy: 0.9613 - val_loss: 0.5548 - val_accuracy: 0.8300\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1895 - accuracy: 0.9463 - val_loss: 0.5593 - val_accuracy: 0.8300\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 5s 93ms/step - loss: 0.1838 - accuracy: 0.9388 - val_loss: 0.5635 - val_accuracy: 0.8250\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1647 - accuracy: 0.9550 - val_loss: 0.5384 - val_accuracy: 0.8250\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1505 - accuracy: 0.9675 - val_loss: 0.5579 - val_accuracy: 0.8200\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1563 - accuracy: 0.9600 - val_loss: 0.5694 - val_accuracy: 0.8250\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1454 - accuracy: 0.9638 - val_loss: 0.5377 - val_accuracy: 0.8150\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1541 - accuracy: 0.9525 - val_loss: 0.5370 - val_accuracy: 0.8200\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1645 - accuracy: 0.9488 - val_loss: 0.5641 - val_accuracy: 0.8250\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1468 - accuracy: 0.9638 - val_loss: 0.5552 - val_accuracy: 0.8200\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1597 - accuracy: 0.9538 - val_loss: 0.5400 - val_accuracy: 0.8350\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1422 - accuracy: 0.9688 - val_loss: 0.5359 - val_accuracy: 0.8150\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1451 - accuracy: 0.9575 - val_loss: 0.5389 - val_accuracy: 0.8350\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1400 - accuracy: 0.9613 - val_loss: 0.5458 - val_accuracy: 0.8400\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 5s 93ms/step - loss: 0.1516 - accuracy: 0.9538 - val_loss: 0.5502 - val_accuracy: 0.8150\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1275 - accuracy: 0.9737 - val_loss: 0.5529 - val_accuracy: 0.8250\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1305 - accuracy: 0.9638 - val_loss: 0.5597 - val_accuracy: 0.8150\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1276 - accuracy: 0.9625 - val_loss: 0.5418 - val_accuracy: 0.8100\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1579 - accuracy: 0.9575 - val_loss: 0.5329 - val_accuracy: 0.8250\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1497 - accuracy: 0.9575 - val_loss: 0.5773 - val_accuracy: 0.8200\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1534 - accuracy: 0.9600 - val_loss: 0.5612 - val_accuracy: 0.8200\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1365 - accuracy: 0.9575 - val_loss: 0.5416 - val_accuracy: 0.8050\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1407 - accuracy: 0.9625 - val_loss: 0.5692 - val_accuracy: 0.8200\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1318 - accuracy: 0.9600 - val_loss: 0.5528 - val_accuracy: 0.8200\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1256 - accuracy: 0.9600 - val_loss: 0.5554 - val_accuracy: 0.8200\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1257 - accuracy: 0.9675 - val_loss: 0.5452 - val_accuracy: 0.8250\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1364 - accuracy: 0.9588 - val_loss: 0.5502 - val_accuracy: 0.8300\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1206 - accuracy: 0.9663 - val_loss: 0.5468 - val_accuracy: 0.8250\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1056 - accuracy: 0.9762 - val_loss: 0.5625 - val_accuracy: 0.8000\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1268 - accuracy: 0.9688 - val_loss: 0.5895 - val_accuracy: 0.8250\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 5s 93ms/step - loss: 0.1210 - accuracy: 0.9650 - val_loss: 0.5563 - val_accuracy: 0.8250\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.0943 - accuracy: 0.9762 - val_loss: 0.5506 - val_accuracy: 0.8000\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1211 - accuracy: 0.9700 - val_loss: 0.5735 - val_accuracy: 0.7950\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1299 - accuracy: 0.9663 - val_loss: 0.5576 - val_accuracy: 0.8050\n",
            "fold 2\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_1 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_1 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_1 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "inception_v3 (Functional)    (None, 8, 8, 2048)        21802784  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               262272    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 22,066,346\n",
            "Trainable params: 263,562\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 1s 64ms/step - loss: 2.4674 - accuracy: 0.0600\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 2.3393 - accuracy: 0.1525 - val_loss: 2.0476 - val_accuracy: 0.3100\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 1.9482 - accuracy: 0.3525 - val_loss: 1.7521 - val_accuracy: 0.5200\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 1.7066 - accuracy: 0.4625 - val_loss: 1.4570 - val_accuracy: 0.6250\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 1.4652 - accuracy: 0.5512 - val_loss: 1.2288 - val_accuracy: 0.7150\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 1.2352 - accuracy: 0.6313 - val_loss: 1.0628 - val_accuracy: 0.7350\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 1.0930 - accuracy: 0.6950 - val_loss: 0.9716 - val_accuracy: 0.7350\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 1.0044 - accuracy: 0.6913 - val_loss: 0.8910 - val_accuracy: 0.7400\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.9130 - accuracy: 0.7225 - val_loss: 0.8470 - val_accuracy: 0.7550\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.8140 - accuracy: 0.7725 - val_loss: 0.7909 - val_accuracy: 0.7600\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.7646 - accuracy: 0.7800 - val_loss: 0.7602 - val_accuracy: 0.7750\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.7538 - accuracy: 0.7625 - val_loss: 0.7457 - val_accuracy: 0.7700\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.6907 - accuracy: 0.8012 - val_loss: 0.7435 - val_accuracy: 0.7600\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.6564 - accuracy: 0.8150 - val_loss: 0.6911 - val_accuracy: 0.7650\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.6283 - accuracy: 0.8250 - val_loss: 0.6754 - val_accuracy: 0.7800\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.5805 - accuracy: 0.8350 - val_loss: 0.6658 - val_accuracy: 0.7750\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.5619 - accuracy: 0.8238 - val_loss: 0.6597 - val_accuracy: 0.7950\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.5713 - accuracy: 0.8438 - val_loss: 0.6408 - val_accuracy: 0.7850\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.5369 - accuracy: 0.8313 - val_loss: 0.6288 - val_accuracy: 0.8050\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 5s 93ms/step - loss: 0.5168 - accuracy: 0.8600 - val_loss: 0.6243 - val_accuracy: 0.8100\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.5036 - accuracy: 0.8487 - val_loss: 0.6274 - val_accuracy: 0.8050\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.4936 - accuracy: 0.8600 - val_loss: 0.6255 - val_accuracy: 0.8000\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.4238 - accuracy: 0.8763 - val_loss: 0.6346 - val_accuracy: 0.8100\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.4568 - accuracy: 0.8550 - val_loss: 0.6140 - val_accuracy: 0.8100\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.4174 - accuracy: 0.8900 - val_loss: 0.6046 - val_accuracy: 0.8050\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.4113 - accuracy: 0.8800 - val_loss: 0.6358 - val_accuracy: 0.8150\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.4120 - accuracy: 0.8700 - val_loss: 0.5980 - val_accuracy: 0.8150\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.3926 - accuracy: 0.8788 - val_loss: 0.6114 - val_accuracy: 0.8250\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.3706 - accuracy: 0.8888 - val_loss: 0.6080 - val_accuracy: 0.8150\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.3698 - accuracy: 0.9025 - val_loss: 0.5912 - val_accuracy: 0.8100\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.3541 - accuracy: 0.8913 - val_loss: 0.6030 - val_accuracy: 0.8250\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.3499 - accuracy: 0.9087 - val_loss: 0.6096 - val_accuracy: 0.8100\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.3369 - accuracy: 0.9013 - val_loss: 0.6091 - val_accuracy: 0.8250\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.3635 - accuracy: 0.8988 - val_loss: 0.5849 - val_accuracy: 0.8100\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.3056 - accuracy: 0.9125 - val_loss: 0.5825 - val_accuracy: 0.8250\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.3253 - accuracy: 0.9087 - val_loss: 0.5909 - val_accuracy: 0.8200\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.3168 - accuracy: 0.8975 - val_loss: 0.5778 - val_accuracy: 0.8400\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.3028 - accuracy: 0.9087 - val_loss: 0.5902 - val_accuracy: 0.8250\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2871 - accuracy: 0.9150 - val_loss: 0.5834 - val_accuracy: 0.8300\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.3016 - accuracy: 0.9075 - val_loss: 0.5924 - val_accuracy: 0.8050\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2675 - accuracy: 0.9287 - val_loss: 0.5845 - val_accuracy: 0.8150\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2709 - accuracy: 0.9200 - val_loss: 0.5783 - val_accuracy: 0.8250\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2842 - accuracy: 0.9075 - val_loss: 0.5734 - val_accuracy: 0.8250\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.2814 - accuracy: 0.9287 - val_loss: 0.5874 - val_accuracy: 0.8300\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2423 - accuracy: 0.9337 - val_loss: 0.5784 - val_accuracy: 0.8300\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.2406 - accuracy: 0.9337 - val_loss: 0.5966 - val_accuracy: 0.8250\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2315 - accuracy: 0.9400 - val_loss: 0.5708 - val_accuracy: 0.8350\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2431 - accuracy: 0.9225 - val_loss: 0.5827 - val_accuracy: 0.8150\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2338 - accuracy: 0.9337 - val_loss: 0.5763 - val_accuracy: 0.8250\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2049 - accuracy: 0.9350 - val_loss: 0.5832 - val_accuracy: 0.8350\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.2484 - accuracy: 0.9275 - val_loss: 0.5766 - val_accuracy: 0.8250\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.2158 - accuracy: 0.9425 - val_loss: 0.5904 - val_accuracy: 0.8400\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2343 - accuracy: 0.9300 - val_loss: 0.5824 - val_accuracy: 0.8250\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.2211 - accuracy: 0.9275 - val_loss: 0.5796 - val_accuracy: 0.8450\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2380 - accuracy: 0.9275 - val_loss: 0.5792 - val_accuracy: 0.8300\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1983 - accuracy: 0.9425 - val_loss: 0.5925 - val_accuracy: 0.8250\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2145 - accuracy: 0.9388 - val_loss: 0.5607 - val_accuracy: 0.8500\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2054 - accuracy: 0.9362 - val_loss: 0.5793 - val_accuracy: 0.8250\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.2075 - accuracy: 0.9413 - val_loss: 0.5941 - val_accuracy: 0.8350\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.2131 - accuracy: 0.9450 - val_loss: 0.5636 - val_accuracy: 0.8400\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1895 - accuracy: 0.9500 - val_loss: 0.5759 - val_accuracy: 0.8300\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.2183 - accuracy: 0.9300 - val_loss: 0.5745 - val_accuracy: 0.8400\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1792 - accuracy: 0.9500 - val_loss: 0.5847 - val_accuracy: 0.8300\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1679 - accuracy: 0.9500 - val_loss: 0.5854 - val_accuracy: 0.8400\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.2131 - accuracy: 0.9388 - val_loss: 0.5812 - val_accuracy: 0.8350\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1998 - accuracy: 0.9337 - val_loss: 0.6100 - val_accuracy: 0.8200\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1680 - accuracy: 0.9600 - val_loss: 0.5725 - val_accuracy: 0.8350\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1653 - accuracy: 0.9538 - val_loss: 0.5874 - val_accuracy: 0.8300\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1736 - accuracy: 0.9513 - val_loss: 0.6036 - val_accuracy: 0.8300\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1591 - accuracy: 0.9563 - val_loss: 0.6043 - val_accuracy: 0.8250\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1569 - accuracy: 0.9538 - val_loss: 0.5855 - val_accuracy: 0.8400\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1485 - accuracy: 0.9613 - val_loss: 0.5842 - val_accuracy: 0.8350\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1716 - accuracy: 0.9388 - val_loss: 0.5733 - val_accuracy: 0.8400\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1631 - accuracy: 0.9563 - val_loss: 0.5633 - val_accuracy: 0.8400\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1392 - accuracy: 0.9663 - val_loss: 0.5776 - val_accuracy: 0.8500\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1585 - accuracy: 0.9550 - val_loss: 0.5806 - val_accuracy: 0.8350\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1591 - accuracy: 0.9563 - val_loss: 0.5877 - val_accuracy: 0.8500\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1554 - accuracy: 0.9550 - val_loss: 0.6029 - val_accuracy: 0.8350\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1592 - accuracy: 0.9600 - val_loss: 0.5706 - val_accuracy: 0.8350\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1525 - accuracy: 0.9588 - val_loss: 0.5831 - val_accuracy: 0.8400\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1409 - accuracy: 0.9650 - val_loss: 0.5859 - val_accuracy: 0.8350\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1251 - accuracy: 0.9650 - val_loss: 0.5847 - val_accuracy: 0.8300\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1498 - accuracy: 0.9588 - val_loss: 0.5724 - val_accuracy: 0.8400\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1389 - accuracy: 0.9650 - val_loss: 0.5710 - val_accuracy: 0.8500\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 5s 93ms/step - loss: 0.1459 - accuracy: 0.9613 - val_loss: 0.5895 - val_accuracy: 0.8350\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1432 - accuracy: 0.9613 - val_loss: 0.5921 - val_accuracy: 0.8300\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1395 - accuracy: 0.9638 - val_loss: 0.6080 - val_accuracy: 0.8350\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1258 - accuracy: 0.9613 - val_loss: 0.5672 - val_accuracy: 0.8350\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1310 - accuracy: 0.9600 - val_loss: 0.5825 - val_accuracy: 0.8350\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1086 - accuracy: 0.9712 - val_loss: 0.5801 - val_accuracy: 0.8400\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1384 - accuracy: 0.9575 - val_loss: 0.5726 - val_accuracy: 0.8400\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1182 - accuracy: 0.9638 - val_loss: 0.6269 - val_accuracy: 0.8450\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1259 - accuracy: 0.9638 - val_loss: 0.5884 - val_accuracy: 0.8300\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1026 - accuracy: 0.9737 - val_loss: 0.5829 - val_accuracy: 0.8450\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1235 - accuracy: 0.9675 - val_loss: 0.5839 - val_accuracy: 0.8350\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1318 - accuracy: 0.9600 - val_loss: 0.6019 - val_accuracy: 0.8400\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1182 - accuracy: 0.9663 - val_loss: 0.5935 - val_accuracy: 0.8300\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1143 - accuracy: 0.9675 - val_loss: 0.5828 - val_accuracy: 0.8300\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1167 - accuracy: 0.9613 - val_loss: 0.6012 - val_accuracy: 0.8400\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1364 - accuracy: 0.9663 - val_loss: 0.5861 - val_accuracy: 0.8250\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.1191 - accuracy: 0.9650 - val_loss: 0.5790 - val_accuracy: 0.8450\n",
            "fold 3\n",
            "Model: \"functional_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_2 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_2 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_2 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "inception_v3 (Functional)    (None, 8, 8, 2048)        21802784  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               262272    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 22,066,346\n",
            "Trainable params: 263,562\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 1s 66ms/step - loss: 2.4192 - accuracy: 0.0750\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 2.2517 - accuracy: 0.2013 - val_loss: 1.9825 - val_accuracy: 0.3950\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 1.8546 - accuracy: 0.3862 - val_loss: 1.7006 - val_accuracy: 0.5700\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 1.5806 - accuracy: 0.5025 - val_loss: 1.4563 - val_accuracy: 0.6550\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 1.3688 - accuracy: 0.5713 - val_loss: 1.2736 - val_accuracy: 0.6750\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 1.1792 - accuracy: 0.6562 - val_loss: 1.1657 - val_accuracy: 0.7150\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 1.0806 - accuracy: 0.6687 - val_loss: 1.0990 - val_accuracy: 0.6950\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.9465 - accuracy: 0.7212 - val_loss: 1.0155 - val_accuracy: 0.7150\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.9002 - accuracy: 0.7362 - val_loss: 0.9378 - val_accuracy: 0.7100\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.8074 - accuracy: 0.7688 - val_loss: 0.8971 - val_accuracy: 0.7600\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.7486 - accuracy: 0.7750 - val_loss: 0.8578 - val_accuracy: 0.7600\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.7277 - accuracy: 0.8012 - val_loss: 0.8148 - val_accuracy: 0.7750\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.7140 - accuracy: 0.7887 - val_loss: 0.8106 - val_accuracy: 0.7350\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.6417 - accuracy: 0.8112 - val_loss: 0.7813 - val_accuracy: 0.7750\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.6008 - accuracy: 0.8325 - val_loss: 0.7682 - val_accuracy: 0.7750\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.5672 - accuracy: 0.8450 - val_loss: 0.7607 - val_accuracy: 0.7750\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.5439 - accuracy: 0.8400 - val_loss: 0.7403 - val_accuracy: 0.7800\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.5465 - accuracy: 0.8388 - val_loss: 0.7278 - val_accuracy: 0.7850\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.5337 - accuracy: 0.8150 - val_loss: 0.7292 - val_accuracy: 0.7900\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.4738 - accuracy: 0.8587 - val_loss: 0.7108 - val_accuracy: 0.7950\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 5s 94ms/step - loss: 0.4826 - accuracy: 0.8637 - val_loss: 0.7104 - val_accuracy: 0.7700\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.4434 - accuracy: 0.8712 - val_loss: 0.7190 - val_accuracy: 0.7700\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.4867 - accuracy: 0.8525 - val_loss: 0.7133 - val_accuracy: 0.7550\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.4393 - accuracy: 0.8587 - val_loss: 0.7068 - val_accuracy: 0.7700\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.4139 - accuracy: 0.8788 - val_loss: 0.6904 - val_accuracy: 0.7750\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.4192 - accuracy: 0.8637 - val_loss: 0.6937 - val_accuracy: 0.7750\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.4230 - accuracy: 0.8662 - val_loss: 0.6891 - val_accuracy: 0.7700\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.3695 - accuracy: 0.9013 - val_loss: 0.6783 - val_accuracy: 0.7800\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.3837 - accuracy: 0.8825 - val_loss: 0.6910 - val_accuracy: 0.7850\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.3862 - accuracy: 0.8763 - val_loss: 0.6857 - val_accuracy: 0.7650\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.3260 - accuracy: 0.9112 - val_loss: 0.6749 - val_accuracy: 0.7650\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.3487 - accuracy: 0.9000 - val_loss: 0.6617 - val_accuracy: 0.7750\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.3134 - accuracy: 0.9100 - val_loss: 0.6683 - val_accuracy: 0.7700\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.3353 - accuracy: 0.9137 - val_loss: 0.6795 - val_accuracy: 0.7650\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.3348 - accuracy: 0.9025 - val_loss: 0.6535 - val_accuracy: 0.7650\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.3216 - accuracy: 0.9038 - val_loss: 0.6682 - val_accuracy: 0.7800\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.3270 - accuracy: 0.9137 - val_loss: 0.6709 - val_accuracy: 0.7950\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2840 - accuracy: 0.9187 - val_loss: 0.6548 - val_accuracy: 0.7900\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2826 - accuracy: 0.9175 - val_loss: 0.6717 - val_accuracy: 0.7700\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2622 - accuracy: 0.9287 - val_loss: 0.6577 - val_accuracy: 0.7750\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2986 - accuracy: 0.9150 - val_loss: 0.6891 - val_accuracy: 0.7600\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2763 - accuracy: 0.9162 - val_loss: 0.6436 - val_accuracy: 0.7750\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2566 - accuracy: 0.9350 - val_loss: 0.6438 - val_accuracy: 0.7700\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2590 - accuracy: 0.9237 - val_loss: 0.6553 - val_accuracy: 0.7500\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2662 - accuracy: 0.9150 - val_loss: 0.6717 - val_accuracy: 0.7500\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2299 - accuracy: 0.9375 - val_loss: 0.6652 - val_accuracy: 0.7600\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2371 - accuracy: 0.9375 - val_loss: 0.6337 - val_accuracy: 0.7700\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2192 - accuracy: 0.9350 - val_loss: 0.6247 - val_accuracy: 0.8000\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2233 - accuracy: 0.9325 - val_loss: 0.6580 - val_accuracy: 0.7700\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2362 - accuracy: 0.9250 - val_loss: 0.6312 - val_accuracy: 0.7750\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2444 - accuracy: 0.9250 - val_loss: 0.6453 - val_accuracy: 0.7600\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1950 - accuracy: 0.9550 - val_loss: 0.6484 - val_accuracy: 0.7750\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1890 - accuracy: 0.9438 - val_loss: 0.6765 - val_accuracy: 0.7650\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2196 - accuracy: 0.9312 - val_loss: 0.6339 - val_accuracy: 0.7550\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1998 - accuracy: 0.9425 - val_loss: 0.6565 - val_accuracy: 0.7700\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1812 - accuracy: 0.9550 - val_loss: 0.6423 - val_accuracy: 0.7850\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2282 - accuracy: 0.9312 - val_loss: 0.6546 - val_accuracy: 0.7700\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1859 - accuracy: 0.9588 - val_loss: 0.6397 - val_accuracy: 0.7600\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1902 - accuracy: 0.9488 - val_loss: 0.6580 - val_accuracy: 0.7550\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1819 - accuracy: 0.9525 - val_loss: 0.6341 - val_accuracy: 0.7550\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1736 - accuracy: 0.9550 - val_loss: 0.6635 - val_accuracy: 0.7700\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2000 - accuracy: 0.9413 - val_loss: 0.6535 - val_accuracy: 0.7500\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1974 - accuracy: 0.9463 - val_loss: 0.6368 - val_accuracy: 0.7450\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1745 - accuracy: 0.9475 - val_loss: 0.6371 - val_accuracy: 0.7700\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1970 - accuracy: 0.9438 - val_loss: 0.6585 - val_accuracy: 0.7500\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1608 - accuracy: 0.9625 - val_loss: 0.6328 - val_accuracy: 0.7700\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1671 - accuracy: 0.9550 - val_loss: 0.6404 - val_accuracy: 0.7900\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1753 - accuracy: 0.9475 - val_loss: 0.6367 - val_accuracy: 0.7550\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1781 - accuracy: 0.9450 - val_loss: 0.6326 - val_accuracy: 0.7900\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1580 - accuracy: 0.9613 - val_loss: 0.6540 - val_accuracy: 0.7550\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1616 - accuracy: 0.9563 - val_loss: 0.6348 - val_accuracy: 0.7700\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1738 - accuracy: 0.9550 - val_loss: 0.6420 - val_accuracy: 0.7750\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1718 - accuracy: 0.9613 - val_loss: 0.6311 - val_accuracy: 0.7600\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1439 - accuracy: 0.9625 - val_loss: 0.6330 - val_accuracy: 0.7700\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1320 - accuracy: 0.9638 - val_loss: 0.6351 - val_accuracy: 0.7800\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1540 - accuracy: 0.9538 - val_loss: 0.6654 - val_accuracy: 0.7550\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1656 - accuracy: 0.9538 - val_loss: 0.6416 - val_accuracy: 0.7600\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1392 - accuracy: 0.9575 - val_loss: 0.6345 - val_accuracy: 0.7800\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1228 - accuracy: 0.9675 - val_loss: 0.6497 - val_accuracy: 0.7600\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1619 - accuracy: 0.9538 - val_loss: 0.6443 - val_accuracy: 0.7700\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1335 - accuracy: 0.9663 - val_loss: 0.6219 - val_accuracy: 0.7900\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1268 - accuracy: 0.9650 - val_loss: 0.6491 - val_accuracy: 0.7900\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1461 - accuracy: 0.9613 - val_loss: 0.6794 - val_accuracy: 0.7550\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1504 - accuracy: 0.9538 - val_loss: 0.6383 - val_accuracy: 0.7850\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1289 - accuracy: 0.9650 - val_loss: 0.6569 - val_accuracy: 0.7600\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1359 - accuracy: 0.9750 - val_loss: 0.6617 - val_accuracy: 0.7700\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1358 - accuracy: 0.9588 - val_loss: 0.6390 - val_accuracy: 0.7700\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1027 - accuracy: 0.9800 - val_loss: 0.6485 - val_accuracy: 0.7750\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1499 - accuracy: 0.9588 - val_loss: 0.6517 - val_accuracy: 0.7850\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1292 - accuracy: 0.9600 - val_loss: 0.6567 - val_accuracy: 0.7800\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1110 - accuracy: 0.9762 - val_loss: 0.6644 - val_accuracy: 0.7600\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1235 - accuracy: 0.9688 - val_loss: 0.6617 - val_accuracy: 0.7650\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1311 - accuracy: 0.9650 - val_loss: 0.6932 - val_accuracy: 0.7650\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1093 - accuracy: 0.9663 - val_loss: 0.6590 - val_accuracy: 0.7700\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1267 - accuracy: 0.9650 - val_loss: 0.6577 - val_accuracy: 0.7850\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1157 - accuracy: 0.9750 - val_loss: 0.6410 - val_accuracy: 0.7850\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.1230 - accuracy: 0.9712 - val_loss: 0.6549 - val_accuracy: 0.7700\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.1115 - accuracy: 0.9650 - val_loss: 0.6761 - val_accuracy: 0.7900\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1093 - accuracy: 0.9775 - val_loss: 0.6753 - val_accuracy: 0.7650\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1043 - accuracy: 0.9700 - val_loss: 0.6566 - val_accuracy: 0.7850\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1021 - accuracy: 0.9737 - val_loss: 0.6951 - val_accuracy: 0.7500\n",
            "fold 4\n",
            "Model: \"functional_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_3 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_3 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_3 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "inception_v3 (Functional)    (None, 8, 8, 2048)        21802784  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_3 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               262272    \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 22,066,346\n",
            "Trainable params: 263,562\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 1s 67ms/step - loss: 2.5167 - accuracy: 0.1000\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 2.3675 - accuracy: 0.1338 - val_loss: 2.1005 - val_accuracy: 0.3150\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 2.0357 - accuracy: 0.3088 - val_loss: 1.8557 - val_accuracy: 0.5100\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 1.7850 - accuracy: 0.4238 - val_loss: 1.5912 - val_accuracy: 0.6100\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 1.5314 - accuracy: 0.5500 - val_loss: 1.3784 - val_accuracy: 0.6600\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 1.3553 - accuracy: 0.5950 - val_loss: 1.1920 - val_accuracy: 0.7150\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 1.1518 - accuracy: 0.6600 - val_loss: 1.0846 - val_accuracy: 0.7300\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 1.0445 - accuracy: 0.6862 - val_loss: 1.0047 - val_accuracy: 0.7100\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.9445 - accuracy: 0.7350 - val_loss: 0.9265 - val_accuracy: 0.7350\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.9173 - accuracy: 0.7175 - val_loss: 0.8652 - val_accuracy: 0.7550\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.8340 - accuracy: 0.7538 - val_loss: 0.8329 - val_accuracy: 0.7450\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.7466 - accuracy: 0.7862 - val_loss: 0.8120 - val_accuracy: 0.7500\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.7092 - accuracy: 0.7912 - val_loss: 0.7802 - val_accuracy: 0.7550\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.6755 - accuracy: 0.7837 - val_loss: 0.7532 - val_accuracy: 0.7750\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.6305 - accuracy: 0.8100 - val_loss: 0.7731 - val_accuracy: 0.7450\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.6239 - accuracy: 0.8200 - val_loss: 0.7261 - val_accuracy: 0.7700\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.5812 - accuracy: 0.8263 - val_loss: 0.7269 - val_accuracy: 0.7750\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.5732 - accuracy: 0.8338 - val_loss: 0.7004 - val_accuracy: 0.7850\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.5432 - accuracy: 0.8288 - val_loss: 0.7071 - val_accuracy: 0.7750\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.4994 - accuracy: 0.8537 - val_loss: 0.6781 - val_accuracy: 0.8000\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.4987 - accuracy: 0.8625 - val_loss: 0.6740 - val_accuracy: 0.7800\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.5072 - accuracy: 0.8375 - val_loss: 0.6770 - val_accuracy: 0.7800\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.4646 - accuracy: 0.8500 - val_loss: 0.6630 - val_accuracy: 0.8050\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.4416 - accuracy: 0.8562 - val_loss: 0.6669 - val_accuracy: 0.7850\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.4151 - accuracy: 0.8825 - val_loss: 0.6412 - val_accuracy: 0.8000\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.4026 - accuracy: 0.8838 - val_loss: 0.6423 - val_accuracy: 0.7850\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.3926 - accuracy: 0.8850 - val_loss: 0.6510 - val_accuracy: 0.7950\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.3927 - accuracy: 0.8813 - val_loss: 0.6315 - val_accuracy: 0.8150\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.3738 - accuracy: 0.8913 - val_loss: 0.6374 - val_accuracy: 0.7850\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.3644 - accuracy: 0.8963 - val_loss: 0.6484 - val_accuracy: 0.7700\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.3626 - accuracy: 0.8938 - val_loss: 0.6209 - val_accuracy: 0.8050\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.3730 - accuracy: 0.8838 - val_loss: 0.6296 - val_accuracy: 0.7900\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.3277 - accuracy: 0.9050 - val_loss: 0.6235 - val_accuracy: 0.7950\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.3267 - accuracy: 0.9100 - val_loss: 0.6230 - val_accuracy: 0.8100\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.3254 - accuracy: 0.8900 - val_loss: 0.6250 - val_accuracy: 0.8100\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.3334 - accuracy: 0.8963 - val_loss: 0.6368 - val_accuracy: 0.8050\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.3135 - accuracy: 0.9175 - val_loss: 0.6338 - val_accuracy: 0.8000\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.3202 - accuracy: 0.9050 - val_loss: 0.6330 - val_accuracy: 0.8100\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.3083 - accuracy: 0.9062 - val_loss: 0.6250 - val_accuracy: 0.7950\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.3117 - accuracy: 0.9137 - val_loss: 0.6392 - val_accuracy: 0.7750\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2906 - accuracy: 0.9200 - val_loss: 0.6189 - val_accuracy: 0.8050\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2835 - accuracy: 0.9200 - val_loss: 0.6362 - val_accuracy: 0.8050\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2626 - accuracy: 0.9337 - val_loss: 0.6264 - val_accuracy: 0.8050\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.2786 - accuracy: 0.9300 - val_loss: 0.6344 - val_accuracy: 0.8150\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2567 - accuracy: 0.9225 - val_loss: 0.6280 - val_accuracy: 0.8100\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2595 - accuracy: 0.9262 - val_loss: 0.6490 - val_accuracy: 0.7850\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2583 - accuracy: 0.9137 - val_loss: 0.6347 - val_accuracy: 0.8000\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.2708 - accuracy: 0.9212 - val_loss: 0.6361 - val_accuracy: 0.8100\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.2411 - accuracy: 0.9275 - val_loss: 0.6297 - val_accuracy: 0.8000\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2179 - accuracy: 0.9463 - val_loss: 0.6332 - val_accuracy: 0.8050\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.2063 - accuracy: 0.9413 - val_loss: 0.6284 - val_accuracy: 0.7900\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2325 - accuracy: 0.9262 - val_loss: 0.6349 - val_accuracy: 0.8100\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2371 - accuracy: 0.9287 - val_loss: 0.6405 - val_accuracy: 0.8050\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.2066 - accuracy: 0.9425 - val_loss: 0.6532 - val_accuracy: 0.8200\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.1909 - accuracy: 0.9525 - val_loss: 0.6656 - val_accuracy: 0.8000\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.2262 - accuracy: 0.9337 - val_loss: 0.6478 - val_accuracy: 0.8100\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 5s 98ms/step - loss: 0.1920 - accuracy: 0.9513 - val_loss: 0.6365 - val_accuracy: 0.8050\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 5s 98ms/step - loss: 0.1957 - accuracy: 0.9413 - val_loss: 0.6388 - val_accuracy: 0.8150\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2253 - accuracy: 0.9425 - val_loss: 0.6353 - val_accuracy: 0.8000\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1890 - accuracy: 0.9525 - val_loss: 0.6579 - val_accuracy: 0.7900\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2000 - accuracy: 0.9425 - val_loss: 0.6417 - val_accuracy: 0.8050\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1986 - accuracy: 0.9400 - val_loss: 0.6395 - val_accuracy: 0.7950\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.1862 - accuracy: 0.9500 - val_loss: 0.6317 - val_accuracy: 0.8100\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.1895 - accuracy: 0.9388 - val_loss: 0.6458 - val_accuracy: 0.8050\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.1793 - accuracy: 0.9488 - val_loss: 0.6288 - val_accuracy: 0.8050\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.1890 - accuracy: 0.9400 - val_loss: 0.6609 - val_accuracy: 0.8000\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1964 - accuracy: 0.9425 - val_loss: 0.6323 - val_accuracy: 0.8000\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.1837 - accuracy: 0.9500 - val_loss: 0.6397 - val_accuracy: 0.7950\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.1740 - accuracy: 0.9488 - val_loss: 0.6540 - val_accuracy: 0.7950\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.1594 - accuracy: 0.9525 - val_loss: 0.6321 - val_accuracy: 0.8050\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.1575 - accuracy: 0.9550 - val_loss: 0.6750 - val_accuracy: 0.8000\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.1631 - accuracy: 0.9563 - val_loss: 0.6596 - val_accuracy: 0.7850\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1527 - accuracy: 0.9600 - val_loss: 0.6867 - val_accuracy: 0.7650\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1711 - accuracy: 0.9588 - val_loss: 0.6634 - val_accuracy: 0.7900\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.1575 - accuracy: 0.9613 - val_loss: 0.6598 - val_accuracy: 0.7950\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1568 - accuracy: 0.9638 - val_loss: 0.6715 - val_accuracy: 0.7700\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.1472 - accuracy: 0.9613 - val_loss: 0.6754 - val_accuracy: 0.8000\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1368 - accuracy: 0.9700 - val_loss: 0.6754 - val_accuracy: 0.8000\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.1537 - accuracy: 0.9550 - val_loss: 0.6530 - val_accuracy: 0.8050\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.1548 - accuracy: 0.9463 - val_loss: 0.6717 - val_accuracy: 0.8000\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1458 - accuracy: 0.9588 - val_loss: 0.6740 - val_accuracy: 0.7800\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.1562 - accuracy: 0.9525 - val_loss: 0.6643 - val_accuracy: 0.7850\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.1281 - accuracy: 0.9638 - val_loss: 0.6633 - val_accuracy: 0.8000\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 5s 98ms/step - loss: 0.1206 - accuracy: 0.9725 - val_loss: 0.6579 - val_accuracy: 0.7950\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.1549 - accuracy: 0.9575 - val_loss: 0.6873 - val_accuracy: 0.7850\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1453 - accuracy: 0.9663 - val_loss: 0.6873 - val_accuracy: 0.7800\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1265 - accuracy: 0.9700 - val_loss: 0.6698 - val_accuracy: 0.8050\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1348 - accuracy: 0.9613 - val_loss: 0.6885 - val_accuracy: 0.7850\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1325 - accuracy: 0.9663 - val_loss: 0.7003 - val_accuracy: 0.7850\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.1263 - accuracy: 0.9737 - val_loss: 0.6651 - val_accuracy: 0.8100\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1201 - accuracy: 0.9650 - val_loss: 0.6915 - val_accuracy: 0.7900\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1136 - accuracy: 0.9750 - val_loss: 0.6903 - val_accuracy: 0.8000\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1204 - accuracy: 0.9650 - val_loss: 0.6840 - val_accuracy: 0.7900\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1182 - accuracy: 0.9675 - val_loss: 0.6823 - val_accuracy: 0.8000\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1128 - accuracy: 0.9688 - val_loss: 0.6892 - val_accuracy: 0.7950\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1062 - accuracy: 0.9762 - val_loss: 0.6906 - val_accuracy: 0.7950\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1133 - accuracy: 0.9650 - val_loss: 0.6893 - val_accuracy: 0.7950\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1265 - accuracy: 0.9550 - val_loss: 0.7111 - val_accuracy: 0.7900\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1291 - accuracy: 0.9638 - val_loss: 0.6752 - val_accuracy: 0.7900\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.0940 - accuracy: 0.9775 - val_loss: 0.6877 - val_accuracy: 0.8000\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1139 - accuracy: 0.9675 - val_loss: 0.7179 - val_accuracy: 0.8000\n",
            "fold 5\n",
            "Model: \"functional_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_4 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_4 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_4 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "inception_v3 (Functional)    (None, 8, 8, 2048)        21802784  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_4 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               262272    \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 22,066,346\n",
            "Trainable params: 263,562\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 1s 66ms/step - loss: 2.4319 - accuracy: 0.1100\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 2.2826 - accuracy: 0.1525 - val_loss: 2.0425 - val_accuracy: 0.3650\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 1.9442 - accuracy: 0.3663 - val_loss: 1.7578 - val_accuracy: 0.5150\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 1.6747 - accuracy: 0.4725 - val_loss: 1.4890 - val_accuracy: 0.6450\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 1.4052 - accuracy: 0.5838 - val_loss: 1.2512 - val_accuracy: 0.7100\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 1.2292 - accuracy: 0.6275 - val_loss: 1.0856 - val_accuracy: 0.7450\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 5s 98ms/step - loss: 1.0785 - accuracy: 0.6762 - val_loss: 0.9736 - val_accuracy: 0.7450\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.9698 - accuracy: 0.7212 - val_loss: 0.8779 - val_accuracy: 0.7900\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.8865 - accuracy: 0.7487 - val_loss: 0.8046 - val_accuracy: 0.7950\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.8190 - accuracy: 0.7663 - val_loss: 0.7655 - val_accuracy: 0.8100\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.7734 - accuracy: 0.7763 - val_loss: 0.7438 - val_accuracy: 0.7900\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.7587 - accuracy: 0.7525 - val_loss: 0.6936 - val_accuracy: 0.8150\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.6849 - accuracy: 0.7925 - val_loss: 0.6839 - val_accuracy: 0.7950\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.6364 - accuracy: 0.8175 - val_loss: 0.6382 - val_accuracy: 0.8150\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.5982 - accuracy: 0.8313 - val_loss: 0.6221 - val_accuracy: 0.8100\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.5788 - accuracy: 0.8225 - val_loss: 0.6018 - val_accuracy: 0.8100\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.5683 - accuracy: 0.8338 - val_loss: 0.5948 - val_accuracy: 0.8100\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.5374 - accuracy: 0.8200 - val_loss: 0.5717 - val_accuracy: 0.8300\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.5464 - accuracy: 0.8288 - val_loss: 0.5781 - val_accuracy: 0.8150\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.4765 - accuracy: 0.8625 - val_loss: 0.5510 - val_accuracy: 0.8350\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.4927 - accuracy: 0.8363 - val_loss: 0.5568 - val_accuracy: 0.8200\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.4381 - accuracy: 0.8775 - val_loss: 0.5355 - val_accuracy: 0.8250\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.4344 - accuracy: 0.8788 - val_loss: 0.5476 - val_accuracy: 0.8400\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.4508 - accuracy: 0.8575 - val_loss: 0.5319 - val_accuracy: 0.8200\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.4177 - accuracy: 0.8763 - val_loss: 0.5250 - val_accuracy: 0.8450\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.4099 - accuracy: 0.8675 - val_loss: 0.5437 - val_accuracy: 0.8300\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.3900 - accuracy: 0.8863 - val_loss: 0.5150 - val_accuracy: 0.8350\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.4031 - accuracy: 0.8750 - val_loss: 0.5342 - val_accuracy: 0.8350\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.4004 - accuracy: 0.8800 - val_loss: 0.5164 - val_accuracy: 0.8450\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.3797 - accuracy: 0.8813 - val_loss: 0.5221 - val_accuracy: 0.8150\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.3903 - accuracy: 0.8788 - val_loss: 0.5234 - val_accuracy: 0.8400\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.3366 - accuracy: 0.8963 - val_loss: 0.5096 - val_accuracy: 0.8250\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.3297 - accuracy: 0.9050 - val_loss: 0.4969 - val_accuracy: 0.8550\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.3589 - accuracy: 0.8913 - val_loss: 0.5016 - val_accuracy: 0.8150\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.3176 - accuracy: 0.9100 - val_loss: 0.4965 - val_accuracy: 0.8300\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.3124 - accuracy: 0.9175 - val_loss: 0.4816 - val_accuracy: 0.8550\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2961 - accuracy: 0.9100 - val_loss: 0.4872 - val_accuracy: 0.8550\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2963 - accuracy: 0.9112 - val_loss: 0.4803 - val_accuracy: 0.8400\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.3163 - accuracy: 0.8900 - val_loss: 0.4974 - val_accuracy: 0.8250\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2872 - accuracy: 0.9087 - val_loss: 0.4829 - val_accuracy: 0.8500\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2674 - accuracy: 0.9175 - val_loss: 0.4833 - val_accuracy: 0.8300\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2931 - accuracy: 0.9075 - val_loss: 0.4751 - val_accuracy: 0.8400\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2610 - accuracy: 0.9388 - val_loss: 0.4736 - val_accuracy: 0.8400\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2430 - accuracy: 0.9400 - val_loss: 0.4816 - val_accuracy: 0.8250\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2696 - accuracy: 0.9250 - val_loss: 0.4740 - val_accuracy: 0.8350\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2535 - accuracy: 0.9187 - val_loss: 0.4682 - val_accuracy: 0.8400\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2502 - accuracy: 0.9212 - val_loss: 0.4627 - val_accuracy: 0.8450\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2202 - accuracy: 0.9400 - val_loss: 0.4583 - val_accuracy: 0.8500\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2346 - accuracy: 0.9362 - val_loss: 0.4643 - val_accuracy: 0.8500\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2222 - accuracy: 0.9337 - val_loss: 0.4732 - val_accuracy: 0.8500\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2321 - accuracy: 0.9350 - val_loss: 0.4672 - val_accuracy: 0.8500\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2197 - accuracy: 0.9438 - val_loss: 0.4768 - val_accuracy: 0.8350\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1953 - accuracy: 0.9475 - val_loss: 0.4766 - val_accuracy: 0.8350\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.2279 - accuracy: 0.9337 - val_loss: 0.4605 - val_accuracy: 0.8450\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1981 - accuracy: 0.9488 - val_loss: 0.4587 - val_accuracy: 0.8400\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2013 - accuracy: 0.9475 - val_loss: 0.4672 - val_accuracy: 0.8400\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2216 - accuracy: 0.9312 - val_loss: 0.4721 - val_accuracy: 0.8450\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2045 - accuracy: 0.9450 - val_loss: 0.4588 - val_accuracy: 0.8500\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1912 - accuracy: 0.9563 - val_loss: 0.4594 - val_accuracy: 0.8400\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1774 - accuracy: 0.9563 - val_loss: 0.4613 - val_accuracy: 0.8450\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1890 - accuracy: 0.9450 - val_loss: 0.4700 - val_accuracy: 0.8400\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1894 - accuracy: 0.9438 - val_loss: 0.4585 - val_accuracy: 0.8450\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1863 - accuracy: 0.9438 - val_loss: 0.4682 - val_accuracy: 0.8500\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1837 - accuracy: 0.9500 - val_loss: 0.4673 - val_accuracy: 0.8450\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1957 - accuracy: 0.9413 - val_loss: 0.4761 - val_accuracy: 0.8500\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2051 - accuracy: 0.9400 - val_loss: 0.4797 - val_accuracy: 0.8400\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1757 - accuracy: 0.9513 - val_loss: 0.4807 - val_accuracy: 0.8500\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1750 - accuracy: 0.9475 - val_loss: 0.4937 - val_accuracy: 0.8350\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1707 - accuracy: 0.9538 - val_loss: 0.4733 - val_accuracy: 0.8350\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1670 - accuracy: 0.9550 - val_loss: 0.4826 - val_accuracy: 0.8500\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.1880 - accuracy: 0.9450 - val_loss: 0.4903 - val_accuracy: 0.8500\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1621 - accuracy: 0.9500 - val_loss: 0.4637 - val_accuracy: 0.8550\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1712 - accuracy: 0.9513 - val_loss: 0.4810 - val_accuracy: 0.8400\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1720 - accuracy: 0.9575 - val_loss: 0.4838 - val_accuracy: 0.8300\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1564 - accuracy: 0.9613 - val_loss: 0.4885 - val_accuracy: 0.8500\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1535 - accuracy: 0.9513 - val_loss: 0.4679 - val_accuracy: 0.8450\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1464 - accuracy: 0.9650 - val_loss: 0.4893 - val_accuracy: 0.8350\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.1496 - accuracy: 0.9675 - val_loss: 0.4903 - val_accuracy: 0.8350\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1337 - accuracy: 0.9650 - val_loss: 0.4784 - val_accuracy: 0.8450\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.1653 - accuracy: 0.9525 - val_loss: 0.4642 - val_accuracy: 0.8500\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.1484 - accuracy: 0.9538 - val_loss: 0.4852 - val_accuracy: 0.8400\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1489 - accuracy: 0.9600 - val_loss: 0.4696 - val_accuracy: 0.8500\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1299 - accuracy: 0.9688 - val_loss: 0.4749 - val_accuracy: 0.8450\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1428 - accuracy: 0.9663 - val_loss: 0.4757 - val_accuracy: 0.8300\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1370 - accuracy: 0.9688 - val_loss: 0.4803 - val_accuracy: 0.8400\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1357 - accuracy: 0.9663 - val_loss: 0.4861 - val_accuracy: 0.8600\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1260 - accuracy: 0.9625 - val_loss: 0.4689 - val_accuracy: 0.8700\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1268 - accuracy: 0.9663 - val_loss: 0.4684 - val_accuracy: 0.8550\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1165 - accuracy: 0.9675 - val_loss: 0.4651 - val_accuracy: 0.8650\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.1267 - accuracy: 0.9712 - val_loss: 0.4875 - val_accuracy: 0.8200\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1349 - accuracy: 0.9600 - val_loss: 0.4602 - val_accuracy: 0.8650\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1582 - accuracy: 0.9513 - val_loss: 0.4602 - val_accuracy: 0.8450\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1126 - accuracy: 0.9750 - val_loss: 0.4586 - val_accuracy: 0.8600\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1281 - accuracy: 0.9650 - val_loss: 0.4578 - val_accuracy: 0.8650\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1298 - accuracy: 0.9563 - val_loss: 0.4696 - val_accuracy: 0.8450\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1255 - accuracy: 0.9650 - val_loss: 0.4806 - val_accuracy: 0.8400\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1199 - accuracy: 0.9700 - val_loss: 0.4695 - val_accuracy: 0.8650\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.1207 - accuracy: 0.9638 - val_loss: 0.4650 - val_accuracy: 0.8500\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.0977 - accuracy: 0.9750 - val_loss: 0.4886 - val_accuracy: 0.8550\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.0883 - accuracy: 0.9787 - val_loss: 0.4923 - val_accuracy: 0.8350\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 5s 95ms/step - loss: 0.1103 - accuracy: 0.9675 - val_loss: 0.4641 - val_accuracy: 0.8400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2E72C3O30fv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a88ee26-312f-4532-83cb-146cdc9144cd"
      },
      "source": [
        "# cross-validated accuracy for pre-trained model before training\n",
        "print(\"Base model accuracy:\", np.mean(base_model_acc_list))\n",
        "# cross-validated accuracy after training\n",
        "print(\"Final accuracy:\", np.mean(final_acc_list))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Base model accuracy: 0.08999999985098839\n",
            "Final accuracy: 0.8380000114440918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dtn-03T_ZaRX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "7ed38f04-4f51-4cfa-d19b-7e3c9549cbfd"
      },
      "source": [
        "# plot graph of cross-validated accuracies\n",
        "acc = np.mean(history_map['accuracy'], axis=0)\n",
        "val_acc = np.mean(history_map['val_accuracy'], axis=0)\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftCYvBkKZmGa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "470a6aa0-0f0b-4b3c-b200-2dd6cdf733e7"
      },
      "source": [
        "# plot graph of cross-validated losses\n",
        "loss = np.mean(history_map['loss'], axis=0)\n",
        "val_loss = np.mean(history_map['val_loss'], axis=0)\n",
        "\n",
        "plt.figure(2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "#plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}