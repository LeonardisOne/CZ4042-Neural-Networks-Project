{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "materials-balanced-cross-validated.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP9sY/d3XVxvoK6sOgsWOgw"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERKJLyg2z-Uq"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fIbsvD01EQm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81da8e4f-1740-49e9-927c-7e5b1ad5a1f3"
      },
      "source": [
        "import pathlib\n",
        "\n",
        "# set path to FMD image directory\n",
        "data_dir = pathlib.Path(\"image\")\n",
        "\n",
        "# total no. of images in FMD dataset\n",
        "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
        "print(image_count)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL3rBqoe3sK5"
      },
      "source": [
        "# set batch size for training\n",
        "batch_size = 16\n",
        "\n",
        "# set dimensions to change images into for training\n",
        "# 299x299 images is required for pre-trained models like Inception V3\n",
        "img_height = 299\n",
        "img_width = 299"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFKuhNRxqxcB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fab031d2-a63d-4a1c-a733-4104f1800881"
      },
      "source": [
        "# get class names from the folder names in data_dir\n",
        "class_names = np.array(sorted([item.name for item in data_dir.glob('*') if item.name != \"LICENSE.txt\"]))\n",
        "print(class_names)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['fabric' 'foliage' 'glass' 'leather' 'metal' 'paper' 'plastic' 'stone'\n",
            " 'water' 'wood']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPI1onTCoUwg"
      },
      "source": [
        "# create list containing the dataset for each class\n",
        "ds_each_class = [tf.data.Dataset.list_files(str(data_dir/f'{class_name}/*.jpg'), shuffle=False) for class_name in class_names]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1j6x5ateo2ml"
      },
      "source": [
        "# shuffle the 100 images in each class with the random seed value of 123 before training\n",
        "for index, ds in enumerate(ds_each_class):\n",
        "  ds_each_class[index] = ds.shuffle(image_count//10, seed=123, reshuffle_each_iteration=False)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty_LijJpqbEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "089f2c23-c493-4aa6-aaa4-1a2a5113a086"
      },
      "source": [
        "# display some samples from a class to verify each class dataset contains only the class images\n",
        "for f in ds_each_class[0].take(10):\n",
        "  print(f.numpy())\n",
        "\n",
        "for f in ds_each_class[1].take(10):\n",
        "  print(f.numpy())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'FMD/image/fabric/fabric_moderate_037_new.jpg'\n",
            "b'FMD/image/fabric/fabric_moderate_004_new.jpg'\n",
            "b'FMD/image/fabric/fabric_moderate_008_new.jpg'\n",
            "b'FMD/image/fabric/fabric_object_003_new.jpg'\n",
            "b'FMD/image/fabric/fabric_object_017_new.jpg'\n",
            "b'FMD/image/fabric/fabric_object_001_new.jpg'\n",
            "b'FMD/image/fabric/fabric_moderate_032_new.jpg'\n",
            "b'FMD/image/fabric/fabric_moderate_030_new.jpg'\n",
            "b'FMD/image/fabric/fabric_moderate_038_new.jpg'\n",
            "b'FMD/image/fabric/fabric_object_009_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_037_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_004_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_008_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_053_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_067_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_051_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_032_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_030_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_038_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_059_new.jpg'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qENYSuO3rwhA"
      },
      "source": [
        "# split first class dataset into 5 equal sized partitions\n",
        "# then for remaining classes' datasets do the same and add to corresponding partition\n",
        "# for 5-fold cross validation\n",
        "A = ds_each_class[0].shard(num_shards=5, index=0)\n",
        "B = ds_each_class[0].shard(num_shards=5, index=1)\n",
        "C = ds_each_class[0].shard(num_shards=5, index=2)\n",
        "D = ds_each_class[0].shard(num_shards=5, index=3)\n",
        "E = ds_each_class[0].shard(num_shards=5, index=4)\n",
        "for i in range(1, 10):\n",
        "  A = A.concatenate(ds_each_class[i].shard(num_shards=5, index=0))\n",
        "  B = B.concatenate(ds_each_class[i].shard(num_shards=5, index=1))\n",
        "  C = C.concatenate(ds_each_class[i].shard(num_shards=5, index=2))\n",
        "  D = D.concatenate(ds_each_class[i].shard(num_shards=5, index=3))\n",
        "  E = E.concatenate(ds_each_class[i].shard(num_shards=5, index=4))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9oYdHkhrwdz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bee9e53e-f762-4565-ba42-9f3316c5e8bc"
      },
      "source": [
        "# check no. of samples in each partition is the same\n",
        "print(A.cardinality().numpy())\n",
        "print(B.cardinality().numpy())\n",
        "print(C.cardinality().numpy())\n",
        "print(D.cardinality().numpy())\n",
        "print(E.cardinality().numpy())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BdD3FMHtGRV"
      },
      "source": [
        "def get_label(file_path):\n",
        "  # convert the path to a list of path components\n",
        "  parts = tf.strings.split(file_path, os.path.sep)\n",
        "  # The second to last is the class-directory\n",
        "  one_hot = parts[-2] == class_names\n",
        "  # Integer encode the label\n",
        "  return tf.argmax(one_hot)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfWduaL4tKDV"
      },
      "source": [
        "def decode_img(img):\n",
        "  # convert the compressed string to a 3D uint8 tensor\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\n",
        "  # resize the image to the desired size\n",
        "  return tf.image.resize(img, [img_height, img_width])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bGGe0KltMTC"
      },
      "source": [
        "def process_path(file_path):\n",
        "  label = get_label(file_path)\n",
        "  # load the raw data from the file as a string\n",
        "  img = tf.io.read_file(file_path)\n",
        "  img = decode_img(img)\n",
        "  return img, label"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcyZLwRxt1dy"
      },
      "source": [
        "# prompt the tf.data runtime to tune the number of elements to prefetch dynamically at runtime\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkBqAQlHtblh"
      },
      "source": [
        "# use the path of image to load the image into each partition of the dataset\n",
        "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
        "A = A.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "B = B.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "C = C.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "D = D.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "E = E.map(process_path, num_parallel_calls=AUTOTUNE)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc9pmaQ5t9H5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d263ab24-1908-4508-99ea-40034faccb3d"
      },
      "source": [
        "for image, label in A.take(1):\n",
        "  print(\"Image shape: \", image.numpy().shape)\n",
        "  print(\"Label: \", label.numpy())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image shape:  (299, 299, 3)\n",
            "Label:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_T2KNdGub1X"
      },
      "source": [
        "# shuffle, batch, and prefetch the dataset\n",
        "def configure_for_performance(ds):\n",
        "  ds = ds.cache()\n",
        "  ds = ds.shuffle(buffer_size=1000)\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
        "  return ds"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWcojoVRuok5"
      },
      "source": [
        "# map the dataset partitions to integers for building training/test sets during cross-validation\n",
        "ds_fold_dict = {0:A, 1:B, 2:C, 3:D, 4:E}"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx3xKoMZU9Yv"
      },
      "source": [
        "# normalise the input values to the pre-trained model's required range of values\n",
        "preprocess_input = keras.applications.inception_v3.preprocess_input"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVOP5fIPwEx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aa38f66-996e-4249-f24b-cd068a5fcc9d"
      },
      "source": [
        "# get pre-trained model\n",
        "base_model = keras.applications.InceptionV3(include_top=False, input_shape=(img_height, img_width, 3))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 8s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS2kAceGVW0e"
      },
      "source": [
        "# don't train base model weights\n",
        "base_model.trainable = False"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGkReMX60ScJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a44ee5f-38a6-4547-caa8-8c90835e3533"
      },
      "source": [
        "base_model.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"inception_v3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 299, 299, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 149, 149, 32) 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 149, 149, 32) 96          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 149, 149, 32) 0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 147, 147, 32) 9216        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 147, 147, 32) 96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 147, 147, 32) 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 147, 147, 64) 18432       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 147, 147, 64) 192         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 147, 147, 64) 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 73, 73, 64)   0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 73, 73, 80)   240         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 73, 73, 80)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 71, 71, 192)  138240      activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 71, 71, 192)  576         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 71, 71, 192)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 35, 35, 96)   55296       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 35, 35, 48)   144         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 35, 35, 96)   288         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 35, 35, 48)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 35, 35, 96)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 35, 35, 192)  0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 35, 35, 64)   76800       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 35, 35, 96)   82944       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 35, 35, 64)   192         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 35, 35, 64)   192         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 35, 35, 32)   96          conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 35, 35, 64)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 35, 35, 64)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 35, 35, 32)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_5[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "                                                                 activation_10[0][0]              \n",
            "                                                                 activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 35, 35, 96)   55296       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 35, 35, 48)   144         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 35, 35, 96)   288         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 35, 35, 48)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 35, 35, 96)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 35, 35, 64)   76800       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 35, 35, 96)   82944       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 35, 35, 64)   192         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 35, 35, 64)   192         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 35, 35, 64)   192         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 35, 35, 64)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 35, 35, 64)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 35, 35, 64)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_12[0][0]              \n",
            "                                                                 activation_14[0][0]              \n",
            "                                                                 activation_17[0][0]              \n",
            "                                                                 activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 35, 35, 96)   55296       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 35, 35, 48)   144         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 35, 35, 96)   288         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 35, 35, 48)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 35, 35, 96)   0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 35, 35, 64)   76800       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 35, 35, 96)   82944       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 35, 35, 64)   192         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 35, 35, 64)   192         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 35, 35, 64)   0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 35, 35, 64)   0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_19[0][0]              \n",
            "                                                                 activation_21[0][0]              \n",
            "                                                                 activation_24[0][0]              \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 35, 35, 64)   192         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 35, 35, 64)   0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 35, 35, 96)   55296       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 35, 35, 96)   288         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 35, 35, 96)   0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 17, 17, 96)   82944       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 17, 17, 384)  1152        conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 17, 17, 96)   288         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 17, 17, 384)  0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 17, 17, 96)   0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_26[0][0]              \n",
            "                                                                 activation_29[0][0]              \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 17, 17, 128)  384         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 17, 17, 128)  0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 17, 17, 128)  114688      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 17, 17, 128)  384         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 17, 17, 128)  0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 17, 17, 128)  114688      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 17, 17, 192)  172032      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 17, 17, 192)  172032      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 17, 17, 192)  576         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 17, 17, 192)  576         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 17, 17, 192)  576         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 17, 17, 192)  0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 17, 17, 192)  0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 17, 17, 192)  0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_30[0][0]              \n",
            "                                                                 activation_33[0][0]              \n",
            "                                                                 activation_38[0][0]              \n",
            "                                                                 activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 17, 17, 160)  480         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 17, 17, 160)  0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 17, 17, 160)  179200      activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 17, 17, 160)  480         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 17, 17, 160)  0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 17, 17, 160)  179200      activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 17, 17, 192)  215040      activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 17, 17, 192)  215040      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 17, 17, 192)  576         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 17, 17, 192)  576         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 17, 17, 192)  0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 17, 17, 192)  0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_40[0][0]              \n",
            "                                                                 activation_43[0][0]              \n",
            "                                                                 activation_48[0][0]              \n",
            "                                                                 activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 17, 17, 160)  480         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 17, 17, 160)  0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 17, 17, 160)  179200      activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 17, 17, 160)  480         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 17, 17, 160)  0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 17, 17, 160)  179200      activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 17, 17, 192)  215040      activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 17, 17, 192)  215040      activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 17, 17, 192)  576         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 17, 17, 192)  576         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 17, 17, 192)  0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 17, 17, 192)  0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_50[0][0]              \n",
            "                                                                 activation_53[0][0]              \n",
            "                                                                 activation_58[0][0]              \n",
            "                                                                 activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 17, 17, 192)  258048      activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 17, 17, 192)  258048      activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_60[0][0]              \n",
            "                                                                 activation_63[0][0]              \n",
            "                                                                 activation_68[0][0]              \n",
            "                                                                 activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 17, 17, 192)  576         conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 17, 17, 192)  0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 17, 17, 192)  258048      activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 8, 8, 320)    552960      activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 8, 8, 192)    331776      activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 8, 8, 320)    960         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 8, 8, 192)    576         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 8, 8, 320)    0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 8, 8, 192)    0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_71[0][0]              \n",
            "                                                                 activation_75[0][0]              \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 8, 8, 448)    1344        conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 8, 8, 448)    0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 8, 8, 384)    1548288     activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 8, 8, 384)    1152        conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 8, 8, 384)    1152        conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 8, 8, 384)    0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 8, 8, 384)    0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 8, 8, 384)    442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 8, 8, 384)    442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 8, 8, 320)    960         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 8, 8, 192)    576         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 8, 8, 320)    0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_78[0][0]              \n",
            "                                                                 activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 8, 8, 768)    0           activation_82[0][0]              \n",
            "                                                                 activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 8, 8, 192)    0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_76[0][0]              \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate[0][0]                \n",
            "                                                                 activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, 8, 8, 448)    1344        conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 8, 8, 448)    0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 8, 8, 384)    1548288     activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, 8, 8, 384)    1152        conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, 8, 8, 384)    1152        conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 8, 8, 384)    0           batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 8, 8, 384)    0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 8, 8, 384)    442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 8, 8, 384)    442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_8 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 8, 8, 320)    960         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, 8, 8, 192)    576         conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 8, 8, 320)    0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_87[0][0]              \n",
            "                                                                 activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_91[0][0]              \n",
            "                                                                 activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 8, 8, 192)    0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_85[0][0]              \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_1[0][0]              \n",
            "                                                                 activation_93[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 21,802,784\n",
            "Trainable params: 0\n",
            "Non-trainable params: 21,802,784\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzhaWb2aX2if"
      },
      "source": [
        "# set a low learning rate to avoid overfitting too quickly\n",
        "base_learning_rate = 0.0001\n",
        "\n",
        "# create the model to train using the pre-trained model as base model\n",
        "def create_model():\n",
        "  # generate additional training data from input training data by augmenting them using random flip, rotation & zoom\n",
        "  data_augmentation = keras.Sequential(\n",
        "    [\n",
        "      layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n",
        "                                                  input_shape=(img_height, \n",
        "                                                                img_width,\n",
        "                                                                3)),\n",
        "      layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "      layers.experimental.preprocessing.RandomZoom(0.1),\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  # average over the spatial locations to convert the features to a single vector per image\n",
        "  global_average_layer = keras.layers.GlobalAveragePooling2D()\n",
        "  # convert these features into a single prediction per image\n",
        "  prediction_layer = keras.layers.Dense(10)\n",
        "\n",
        "  # Build a model by chaining together the layers using the Keras Functional API.\n",
        "  inputs = keras.Input(shape=(img_height, img_width, 3))\n",
        "  x = data_augmentation(inputs)\n",
        "  x = preprocess_input(x)\n",
        "  x = base_model(x, training=False) # use training=False as the base model contains a BatchNormalization layer\n",
        "  x = global_average_layer(x)\n",
        "  x = keras.layers.Dropout(0.2)(x) # add dropout to fully connected layer to reduce overfitting\n",
        "  outputs = prediction_layer(x)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  optimizer = keras.optimizers.Adam(lr=base_learning_rate)\n",
        "  loss= keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  # compile model with Adam optimizer with specified learning rate\n",
        "  model.compile(optimizer=optimizer,\n",
        "                loss=loss,\n",
        "                metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-5TEZRgY2l_"
      },
      "source": [
        "no_epochs = 100"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya698zRbu7Ep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ead6405-0d6a-4373-fac4-0901a73b4a9f"
      },
      "source": [
        "# store results to plot graphs and get cross-validated accuracies\n",
        "history_map = {}\n",
        "base_model_acc_list = []\n",
        "final_acc_list = []\n",
        "\n",
        "# do 5-fold cross-validation\n",
        "for i in range(5):\n",
        "  print('fold', i + 1)\n",
        "  temp_dict = ds_fold_dict.copy()\n",
        "  # get test set for this iteration\n",
        "  current_val_ds = temp_dict[i]\n",
        "\n",
        "  # get training set for this iteration from remaining data samples\n",
        "  del temp_dict[i]\n",
        "  current_train_ds = None\n",
        "  for ds_shard in temp_dict.values():\n",
        "    if current_train_ds is None:\n",
        "      current_train_ds = ds_shard\n",
        "    else:\n",
        "      current_train_ds = current_train_ds.concatenate(ds_shard)\n",
        "  \n",
        "  # configure both training and test sets to improve performance\n",
        "  current_train_ds = configure_for_performance(current_train_ds)\n",
        "  current_val_ds = configure_for_performance(current_val_ds)\n",
        "\n",
        "  # create a new model\n",
        "  model = create_model()\n",
        "  # get initial test accuracy\n",
        "  base_model_acc_list.append(model.evaluate(current_val_ds)[1])\n",
        "  # train for specified epochs\n",
        "  history = model.fit(current_train_ds,\n",
        "                    epochs=no_epochs,\n",
        "                    validation_data=current_val_ds)\n",
        "  \n",
        "  # save results\n",
        "  if i == 0:\n",
        "    history_map['accuracy'] = [history.history['accuracy']]\n",
        "    history_map['val_accuracy'] = [history.history['val_accuracy']]\n",
        "    history_map['loss'] = [history.history['loss']]\n",
        "    history_map['val_loss'] = [history.history['val_loss']]\n",
        "  else:\n",
        "    history_map['accuracy'].append(history.history['accuracy'])\n",
        "    history_map['val_accuracy'].append(history.history['val_accuracy'])\n",
        "    history_map['loss'].append(history.history['loss'])\n",
        "    history_map['val_loss'].append(history.history['val_loss'])\n",
        "  \n",
        "  # get final test accuracy by taking the max accuracy over the whole training\n",
        "  final_acc_list.append(np.amax(history.history['val_accuracy']))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fold 1\n",
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential (Sequential)      (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv (TensorF [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub (TensorFlowO [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "inception_v3 (Functional)    (None, 8, 8, 2048)        21802784  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 21,823,274\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 2s 135ms/step - loss: 2.4031 - accuracy: 0.1600\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 2.4038 - accuracy: 0.1238 - val_loss: 2.1765 - val_accuracy: 0.2300\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 2.1702 - accuracy: 0.2250 - val_loss: 2.0166 - val_accuracy: 0.3200\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 1.9974 - accuracy: 0.3113 - val_loss: 1.8703 - val_accuracy: 0.4300\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 1.8500 - accuracy: 0.3800 - val_loss: 1.7344 - val_accuracy: 0.4800\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 1.7125 - accuracy: 0.4675 - val_loss: 1.6255 - val_accuracy: 0.5350\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 1.6045 - accuracy: 0.5138 - val_loss: 1.5228 - val_accuracy: 0.5900\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 1.4805 - accuracy: 0.5975 - val_loss: 1.4322 - val_accuracy: 0.6200\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 1.4098 - accuracy: 0.6187 - val_loss: 1.3540 - val_accuracy: 0.6300\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 6s 121ms/step - loss: 1.3425 - accuracy: 0.6338 - val_loss: 1.2880 - val_accuracy: 0.6800\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 1.2366 - accuracy: 0.6963 - val_loss: 1.2214 - val_accuracy: 0.7050\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 1.2058 - accuracy: 0.6913 - val_loss: 1.1705 - val_accuracy: 0.7300\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 1.1197 - accuracy: 0.7250 - val_loss: 1.1235 - val_accuracy: 0.7250\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 1.0650 - accuracy: 0.7250 - val_loss: 1.0793 - val_accuracy: 0.7400\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 6s 121ms/step - loss: 1.0345 - accuracy: 0.7462 - val_loss: 1.0417 - val_accuracy: 0.7600\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 6s 121ms/step - loss: 1.0132 - accuracy: 0.7412 - val_loss: 1.0100 - val_accuracy: 0.7600\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 0.9719 - accuracy: 0.7525 - val_loss: 0.9768 - val_accuracy: 0.7700\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 0.9275 - accuracy: 0.7675 - val_loss: 0.9510 - val_accuracy: 0.7650\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 0.8722 - accuracy: 0.7962 - val_loss: 0.9246 - val_accuracy: 0.7700\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 6s 121ms/step - loss: 0.8698 - accuracy: 0.7837 - val_loss: 0.9017 - val_accuracy: 0.7800\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 6s 121ms/step - loss: 0.8224 - accuracy: 0.8025 - val_loss: 0.8806 - val_accuracy: 0.7750\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.8125 - accuracy: 0.7937 - val_loss: 0.8585 - val_accuracy: 0.7850\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 6s 121ms/step - loss: 0.7749 - accuracy: 0.8188 - val_loss: 0.8415 - val_accuracy: 0.7850\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 0.7807 - accuracy: 0.7975 - val_loss: 0.8249 - val_accuracy: 0.7900\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 0.7389 - accuracy: 0.8238 - val_loss: 0.8104 - val_accuracy: 0.7900\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 0.7116 - accuracy: 0.8338 - val_loss: 0.7950 - val_accuracy: 0.7950\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 6s 121ms/step - loss: 0.7179 - accuracy: 0.8275 - val_loss: 0.7792 - val_accuracy: 0.7900\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.6785 - accuracy: 0.8350 - val_loss: 0.7689 - val_accuracy: 0.7950\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 0.6927 - accuracy: 0.8263 - val_loss: 0.7541 - val_accuracy: 0.7950\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 0.6644 - accuracy: 0.8438 - val_loss: 0.7477 - val_accuracy: 0.8000\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 0.6578 - accuracy: 0.8213 - val_loss: 0.7344 - val_accuracy: 0.8000\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 0.6404 - accuracy: 0.8375 - val_loss: 0.7254 - val_accuracy: 0.8000\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.6354 - accuracy: 0.8438 - val_loss: 0.7202 - val_accuracy: 0.8000\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.6290 - accuracy: 0.8512 - val_loss: 0.7103 - val_accuracy: 0.8150\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 0.6006 - accuracy: 0.8475 - val_loss: 0.7031 - val_accuracy: 0.8000\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 0.5837 - accuracy: 0.8625 - val_loss: 0.6928 - val_accuracy: 0.8050\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.5808 - accuracy: 0.8525 - val_loss: 0.6859 - val_accuracy: 0.8100\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.5721 - accuracy: 0.8600 - val_loss: 0.6810 - val_accuracy: 0.8100\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.5579 - accuracy: 0.8562 - val_loss: 0.6726 - val_accuracy: 0.8100\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.5598 - accuracy: 0.8612 - val_loss: 0.6680 - val_accuracy: 0.8000\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.5372 - accuracy: 0.8737 - val_loss: 0.6579 - val_accuracy: 0.8100\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.5342 - accuracy: 0.8712 - val_loss: 0.6543 - val_accuracy: 0.8050\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 0.5213 - accuracy: 0.8725 - val_loss: 0.6512 - val_accuracy: 0.8100\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.5078 - accuracy: 0.8788 - val_loss: 0.6474 - val_accuracy: 0.8100\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.5026 - accuracy: 0.8800 - val_loss: 0.6387 - val_accuracy: 0.8150\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4903 - accuracy: 0.8737 - val_loss: 0.6367 - val_accuracy: 0.8050\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4770 - accuracy: 0.8888 - val_loss: 0.6326 - val_accuracy: 0.8100\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4969 - accuracy: 0.8675 - val_loss: 0.6295 - val_accuracy: 0.8050\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4966 - accuracy: 0.8750 - val_loss: 0.6270 - val_accuracy: 0.8100\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4686 - accuracy: 0.8838 - val_loss: 0.6222 - val_accuracy: 0.8050\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4679 - accuracy: 0.8888 - val_loss: 0.6150 - val_accuracy: 0.8050\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4524 - accuracy: 0.8825 - val_loss: 0.6192 - val_accuracy: 0.8050\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4524 - accuracy: 0.8938 - val_loss: 0.6064 - val_accuracy: 0.8050\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4412 - accuracy: 0.8925 - val_loss: 0.6062 - val_accuracy: 0.8000\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4381 - accuracy: 0.8913 - val_loss: 0.6030 - val_accuracy: 0.8100\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4337 - accuracy: 0.8963 - val_loss: 0.5979 - val_accuracy: 0.8050\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4369 - accuracy: 0.8963 - val_loss: 0.5978 - val_accuracy: 0.8150\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4238 - accuracy: 0.9038 - val_loss: 0.5962 - val_accuracy: 0.8000\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4192 - accuracy: 0.8938 - val_loss: 0.5966 - val_accuracy: 0.8050\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4257 - accuracy: 0.8988 - val_loss: 0.5902 - val_accuracy: 0.8100\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3998 - accuracy: 0.9062 - val_loss: 0.5887 - val_accuracy: 0.8100\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3998 - accuracy: 0.8963 - val_loss: 0.5811 - val_accuracy: 0.8050\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4066 - accuracy: 0.9025 - val_loss: 0.5867 - val_accuracy: 0.8100\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3923 - accuracy: 0.8988 - val_loss: 0.5835 - val_accuracy: 0.8150\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3969 - accuracy: 0.9038 - val_loss: 0.5779 - val_accuracy: 0.8150\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.4043 - accuracy: 0.9050 - val_loss: 0.5798 - val_accuracy: 0.8100\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3833 - accuracy: 0.9075 - val_loss: 0.5764 - val_accuracy: 0.8150\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3916 - accuracy: 0.8988 - val_loss: 0.5739 - val_accuracy: 0.8200\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3636 - accuracy: 0.9100 - val_loss: 0.5694 - val_accuracy: 0.8150\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3699 - accuracy: 0.9162 - val_loss: 0.5719 - val_accuracy: 0.8200\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3531 - accuracy: 0.9175 - val_loss: 0.5720 - val_accuracy: 0.8100\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3627 - accuracy: 0.9150 - val_loss: 0.5710 - val_accuracy: 0.8100\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3493 - accuracy: 0.9150 - val_loss: 0.5651 - val_accuracy: 0.8150\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3594 - accuracy: 0.8975 - val_loss: 0.5643 - val_accuracy: 0.8100\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3366 - accuracy: 0.9287 - val_loss: 0.5641 - val_accuracy: 0.8150\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3357 - accuracy: 0.9212 - val_loss: 0.5625 - val_accuracy: 0.8150\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3300 - accuracy: 0.9237 - val_loss: 0.5591 - val_accuracy: 0.8200\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3412 - accuracy: 0.9137 - val_loss: 0.5571 - val_accuracy: 0.8150\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3342 - accuracy: 0.9150 - val_loss: 0.5575 - val_accuracy: 0.8250\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3263 - accuracy: 0.9287 - val_loss: 0.5576 - val_accuracy: 0.8250\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3379 - accuracy: 0.9162 - val_loss: 0.5537 - val_accuracy: 0.8200\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.3184 - accuracy: 0.9225 - val_loss: 0.5545 - val_accuracy: 0.8100\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3108 - accuracy: 0.9225 - val_loss: 0.5548 - val_accuracy: 0.8100\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3187 - accuracy: 0.9225 - val_loss: 0.5540 - val_accuracy: 0.8200\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.3126 - accuracy: 0.9200 - val_loss: 0.5549 - val_accuracy: 0.8250\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3084 - accuracy: 0.9250 - val_loss: 0.5505 - val_accuracy: 0.8150\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3169 - accuracy: 0.9187 - val_loss: 0.5484 - val_accuracy: 0.8150\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3120 - accuracy: 0.9275 - val_loss: 0.5452 - val_accuracy: 0.8200\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.2879 - accuracy: 0.9300 - val_loss: 0.5463 - val_accuracy: 0.8250\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3015 - accuracy: 0.9337 - val_loss: 0.5475 - val_accuracy: 0.8200\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3056 - accuracy: 0.9362 - val_loss: 0.5487 - val_accuracy: 0.8100\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.2938 - accuracy: 0.9375 - val_loss: 0.5438 - val_accuracy: 0.8150\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.2764 - accuracy: 0.9362 - val_loss: 0.5444 - val_accuracy: 0.8150\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.2728 - accuracy: 0.9350 - val_loss: 0.5476 - val_accuracy: 0.8150\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.2844 - accuracy: 0.9325 - val_loss: 0.5410 - val_accuracy: 0.8200\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.2876 - accuracy: 0.9275 - val_loss: 0.5436 - val_accuracy: 0.8150\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.2939 - accuracy: 0.9350 - val_loss: 0.5406 - val_accuracy: 0.8200\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.2731 - accuracy: 0.9400 - val_loss: 0.5378 - val_accuracy: 0.8100\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.2646 - accuracy: 0.9400 - val_loss: 0.5419 - val_accuracy: 0.8050\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.2857 - accuracy: 0.9250 - val_loss: 0.5399 - val_accuracy: 0.8100\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.2663 - accuracy: 0.9362 - val_loss: 0.5373 - val_accuracy: 0.8100\n",
            "fold 2\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_1 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_1 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_1 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "inception_v3 (Functional)    (None, 8, 8, 2048)        21802784  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 21,823,274\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 1s 84ms/step - loss: 2.6307 - accuracy: 0.1100\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 2.5011 - accuracy: 0.1125 - val_loss: 2.2970 - val_accuracy: 0.1250\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 2.2691 - accuracy: 0.1762 - val_loss: 2.1173 - val_accuracy: 0.2500\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 2.0723 - accuracy: 0.2875 - val_loss: 1.9558 - val_accuracy: 0.3350\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 1.9338 - accuracy: 0.3487 - val_loss: 1.8074 - val_accuracy: 0.4400\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 1.7646 - accuracy: 0.4550 - val_loss: 1.6811 - val_accuracy: 0.5300\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 1.6491 - accuracy: 0.5188 - val_loss: 1.5686 - val_accuracy: 0.5950\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 1.5278 - accuracy: 0.5813 - val_loss: 1.4705 - val_accuracy: 0.6500\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 1.4273 - accuracy: 0.6288 - val_loss: 1.3876 - val_accuracy: 0.6750\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 1.3521 - accuracy: 0.6712 - val_loss: 1.3103 - val_accuracy: 0.7000\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 1.2650 - accuracy: 0.6762 - val_loss: 1.2469 - val_accuracy: 0.7150\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 1.2234 - accuracy: 0.6913 - val_loss: 1.1890 - val_accuracy: 0.7300\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 1.1254 - accuracy: 0.7337 - val_loss: 1.1372 - val_accuracy: 0.7300\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 1.0883 - accuracy: 0.7400 - val_loss: 1.0921 - val_accuracy: 0.7400\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 1.0454 - accuracy: 0.7550 - val_loss: 1.0503 - val_accuracy: 0.7700\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 1.0282 - accuracy: 0.7613 - val_loss: 1.0195 - val_accuracy: 0.7650\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.9744 - accuracy: 0.7725 - val_loss: 0.9853 - val_accuracy: 0.7850\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.9413 - accuracy: 0.7663 - val_loss: 0.9553 - val_accuracy: 0.7800\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.9005 - accuracy: 0.7887 - val_loss: 0.9348 - val_accuracy: 0.7800\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.8722 - accuracy: 0.7800 - val_loss: 0.9121 - val_accuracy: 0.7850\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.8713 - accuracy: 0.7750 - val_loss: 0.8860 - val_accuracy: 0.7900\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.8017 - accuracy: 0.8075 - val_loss: 0.8653 - val_accuracy: 0.7750\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.8030 - accuracy: 0.7975 - val_loss: 0.8502 - val_accuracy: 0.7850\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.7724 - accuracy: 0.8150 - val_loss: 0.8337 - val_accuracy: 0.7850\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.7570 - accuracy: 0.8100 - val_loss: 0.8137 - val_accuracy: 0.7850\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.7096 - accuracy: 0.8350 - val_loss: 0.8025 - val_accuracy: 0.7900\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.6918 - accuracy: 0.8363 - val_loss: 0.7873 - val_accuracy: 0.8050\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.7168 - accuracy: 0.8275 - val_loss: 0.7742 - val_accuracy: 0.7950\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.7016 - accuracy: 0.8350 - val_loss: 0.7666 - val_accuracy: 0.7950\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.6799 - accuracy: 0.8300 - val_loss: 0.7572 - val_accuracy: 0.8000\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.6421 - accuracy: 0.8462 - val_loss: 0.7476 - val_accuracy: 0.8050\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.6396 - accuracy: 0.8388 - val_loss: 0.7366 - val_accuracy: 0.7950\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.6217 - accuracy: 0.8500 - val_loss: 0.7253 - val_accuracy: 0.8050\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 0.5987 - accuracy: 0.8600 - val_loss: 0.7190 - val_accuracy: 0.8150\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 0.6086 - accuracy: 0.8550 - val_loss: 0.7102 - val_accuracy: 0.8150\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.5896 - accuracy: 0.8763 - val_loss: 0.7052 - val_accuracy: 0.8150\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.5840 - accuracy: 0.8575 - val_loss: 0.6962 - val_accuracy: 0.8150\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.5640 - accuracy: 0.8700 - val_loss: 0.6889 - val_accuracy: 0.8100\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.5362 - accuracy: 0.8763 - val_loss: 0.6826 - val_accuracy: 0.8100\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.5563 - accuracy: 0.8662 - val_loss: 0.6781 - val_accuracy: 0.8150\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.5639 - accuracy: 0.8525 - val_loss: 0.6734 - val_accuracy: 0.8050\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.5263 - accuracy: 0.8675 - val_loss: 0.6667 - val_accuracy: 0.8150\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.5193 - accuracy: 0.8725 - val_loss: 0.6600 - val_accuracy: 0.8100\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.5150 - accuracy: 0.8662 - val_loss: 0.6517 - val_accuracy: 0.8200\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.5030 - accuracy: 0.8863 - val_loss: 0.6525 - val_accuracy: 0.8100\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4955 - accuracy: 0.8888 - val_loss: 0.6459 - val_accuracy: 0.8100\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.4820 - accuracy: 0.8863 - val_loss: 0.6422 - val_accuracy: 0.8150\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 6s 121ms/step - loss: 0.4705 - accuracy: 0.8825 - val_loss: 0.6389 - val_accuracy: 0.8200\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 0.4747 - accuracy: 0.8863 - val_loss: 0.6351 - val_accuracy: 0.8150\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.4708 - accuracy: 0.8950 - val_loss: 0.6307 - val_accuracy: 0.8150\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.4615 - accuracy: 0.8875 - val_loss: 0.6302 - val_accuracy: 0.8200\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.4392 - accuracy: 0.8875 - val_loss: 0.6268 - val_accuracy: 0.8300\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4303 - accuracy: 0.8938 - val_loss: 0.6208 - val_accuracy: 0.8250\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4319 - accuracy: 0.9100 - val_loss: 0.6165 - val_accuracy: 0.8200\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 6s 121ms/step - loss: 0.4357 - accuracy: 0.9075 - val_loss: 0.6198 - val_accuracy: 0.8300\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.4415 - accuracy: 0.8788 - val_loss: 0.6097 - val_accuracy: 0.8250\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.4265 - accuracy: 0.8963 - val_loss: 0.6067 - val_accuracy: 0.8300\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.4209 - accuracy: 0.8863 - val_loss: 0.6050 - val_accuracy: 0.8200\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.4025 - accuracy: 0.9112 - val_loss: 0.6029 - val_accuracy: 0.8350\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.4098 - accuracy: 0.8950 - val_loss: 0.6001 - val_accuracy: 0.8250\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.4144 - accuracy: 0.9075 - val_loss: 0.5998 - val_accuracy: 0.8250\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3848 - accuracy: 0.9087 - val_loss: 0.5966 - val_accuracy: 0.8300\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.4122 - accuracy: 0.8988 - val_loss: 0.5930 - val_accuracy: 0.8350\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3913 - accuracy: 0.9000 - val_loss: 0.5905 - val_accuracy: 0.8350\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3795 - accuracy: 0.9075 - val_loss: 0.5888 - val_accuracy: 0.8350\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3825 - accuracy: 0.9062 - val_loss: 0.5873 - val_accuracy: 0.8300\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3994 - accuracy: 0.8950 - val_loss: 0.5865 - val_accuracy: 0.8300\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 0.3798 - accuracy: 0.9062 - val_loss: 0.5826 - val_accuracy: 0.8300\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3626 - accuracy: 0.9150 - val_loss: 0.5840 - val_accuracy: 0.8350\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3541 - accuracy: 0.9162 - val_loss: 0.5825 - val_accuracy: 0.8300\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3831 - accuracy: 0.9013 - val_loss: 0.5795 - val_accuracy: 0.8350\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3582 - accuracy: 0.9100 - val_loss: 0.5806 - val_accuracy: 0.8350\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3476 - accuracy: 0.9212 - val_loss: 0.5730 - val_accuracy: 0.8350\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3533 - accuracy: 0.9162 - val_loss: 0.5759 - val_accuracy: 0.8450\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3281 - accuracy: 0.9337 - val_loss: 0.5706 - val_accuracy: 0.8450\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 0.3388 - accuracy: 0.9137 - val_loss: 0.5736 - val_accuracy: 0.8350\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3498 - accuracy: 0.9237 - val_loss: 0.5679 - val_accuracy: 0.8300\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 0.3267 - accuracy: 0.9300 - val_loss: 0.5704 - val_accuracy: 0.8350\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3591 - accuracy: 0.9025 - val_loss: 0.5651 - val_accuracy: 0.8400\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3118 - accuracy: 0.9350 - val_loss: 0.5653 - val_accuracy: 0.8400\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3329 - accuracy: 0.9225 - val_loss: 0.5657 - val_accuracy: 0.8250\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3359 - accuracy: 0.9200 - val_loss: 0.5664 - val_accuracy: 0.8300\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 0.3262 - accuracy: 0.9275 - val_loss: 0.5656 - val_accuracy: 0.8400\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3033 - accuracy: 0.9275 - val_loss: 0.5614 - val_accuracy: 0.8450\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.2940 - accuracy: 0.9375 - val_loss: 0.5575 - val_accuracy: 0.8300\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3112 - accuracy: 0.9300 - val_loss: 0.5620 - val_accuracy: 0.8350\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 0.3036 - accuracy: 0.9300 - val_loss: 0.5547 - val_accuracy: 0.8350\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.2986 - accuracy: 0.9287 - val_loss: 0.5587 - val_accuracy: 0.8300\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3039 - accuracy: 0.9225 - val_loss: 0.5571 - val_accuracy: 0.8400\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.2770 - accuracy: 0.9450 - val_loss: 0.5567 - val_accuracy: 0.8250\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.2782 - accuracy: 0.9425 - val_loss: 0.5542 - val_accuracy: 0.8300\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.2914 - accuracy: 0.9275 - val_loss: 0.5571 - val_accuracy: 0.8300\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.2798 - accuracy: 0.9413 - val_loss: 0.5536 - val_accuracy: 0.8350\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.2830 - accuracy: 0.9300 - val_loss: 0.5537 - val_accuracy: 0.8250\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.2805 - accuracy: 0.9362 - val_loss: 0.5472 - val_accuracy: 0.8250\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.2596 - accuracy: 0.9450 - val_loss: 0.5530 - val_accuracy: 0.8300\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 0.2792 - accuracy: 0.9337 - val_loss: 0.5533 - val_accuracy: 0.8400\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.2819 - accuracy: 0.9362 - val_loss: 0.5522 - val_accuracy: 0.8400\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.2675 - accuracy: 0.9362 - val_loss: 0.5521 - val_accuracy: 0.8400\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.2739 - accuracy: 0.9388 - val_loss: 0.5568 - val_accuracy: 0.8350\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.2657 - accuracy: 0.9350 - val_loss: 0.5532 - val_accuracy: 0.8300\n",
            "fold 3\n",
            "Model: \"functional_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_2 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_2 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_2 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "inception_v3 (Functional)    (None, 8, 8, 2048)        21802784  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 21,823,274\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 1s 84ms/step - loss: 2.4772 - accuracy: 0.0550\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 2.4122 - accuracy: 0.1225 - val_loss: 2.2475 - val_accuracy: 0.1700\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 2.1794 - accuracy: 0.2200 - val_loss: 2.0901 - val_accuracy: 0.2750\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 2.0093 - accuracy: 0.2887 - val_loss: 1.9541 - val_accuracy: 0.3600\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 1.8340 - accuracy: 0.4075 - val_loss: 1.8249 - val_accuracy: 0.4400\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 1.7420 - accuracy: 0.4725 - val_loss: 1.7116 - val_accuracy: 0.5250\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 1.5948 - accuracy: 0.5163 - val_loss: 1.6188 - val_accuracy: 0.5650\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 1.4619 - accuracy: 0.6137 - val_loss: 1.5292 - val_accuracy: 0.6100\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 1.4045 - accuracy: 0.6313 - val_loss: 1.4512 - val_accuracy: 0.6450\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 1.3004 - accuracy: 0.6662 - val_loss: 1.3852 - val_accuracy: 0.6550\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 1.2420 - accuracy: 0.6875 - val_loss: 1.3258 - val_accuracy: 0.6750\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 1.1664 - accuracy: 0.7175 - val_loss: 1.2750 - val_accuracy: 0.6950\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 1.1056 - accuracy: 0.7312 - val_loss: 1.2261 - val_accuracy: 0.7050\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 1.0413 - accuracy: 0.7400 - val_loss: 1.1841 - val_accuracy: 0.7150\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 1.0192 - accuracy: 0.7513 - val_loss: 1.1485 - val_accuracy: 0.7450\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.9890 - accuracy: 0.7462 - val_loss: 1.1157 - val_accuracy: 0.7350\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.9304 - accuracy: 0.7663 - val_loss: 1.0832 - val_accuracy: 0.7500\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.8996 - accuracy: 0.7962 - val_loss: 1.0550 - val_accuracy: 0.7500\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.8817 - accuracy: 0.7875 - val_loss: 1.0287 - val_accuracy: 0.7550\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.8498 - accuracy: 0.7875 - val_loss: 1.0123 - val_accuracy: 0.7550\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.8000 - accuracy: 0.8150 - val_loss: 0.9864 - val_accuracy: 0.7700\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.7778 - accuracy: 0.8150 - val_loss: 0.9718 - val_accuracy: 0.7600\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.7450 - accuracy: 0.8350 - val_loss: 0.9468 - val_accuracy: 0.7700\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.7505 - accuracy: 0.8188 - val_loss: 0.9323 - val_accuracy: 0.7550\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.7425 - accuracy: 0.8188 - val_loss: 0.9164 - val_accuracy: 0.7600\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.6822 - accuracy: 0.8350 - val_loss: 0.9010 - val_accuracy: 0.7600\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.6751 - accuracy: 0.8188 - val_loss: 0.8879 - val_accuracy: 0.7650\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.6755 - accuracy: 0.8175 - val_loss: 0.8762 - val_accuracy: 0.7750\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.6688 - accuracy: 0.8400 - val_loss: 0.8643 - val_accuracy: 0.7750\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.6667 - accuracy: 0.8375 - val_loss: 0.8489 - val_accuracy: 0.7900\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.6412 - accuracy: 0.8375 - val_loss: 0.8477 - val_accuracy: 0.7700\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.6113 - accuracy: 0.8487 - val_loss: 0.8314 - val_accuracy: 0.7850\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.5829 - accuracy: 0.8550 - val_loss: 0.8251 - val_accuracy: 0.7800\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.5846 - accuracy: 0.8500 - val_loss: 0.8140 - val_accuracy: 0.7750\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.5904 - accuracy: 0.8475 - val_loss: 0.8020 - val_accuracy: 0.7850\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.5528 - accuracy: 0.8612 - val_loss: 0.7972 - val_accuracy: 0.7850\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.5670 - accuracy: 0.8487 - val_loss: 0.7867 - val_accuracy: 0.7850\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.5192 - accuracy: 0.8950 - val_loss: 0.7858 - val_accuracy: 0.7700\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.5337 - accuracy: 0.8675 - val_loss: 0.7795 - val_accuracy: 0.7850\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.5196 - accuracy: 0.8650 - val_loss: 0.7747 - val_accuracy: 0.7650\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 6s 122ms/step - loss: 0.5078 - accuracy: 0.8788 - val_loss: 0.7662 - val_accuracy: 0.7750\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.5069 - accuracy: 0.8662 - val_loss: 0.7631 - val_accuracy: 0.7900\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.4925 - accuracy: 0.8700 - val_loss: 0.7543 - val_accuracy: 0.7750\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.4777 - accuracy: 0.8737 - val_loss: 0.7571 - val_accuracy: 0.7750\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4883 - accuracy: 0.8863 - val_loss: 0.7421 - val_accuracy: 0.7750\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4835 - accuracy: 0.8725 - val_loss: 0.7413 - val_accuracy: 0.7750\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.4518 - accuracy: 0.8925 - val_loss: 0.7381 - val_accuracy: 0.7700\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4571 - accuracy: 0.8888 - val_loss: 0.7302 - val_accuracy: 0.7700\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4524 - accuracy: 0.8850 - val_loss: 0.7286 - val_accuracy: 0.7700\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.4694 - accuracy: 0.8737 - val_loss: 0.7265 - val_accuracy: 0.7750\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4554 - accuracy: 0.8888 - val_loss: 0.7206 - val_accuracy: 0.7750\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4202 - accuracy: 0.8900 - val_loss: 0.7223 - val_accuracy: 0.7850\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.4268 - accuracy: 0.8988 - val_loss: 0.7196 - val_accuracy: 0.7800\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4508 - accuracy: 0.8863 - val_loss: 0.7108 - val_accuracy: 0.7700\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.4195 - accuracy: 0.8938 - val_loss: 0.7119 - val_accuracy: 0.7800\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.4141 - accuracy: 0.9013 - val_loss: 0.7048 - val_accuracy: 0.7750\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.4044 - accuracy: 0.9050 - val_loss: 0.7031 - val_accuracy: 0.7750\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4138 - accuracy: 0.8988 - val_loss: 0.7031 - val_accuracy: 0.7850\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3809 - accuracy: 0.9013 - val_loss: 0.7002 - val_accuracy: 0.7800\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4005 - accuracy: 0.8950 - val_loss: 0.6944 - val_accuracy: 0.7800\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3843 - accuracy: 0.8975 - val_loss: 0.6892 - val_accuracy: 0.7850\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3853 - accuracy: 0.9125 - val_loss: 0.6912 - val_accuracy: 0.7800\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.3832 - accuracy: 0.9050 - val_loss: 0.6909 - val_accuracy: 0.7800\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3600 - accuracy: 0.9100 - val_loss: 0.6888 - val_accuracy: 0.7800\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3558 - accuracy: 0.9137 - val_loss: 0.6824 - val_accuracy: 0.7900\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.3637 - accuracy: 0.9162 - val_loss: 0.6799 - val_accuracy: 0.7800\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3555 - accuracy: 0.9250 - val_loss: 0.6837 - val_accuracy: 0.7750\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.3425 - accuracy: 0.9175 - val_loss: 0.6850 - val_accuracy: 0.7850\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3437 - accuracy: 0.9250 - val_loss: 0.6780 - val_accuracy: 0.7750\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.3414 - accuracy: 0.9112 - val_loss: 0.6776 - val_accuracy: 0.7900\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.3408 - accuracy: 0.9087 - val_loss: 0.6822 - val_accuracy: 0.7800\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.3362 - accuracy: 0.9137 - val_loss: 0.6747 - val_accuracy: 0.7900\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.3238 - accuracy: 0.9300 - val_loss: 0.6635 - val_accuracy: 0.7850\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.3238 - accuracy: 0.9162 - val_loss: 0.6670 - val_accuracy: 0.7750\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3235 - accuracy: 0.9262 - val_loss: 0.6669 - val_accuracy: 0.7850\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3117 - accuracy: 0.9325 - val_loss: 0.6594 - val_accuracy: 0.7850\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.3399 - accuracy: 0.9150 - val_loss: 0.6578 - val_accuracy: 0.7850\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.3188 - accuracy: 0.9212 - val_loss: 0.6591 - val_accuracy: 0.7750\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.3264 - accuracy: 0.9150 - val_loss: 0.6603 - val_accuracy: 0.7800\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.3100 - accuracy: 0.9337 - val_loss: 0.6490 - val_accuracy: 0.7900\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.2939 - accuracy: 0.9262 - val_loss: 0.6517 - val_accuracy: 0.7750\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.3082 - accuracy: 0.9225 - val_loss: 0.6583 - val_accuracy: 0.7850\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.3030 - accuracy: 0.9175 - val_loss: 0.6568 - val_accuracy: 0.7800\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.2990 - accuracy: 0.9375 - val_loss: 0.6531 - val_accuracy: 0.7750\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.3061 - accuracy: 0.9300 - val_loss: 0.6483 - val_accuracy: 0.7750\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.3003 - accuracy: 0.9187 - val_loss: 0.6498 - val_accuracy: 0.7800\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.2706 - accuracy: 0.9375 - val_loss: 0.6520 - val_accuracy: 0.7850\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.2746 - accuracy: 0.9337 - val_loss: 0.6469 - val_accuracy: 0.7750\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.2877 - accuracy: 0.9325 - val_loss: 0.6475 - val_accuracy: 0.7850\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.2816 - accuracy: 0.9375 - val_loss: 0.6532 - val_accuracy: 0.7800\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.2913 - accuracy: 0.9275 - val_loss: 0.6402 - val_accuracy: 0.7850\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.2811 - accuracy: 0.9275 - val_loss: 0.6444 - val_accuracy: 0.7800\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.2587 - accuracy: 0.9425 - val_loss: 0.6417 - val_accuracy: 0.7800\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.2806 - accuracy: 0.9325 - val_loss: 0.6437 - val_accuracy: 0.7800\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.2718 - accuracy: 0.9400 - val_loss: 0.6395 - val_accuracy: 0.7800\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.2808 - accuracy: 0.9262 - val_loss: 0.6392 - val_accuracy: 0.7850\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.2658 - accuracy: 0.9300 - val_loss: 0.6442 - val_accuracy: 0.7800\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.2752 - accuracy: 0.9250 - val_loss: 0.6375 - val_accuracy: 0.7850\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.2533 - accuracy: 0.9425 - val_loss: 0.6376 - val_accuracy: 0.7800\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.2563 - accuracy: 0.9400 - val_loss: 0.6393 - val_accuracy: 0.7750\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.2317 - accuracy: 0.9525 - val_loss: 0.6305 - val_accuracy: 0.7950\n",
            "fold 4\n",
            "Model: \"functional_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_3 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_3 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_3 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "inception_v3 (Functional)    (None, 8, 8, 2048)        21802784  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_3 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 21,823,274\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 1s 85ms/step - loss: 2.4271 - accuracy: 0.0850\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 2.3434 - accuracy: 0.1437 - val_loss: 2.1873 - val_accuracy: 0.1650\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 2.1168 - accuracy: 0.2375 - val_loss: 2.0242 - val_accuracy: 0.3100\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 1.9580 - accuracy: 0.3363 - val_loss: 1.8782 - val_accuracy: 0.4050\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 1.8211 - accuracy: 0.4187 - val_loss: 1.7514 - val_accuracy: 0.4950\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 1.6994 - accuracy: 0.4850 - val_loss: 1.6398 - val_accuracy: 0.5400\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 1.5547 - accuracy: 0.5525 - val_loss: 1.5394 - val_accuracy: 0.5900\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 1.4514 - accuracy: 0.6025 - val_loss: 1.4549 - val_accuracy: 0.6650\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 1.3834 - accuracy: 0.6112 - val_loss: 1.3882 - val_accuracy: 0.6850\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 1.3271 - accuracy: 0.6587 - val_loss: 1.3209 - val_accuracy: 0.6900\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 1.2259 - accuracy: 0.6875 - val_loss: 1.2642 - val_accuracy: 0.7100\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 1.1608 - accuracy: 0.7013 - val_loss: 1.2145 - val_accuracy: 0.7150\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 1.1069 - accuracy: 0.7362 - val_loss: 1.1702 - val_accuracy: 0.7250\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 1.0503 - accuracy: 0.7525 - val_loss: 1.1299 - val_accuracy: 0.7400\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 1.0143 - accuracy: 0.7462 - val_loss: 1.0981 - val_accuracy: 0.7350\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.9536 - accuracy: 0.7713 - val_loss: 1.0615 - val_accuracy: 0.7500\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.9482 - accuracy: 0.7675 - val_loss: 1.0353 - val_accuracy: 0.7550\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.9010 - accuracy: 0.7738 - val_loss: 1.0122 - val_accuracy: 0.7350\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.8536 - accuracy: 0.8100 - val_loss: 0.9840 - val_accuracy: 0.7650\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.8422 - accuracy: 0.8037 - val_loss: 0.9609 - val_accuracy: 0.7650\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.8122 - accuracy: 0.8062 - val_loss: 0.9452 - val_accuracy: 0.7650\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.7628 - accuracy: 0.8275 - val_loss: 0.9240 - val_accuracy: 0.7700\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.7690 - accuracy: 0.8025 - val_loss: 0.9074 - val_accuracy: 0.7750\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.7775 - accuracy: 0.8025 - val_loss: 0.8919 - val_accuracy: 0.7700\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.7369 - accuracy: 0.8112 - val_loss: 0.8751 - val_accuracy: 0.7900\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.7286 - accuracy: 0.8112 - val_loss: 0.8645 - val_accuracy: 0.7850\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.7213 - accuracy: 0.8250 - val_loss: 0.8509 - val_accuracy: 0.7800\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.6655 - accuracy: 0.8450 - val_loss: 0.8365 - val_accuracy: 0.7900\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.6665 - accuracy: 0.8288 - val_loss: 0.8289 - val_accuracy: 0.7800\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.6495 - accuracy: 0.8413 - val_loss: 0.8161 - val_accuracy: 0.7850\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.6229 - accuracy: 0.8487 - val_loss: 0.8134 - val_accuracy: 0.7800\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.6301 - accuracy: 0.8475 - val_loss: 0.7989 - val_accuracy: 0.7950\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.6061 - accuracy: 0.8537 - val_loss: 0.7916 - val_accuracy: 0.7850\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.5771 - accuracy: 0.8662 - val_loss: 0.7825 - val_accuracy: 0.7950\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.5736 - accuracy: 0.8562 - val_loss: 0.7730 - val_accuracy: 0.7950\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.5674 - accuracy: 0.8575 - val_loss: 0.7700 - val_accuracy: 0.8000\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.5843 - accuracy: 0.8525 - val_loss: 0.7664 - val_accuracy: 0.7850\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.5535 - accuracy: 0.8612 - val_loss: 0.7573 - val_accuracy: 0.7900\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.5429 - accuracy: 0.8662 - val_loss: 0.7523 - val_accuracy: 0.7900\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.5284 - accuracy: 0.8775 - val_loss: 0.7434 - val_accuracy: 0.7850\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.5019 - accuracy: 0.8838 - val_loss: 0.7399 - val_accuracy: 0.8100\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.5159 - accuracy: 0.8775 - val_loss: 0.7358 - val_accuracy: 0.7850\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.5218 - accuracy: 0.8625 - val_loss: 0.7315 - val_accuracy: 0.7950\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.5032 - accuracy: 0.8800 - val_loss: 0.7240 - val_accuracy: 0.8100\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.4929 - accuracy: 0.8863 - val_loss: 0.7250 - val_accuracy: 0.7950\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.4916 - accuracy: 0.8788 - val_loss: 0.7205 - val_accuracy: 0.8000\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.4784 - accuracy: 0.8700 - val_loss: 0.7162 - val_accuracy: 0.8050\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.4653 - accuracy: 0.8800 - val_loss: 0.7129 - val_accuracy: 0.8050\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.4617 - accuracy: 0.8875 - val_loss: 0.7121 - val_accuracy: 0.7850\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4783 - accuracy: 0.8737 - val_loss: 0.7070 - val_accuracy: 0.7950\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.4231 - accuracy: 0.9000 - val_loss: 0.7062 - val_accuracy: 0.8000\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.4377 - accuracy: 0.8975 - val_loss: 0.7007 - val_accuracy: 0.8100\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.4497 - accuracy: 0.8838 - val_loss: 0.6943 - val_accuracy: 0.8050\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.4394 - accuracy: 0.9038 - val_loss: 0.6945 - val_accuracy: 0.8000\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.4215 - accuracy: 0.8963 - val_loss: 0.6951 - val_accuracy: 0.7900\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.4033 - accuracy: 0.9038 - val_loss: 0.6856 - val_accuracy: 0.8000\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4124 - accuracy: 0.9050 - val_loss: 0.6883 - val_accuracy: 0.7950\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4108 - accuracy: 0.8975 - val_loss: 0.6812 - val_accuracy: 0.8100\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.4090 - accuracy: 0.9062 - val_loss: 0.6783 - val_accuracy: 0.8050\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4044 - accuracy: 0.8938 - val_loss: 0.6785 - val_accuracy: 0.8000\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4108 - accuracy: 0.9013 - val_loss: 0.6710 - val_accuracy: 0.8050\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3937 - accuracy: 0.9013 - val_loss: 0.6750 - val_accuracy: 0.7850\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.3870 - accuracy: 0.9038 - val_loss: 0.6731 - val_accuracy: 0.7900\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3734 - accuracy: 0.9075 - val_loss: 0.6716 - val_accuracy: 0.7900\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3853 - accuracy: 0.9100 - val_loss: 0.6716 - val_accuracy: 0.7950\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3581 - accuracy: 0.9100 - val_loss: 0.6662 - val_accuracy: 0.8100\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3655 - accuracy: 0.9087 - val_loss: 0.6654 - val_accuracy: 0.8000\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3639 - accuracy: 0.9162 - val_loss: 0.6662 - val_accuracy: 0.7850\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.3620 - accuracy: 0.9137 - val_loss: 0.6623 - val_accuracy: 0.8050\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3447 - accuracy: 0.9137 - val_loss: 0.6613 - val_accuracy: 0.7950\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.3473 - accuracy: 0.9087 - val_loss: 0.6585 - val_accuracy: 0.8000\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3567 - accuracy: 0.9100 - val_loss: 0.6649 - val_accuracy: 0.8000\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.3361 - accuracy: 0.9287 - val_loss: 0.6540 - val_accuracy: 0.8050\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3477 - accuracy: 0.9162 - val_loss: 0.6507 - val_accuracy: 0.8150\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3331 - accuracy: 0.9237 - val_loss: 0.6610 - val_accuracy: 0.7900\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3294 - accuracy: 0.9225 - val_loss: 0.6538 - val_accuracy: 0.7850\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3281 - accuracy: 0.9200 - val_loss: 0.6498 - val_accuracy: 0.8050\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3249 - accuracy: 0.9137 - val_loss: 0.6500 - val_accuracy: 0.7950\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3302 - accuracy: 0.9200 - val_loss: 0.6490 - val_accuracy: 0.8000\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3438 - accuracy: 0.9150 - val_loss: 0.6489 - val_accuracy: 0.7900\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3108 - accuracy: 0.9312 - val_loss: 0.6496 - val_accuracy: 0.7900\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3226 - accuracy: 0.9287 - val_loss: 0.6429 - val_accuracy: 0.7950\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.2908 - accuracy: 0.9287 - val_loss: 0.6419 - val_accuracy: 0.7950\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.2868 - accuracy: 0.9350 - val_loss: 0.6405 - val_accuracy: 0.7950\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3071 - accuracy: 0.9200 - val_loss: 0.6416 - val_accuracy: 0.7950\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3124 - accuracy: 0.9237 - val_loss: 0.6390 - val_accuracy: 0.8050\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.2997 - accuracy: 0.9200 - val_loss: 0.6417 - val_accuracy: 0.7950\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.2908 - accuracy: 0.9300 - val_loss: 0.6438 - val_accuracy: 0.7850\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.2870 - accuracy: 0.9312 - val_loss: 0.6455 - val_accuracy: 0.7950\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3059 - accuracy: 0.9150 - val_loss: 0.6397 - val_accuracy: 0.8000\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.2840 - accuracy: 0.9337 - val_loss: 0.6379 - val_accuracy: 0.7950\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.2779 - accuracy: 0.9362 - val_loss: 0.6387 - val_accuracy: 0.7800\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.2770 - accuracy: 0.9325 - val_loss: 0.6421 - val_accuracy: 0.8000\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 6s 127ms/step - loss: 0.3025 - accuracy: 0.9225 - val_loss: 0.6354 - val_accuracy: 0.7800\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.2922 - accuracy: 0.9262 - val_loss: 0.6397 - val_accuracy: 0.7950\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.2724 - accuracy: 0.9388 - val_loss: 0.6376 - val_accuracy: 0.8050\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.2681 - accuracy: 0.9438 - val_loss: 0.6370 - val_accuracy: 0.7800\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.2520 - accuracy: 0.9450 - val_loss: 0.6356 - val_accuracy: 0.7900\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.2634 - accuracy: 0.9375 - val_loss: 0.6329 - val_accuracy: 0.7950\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.2742 - accuracy: 0.9175 - val_loss: 0.6310 - val_accuracy: 0.8050\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.2509 - accuracy: 0.9463 - val_loss: 0.6314 - val_accuracy: 0.7850\n",
            "fold 5\n",
            "Model: \"functional_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_4 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_4 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_4 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "inception_v3 (Functional)    (None, 8, 8, 2048)        21802784  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_4 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 21,823,274\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 1s 84ms/step - loss: 2.4583 - accuracy: 0.0900\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 2.3884 - accuracy: 0.1238 - val_loss: 2.2534 - val_accuracy: 0.1200\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 2.1689 - accuracy: 0.1825 - val_loss: 2.0874 - val_accuracy: 0.2100\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 2.0209 - accuracy: 0.2825 - val_loss: 1.9389 - val_accuracy: 0.3250\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 1.8552 - accuracy: 0.3800 - val_loss: 1.8000 - val_accuracy: 0.4300\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 1.7093 - accuracy: 0.4837 - val_loss: 1.6870 - val_accuracy: 0.4850\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 1.6052 - accuracy: 0.5213 - val_loss: 1.5776 - val_accuracy: 0.5750\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 1.4993 - accuracy: 0.5700 - val_loss: 1.4854 - val_accuracy: 0.5650\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 1.3955 - accuracy: 0.6275 - val_loss: 1.4033 - val_accuracy: 0.6450\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 1.3173 - accuracy: 0.6587 - val_loss: 1.3303 - val_accuracy: 0.6350\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 1.2393 - accuracy: 0.6800 - val_loss: 1.2684 - val_accuracy: 0.6700\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 1.1869 - accuracy: 0.7075 - val_loss: 1.2137 - val_accuracy: 0.6800\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 1.1232 - accuracy: 0.7150 - val_loss: 1.1678 - val_accuracy: 0.7100\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 1.0670 - accuracy: 0.7225 - val_loss: 1.1198 - val_accuracy: 0.7350\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 1.0260 - accuracy: 0.7437 - val_loss: 1.0807 - val_accuracy: 0.7300\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 1.0052 - accuracy: 0.7513 - val_loss: 1.0459 - val_accuracy: 0.7500\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.9525 - accuracy: 0.7800 - val_loss: 1.0053 - val_accuracy: 0.7650\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.9335 - accuracy: 0.7613 - val_loss: 0.9789 - val_accuracy: 0.7800\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.8669 - accuracy: 0.8037 - val_loss: 0.9469 - val_accuracy: 0.7800\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.8512 - accuracy: 0.7850 - val_loss: 0.9241 - val_accuracy: 0.7800\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.8243 - accuracy: 0.7962 - val_loss: 0.9024 - val_accuracy: 0.7800\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.7993 - accuracy: 0.8100 - val_loss: 0.8779 - val_accuracy: 0.7950\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.7875 - accuracy: 0.8150 - val_loss: 0.8570 - val_accuracy: 0.8050\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.7782 - accuracy: 0.8125 - val_loss: 0.8394 - val_accuracy: 0.7850\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.7415 - accuracy: 0.8112 - val_loss: 0.8231 - val_accuracy: 0.8000\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.7321 - accuracy: 0.8037 - val_loss: 0.8110 - val_accuracy: 0.7950\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.7224 - accuracy: 0.8125 - val_loss: 0.7949 - val_accuracy: 0.8100\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.6807 - accuracy: 0.8350 - val_loss: 0.7791 - val_accuracy: 0.8050\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.6851 - accuracy: 0.8288 - val_loss: 0.7634 - val_accuracy: 0.8150\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.6583 - accuracy: 0.8487 - val_loss: 0.7513 - val_accuracy: 0.8200\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.6415 - accuracy: 0.8413 - val_loss: 0.7416 - val_accuracy: 0.8150\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.6265 - accuracy: 0.8525 - val_loss: 0.7305 - val_accuracy: 0.8150\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.6243 - accuracy: 0.8562 - val_loss: 0.7211 - val_accuracy: 0.8200\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.6106 - accuracy: 0.8400 - val_loss: 0.7096 - val_accuracy: 0.8150\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.6109 - accuracy: 0.8425 - val_loss: 0.7051 - val_accuracy: 0.8050\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.5834 - accuracy: 0.8750 - val_loss: 0.6894 - val_accuracy: 0.8200\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.5860 - accuracy: 0.8413 - val_loss: 0.6827 - val_accuracy: 0.8200\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.5749 - accuracy: 0.8587 - val_loss: 0.6754 - val_accuracy: 0.8250\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.5726 - accuracy: 0.8525 - val_loss: 0.6658 - val_accuracy: 0.8150\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.5629 - accuracy: 0.8575 - val_loss: 0.6554 - val_accuracy: 0.8150\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.5340 - accuracy: 0.8750 - val_loss: 0.6532 - val_accuracy: 0.8200\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.5388 - accuracy: 0.8600 - val_loss: 0.6447 - val_accuracy: 0.8350\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.5339 - accuracy: 0.8637 - val_loss: 0.6417 - val_accuracy: 0.8350\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.5164 - accuracy: 0.8662 - val_loss: 0.6366 - val_accuracy: 0.8300\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.5106 - accuracy: 0.8737 - val_loss: 0.6299 - val_accuracy: 0.8300\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4782 - accuracy: 0.8725 - val_loss: 0.6282 - val_accuracy: 0.8300\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4799 - accuracy: 0.8788 - val_loss: 0.6222 - val_accuracy: 0.8250\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.4914 - accuracy: 0.8737 - val_loss: 0.6158 - val_accuracy: 0.8350\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4656 - accuracy: 0.8888 - val_loss: 0.6140 - val_accuracy: 0.8350\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4785 - accuracy: 0.8875 - val_loss: 0.6103 - val_accuracy: 0.8350\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.4790 - accuracy: 0.8662 - val_loss: 0.6077 - val_accuracy: 0.8400\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4590 - accuracy: 0.8925 - val_loss: 0.5978 - val_accuracy: 0.8400\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.4723 - accuracy: 0.8850 - val_loss: 0.5917 - val_accuracy: 0.8350\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4394 - accuracy: 0.8850 - val_loss: 0.5879 - val_accuracy: 0.8450\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.4252 - accuracy: 0.9013 - val_loss: 0.5869 - val_accuracy: 0.8450\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.4188 - accuracy: 0.9050 - val_loss: 0.5830 - val_accuracy: 0.8350\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.4116 - accuracy: 0.9013 - val_loss: 0.5738 - val_accuracy: 0.8400\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.4166 - accuracy: 0.8925 - val_loss: 0.5738 - val_accuracy: 0.8450\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.3969 - accuracy: 0.9100 - val_loss: 0.5718 - val_accuracy: 0.8350\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4140 - accuracy: 0.8863 - val_loss: 0.5718 - val_accuracy: 0.8400\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3989 - accuracy: 0.8950 - val_loss: 0.5669 - val_accuracy: 0.8400\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.4069 - accuracy: 0.9038 - val_loss: 0.5605 - val_accuracy: 0.8450\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.4068 - accuracy: 0.9025 - val_loss: 0.5607 - val_accuracy: 0.8450\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.4014 - accuracy: 0.8988 - val_loss: 0.5602 - val_accuracy: 0.8400\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3852 - accuracy: 0.9062 - val_loss: 0.5577 - val_accuracy: 0.8450\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.3737 - accuracy: 0.9125 - val_loss: 0.5539 - val_accuracy: 0.8500\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.3587 - accuracy: 0.9150 - val_loss: 0.5501 - val_accuracy: 0.8450\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3521 - accuracy: 0.9137 - val_loss: 0.5460 - val_accuracy: 0.8450\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.3625 - accuracy: 0.9212 - val_loss: 0.5480 - val_accuracy: 0.8400\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3631 - accuracy: 0.9225 - val_loss: 0.5470 - val_accuracy: 0.8500\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3557 - accuracy: 0.9225 - val_loss: 0.5435 - val_accuracy: 0.8450\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3632 - accuracy: 0.9162 - val_loss: 0.5402 - val_accuracy: 0.8550\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3479 - accuracy: 0.9225 - val_loss: 0.5351 - val_accuracy: 0.8500\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.3424 - accuracy: 0.9200 - val_loss: 0.5351 - val_accuracy: 0.8500\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3652 - accuracy: 0.8925 - val_loss: 0.5356 - val_accuracy: 0.8450\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.3165 - accuracy: 0.9337 - val_loss: 0.5313 - val_accuracy: 0.8350\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.3254 - accuracy: 0.9262 - val_loss: 0.5333 - val_accuracy: 0.8500\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3277 - accuracy: 0.9237 - val_loss: 0.5296 - val_accuracy: 0.8450\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3344 - accuracy: 0.9225 - val_loss: 0.5230 - val_accuracy: 0.8500\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3343 - accuracy: 0.9187 - val_loss: 0.5308 - val_accuracy: 0.8400\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.3239 - accuracy: 0.9275 - val_loss: 0.5253 - val_accuracy: 0.8400\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.3614 - accuracy: 0.9087 - val_loss: 0.5229 - val_accuracy: 0.8400\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.3062 - accuracy: 0.9287 - val_loss: 0.5205 - val_accuracy: 0.8500\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.3202 - accuracy: 0.9287 - val_loss: 0.5215 - val_accuracy: 0.8450\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.3118 - accuracy: 0.9187 - val_loss: 0.5157 - val_accuracy: 0.8500\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.3033 - accuracy: 0.9275 - val_loss: 0.5098 - val_accuracy: 0.8500\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3003 - accuracy: 0.9162 - val_loss: 0.5130 - val_accuracy: 0.8450\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.3088 - accuracy: 0.9200 - val_loss: 0.5129 - val_accuracy: 0.8600\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.2939 - accuracy: 0.9362 - val_loss: 0.5095 - val_accuracy: 0.8600\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3206 - accuracy: 0.9175 - val_loss: 0.5094 - val_accuracy: 0.8550\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.2850 - accuracy: 0.9375 - val_loss: 0.5059 - val_accuracy: 0.8550\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.2851 - accuracy: 0.9325 - val_loss: 0.5055 - val_accuracy: 0.8500\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.3033 - accuracy: 0.9162 - val_loss: 0.5023 - val_accuracy: 0.8550\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.2760 - accuracy: 0.9375 - val_loss: 0.5050 - val_accuracy: 0.8500\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.2802 - accuracy: 0.9337 - val_loss: 0.5055 - val_accuracy: 0.8550\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.2867 - accuracy: 0.9275 - val_loss: 0.5046 - val_accuracy: 0.8500\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.2854 - accuracy: 0.9375 - val_loss: 0.5010 - val_accuracy: 0.8450\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.2871 - accuracy: 0.9287 - val_loss: 0.4973 - val_accuracy: 0.8700\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.2859 - accuracy: 0.9212 - val_loss: 0.4977 - val_accuracy: 0.8550\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.2613 - accuracy: 0.9463 - val_loss: 0.5014 - val_accuracy: 0.8550\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.2574 - accuracy: 0.9425 - val_loss: 0.4991 - val_accuracy: 0.8500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2E72C3O30fv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b47668c6-35fb-483f-8986-1899db0ec51e"
      },
      "source": [
        "# cross-validated accuracy for pre-trained model before training\n",
        "print(\"Base model accuracy:\", np.mean(base_model_acc_list))\n",
        "# cross-validated accuracy after training\n",
        "print(\"Final accuracy:\", np.mean(final_acc_list))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Base model accuracy: 0.1\n",
            "Final accuracy: 0.8300000071525574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dtn-03T_ZaRX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "dfece636-f4da-47ad-85ee-67419f3d468c"
      },
      "source": [
        "# plot graph of cross-validated accuracies\n",
        "acc = np.mean(history_map['accuracy'], axis=0)\n",
        "val_acc = np.mean(history_map['val_accuracy'], axis=0)\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftCYvBkKZmGa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "ef3cb29b-5c53-417c-ccd5-5218790aa8be"
      },
      "source": [
        "# plot graph of cross-validated losses\n",
        "loss = np.mean(history_map['loss'], axis=0)\n",
        "val_loss = np.mean(history_map['val_loss'], axis=0)\n",
        "\n",
        "plt.figure(2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "#plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}