{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "materials-resnet-balanced-cross-validated.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMoDCsR1tjC0mQoqsWiQB6U"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERKJLyg2z-Uq"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fIbsvD01EQm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3df3f11-c12e-4c63-b550-9768443c9c85"
      },
      "source": [
        "import pathlib\n",
        "\n",
        "# set path to FMD image directory\n",
        "data_dir = pathlib.Path(\"image\")\n",
        "\n",
        "# total no. of images in FMD dataset\n",
        "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
        "print(image_count)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL3rBqoe3sK5"
      },
      "source": [
        "# set batch size for training\n",
        "batch_size = 16\n",
        "\n",
        "# set dimensions to change images into for training\n",
        "# 299x299 images is used to train for consistency between different pre-trained models\n",
        "img_height = 299\n",
        "img_width = 299"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFKuhNRxqxcB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bb98fab-64d6-4767-8305-913b3d0bf602"
      },
      "source": [
        "# get class names from the folder names in data_dir\n",
        "class_names = np.array(sorted([item.name for item in data_dir.glob('*') if item.name != \"LICENSE.txt\"]))\n",
        "print(class_names)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['fabric' 'foliage' 'glass' 'leather' 'metal' 'paper' 'plastic' 'stone'\n",
            " 'water' 'wood']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap_FpvbEqWrS"
      },
      "source": [
        "# create list containing the dataset for each class\n",
        "ds_each_class = [tf.data.Dataset.list_files(str(data_dir/f'{class_name}/*.jpg'), shuffle=False) for class_name in class_names]\n",
        "\n",
        "# shuffle the 100 images in each class with the random seed value of 123 before training\n",
        "for index, ds in enumerate(ds_each_class):\n",
        "  ds_each_class[index] = ds.shuffle(image_count//10, seed=123, reshuffle_each_iteration=False)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty_LijJpqbEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59e301a1-1b63-4b98-c4f1-c7229ffa213a"
      },
      "source": [
        "# display some samples from a class to verify each class dataset contains only the class images\n",
        "for f in ds_each_class[0].take(10):\n",
        "  print(f.numpy())\n",
        "\n",
        "for f in ds_each_class[1].take(10):\n",
        "  print(f.numpy())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'FMD/image/fabric/fabric_moderate_037_new.jpg'\n",
            "b'FMD/image/fabric/fabric_moderate_004_new.jpg'\n",
            "b'FMD/image/fabric/fabric_moderate_008_new.jpg'\n",
            "b'FMD/image/fabric/fabric_object_003_new.jpg'\n",
            "b'FMD/image/fabric/fabric_object_017_new.jpg'\n",
            "b'FMD/image/fabric/fabric_object_001_new.jpg'\n",
            "b'FMD/image/fabric/fabric_moderate_032_new.jpg'\n",
            "b'FMD/image/fabric/fabric_moderate_030_new.jpg'\n",
            "b'FMD/image/fabric/fabric_moderate_038_new.jpg'\n",
            "b'FMD/image/fabric/fabric_object_009_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_037_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_004_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_008_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_053_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_067_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_051_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_032_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_030_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_038_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_059_new.jpg'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACILObxurdXN"
      },
      "source": [
        "# split first class dataset into 5 equal sized partitions\n",
        "# then for remaining classes' datasets do the same and add to corresponding partition\n",
        "# for 5-fold cross validation\n",
        "A = ds_each_class[0].shard(num_shards=5, index=0)\n",
        "B = ds_each_class[0].shard(num_shards=5, index=1)\n",
        "C = ds_each_class[0].shard(num_shards=5, index=2)\n",
        "D = ds_each_class[0].shard(num_shards=5, index=3)\n",
        "E = ds_each_class[0].shard(num_shards=5, index=4)\n",
        "for i in range(1, 10):\n",
        "  A = A.concatenate(ds_each_class[i].shard(num_shards=5, index=0))\n",
        "  B = B.concatenate(ds_each_class[i].shard(num_shards=5, index=1))\n",
        "  C = C.concatenate(ds_each_class[i].shard(num_shards=5, index=2))\n",
        "  D = D.concatenate(ds_each_class[i].shard(num_shards=5, index=3))\n",
        "  E = E.concatenate(ds_each_class[i].shard(num_shards=5, index=4))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9oYdHkhrwdz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b69bc7b7-5f90-4ec4-92ca-f34fd2354cda"
      },
      "source": [
        "# check no. of samples in each partition is the same\n",
        "print(A.cardinality().numpy())\n",
        "print(B.cardinality().numpy())\n",
        "print(C.cardinality().numpy())\n",
        "print(D.cardinality().numpy())\n",
        "print(E.cardinality().numpy())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BdD3FMHtGRV"
      },
      "source": [
        "def get_label(file_path):\n",
        "  # convert the path to a list of path components\n",
        "  parts = tf.strings.split(file_path, os.path.sep)\n",
        "  # The second to last is the class-directory\n",
        "  one_hot = parts[-2] == class_names\n",
        "  # Integer encode the label\n",
        "  return tf.argmax(one_hot)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfWduaL4tKDV"
      },
      "source": [
        "def decode_img(img):\n",
        "  # convert the compressed string to a 3D uint8 tensor\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\n",
        "  # resize the image to the desired size\n",
        "  return tf.image.resize(img, [img_height, img_width])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bGGe0KltMTC"
      },
      "source": [
        "def process_path(file_path):\n",
        "  label = get_label(file_path)\n",
        "  # load the raw data from the file as a string\n",
        "  img = tf.io.read_file(file_path)\n",
        "  img = decode_img(img)\n",
        "  return img, label"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcyZLwRxt1dy"
      },
      "source": [
        "# prompt the tf.data runtime to tune the number of elements to prefetch dynamically at runtime\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkBqAQlHtblh"
      },
      "source": [
        "# use the path of image to load the image into each partition of the dataset\n",
        "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
        "A = A.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "B = B.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "C = C.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "D = D.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "E = E.map(process_path, num_parallel_calls=AUTOTUNE)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc9pmaQ5t9H5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95bbab39-c457-4dbe-b672-4315dce28d9e"
      },
      "source": [
        "for image, label in A.take(1):\n",
        "  print(\"Image shape: \", image.numpy().shape)\n",
        "  print(\"Label: \", label.numpy())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image shape:  (299, 299, 3)\n",
            "Label:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_T2KNdGub1X"
      },
      "source": [
        "# shuffle, batch, and prefetch the dataset\n",
        "def configure_for_performance(ds):\n",
        "  ds = ds.cache()\n",
        "  ds = ds.shuffle(buffer_size=1000)\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
        "  return ds"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWcojoVRuok5"
      },
      "source": [
        "# map the dataset partitions to integers for building training/test sets during cross-validation\n",
        "ds_fold_dict = {0:A, 1:B, 2:C, 3:D, 4:E}"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx3xKoMZU9Yv"
      },
      "source": [
        "# normalise the input values to the pre-trained model's required range of values\n",
        "preprocess_input = keras.applications.resnet_v2.preprocess_input"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVOP5fIPwEx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0538b039-0445-4a12-843b-d17556dc7bda"
      },
      "source": [
        "# get pre-trained model\n",
        "base_model = keras.applications.ResNet50V2(include_top=False, input_shape=(img_height, img_width, 3))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94674944/94668760 [==============================] - 23s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS2kAceGVW0e"
      },
      "source": [
        "# don't train base model weights\n",
        "base_model.trainable = False"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGkReMX60ScJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1005ee1b-e526-4a46-be3a-fd4c5346eaf6"
      },
      "source": [
        "base_model.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"resnet50v2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 299, 299, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 305, 305, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 150, 150, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 152, 152, 64) 0           conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 75, 75, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_preact_bn (BatchNo (None, 75, 75, 64)   256         pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_preact_relu (Activ (None, 75, 75, 64)   0           conv2_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 75, 75, 64)   4096        conv2_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 75, 75, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_pad (ZeroPadding (None, 77, 77, 64)   0           conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 75, 75, 64)   36864       conv2_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 75, 75, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 75, 75, 256)  16640       conv2_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 75, 75, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Add)          (None, 75, 75, 256)  0           conv2_block1_0_conv[0][0]        \n",
            "                                                                 conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_preact_bn (BatchNo (None, 75, 75, 256)  1024        conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_preact_relu (Activ (None, 75, 75, 256)  0           conv2_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 75, 75, 64)   16384       conv2_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 75, 75, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_pad (ZeroPadding (None, 77, 77, 64)   0           conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 75, 75, 64)   36864       conv2_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 75, 75, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 75, 75, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Add)          (None, 75, 75, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_preact_bn (BatchNo (None, 75, 75, 256)  1024        conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_preact_relu (Activ (None, 75, 75, 256)  0           conv2_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 75, 75, 64)   16384       conv2_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 75, 75, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_pad (ZeroPadding (None, 77, 77, 64)   0           conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 38, 38, 64)   36864       conv2_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 38, 38, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 38, 38, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 38, 38, 256)  0           conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 38, 38, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Add)          (None, 38, 38, 256)  0           max_pooling2d[0][0]              \n",
            "                                                                 conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_preact_bn (BatchNo (None, 38, 38, 256)  1024        conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_preact_relu (Activ (None, 38, 38, 256)  0           conv3_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 38, 38, 128)  32768       conv3_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 38, 38, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_pad (ZeroPadding (None, 40, 40, 128)  0           conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 38, 38, 128)  147456      conv3_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 38, 38, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 38, 38, 512)  131584      conv3_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Add)          (None, 38, 38, 512)  0           conv3_block1_0_conv[0][0]        \n",
            "                                                                 conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_preact_bn (BatchNo (None, 38, 38, 512)  2048        conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_preact_relu (Activ (None, 38, 38, 512)  0           conv3_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 38, 38, 128)  65536       conv3_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 38, 38, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_pad (ZeroPadding (None, 40, 40, 128)  0           conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 38, 38, 128)  147456      conv3_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 38, 38, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Add)          (None, 38, 38, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_preact_bn (BatchNo (None, 38, 38, 512)  2048        conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_preact_relu (Activ (None, 38, 38, 512)  0           conv3_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 38, 38, 128)  65536       conv3_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 38, 38, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_pad (ZeroPadding (None, 40, 40, 128)  0           conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 38, 38, 128)  147456      conv3_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 38, 38, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Add)          (None, 38, 38, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_preact_bn (BatchNo (None, 38, 38, 512)  2048        conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_preact_relu (Activ (None, 38, 38, 512)  0           conv3_block4_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 38, 38, 128)  65536       conv3_block4_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 38, 38, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_pad (ZeroPadding (None, 40, 40, 128)  0           conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 19, 19, 128)  147456      conv3_block4_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 19, 19, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 19, 19, 512)  0           conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 19, 19, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Add)          (None, 19, 19, 512)  0           max_pooling2d_1[0][0]            \n",
            "                                                                 conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_preact_bn (BatchNo (None, 19, 19, 512)  2048        conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_preact_relu (Activ (None, 19, 19, 512)  0           conv4_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 19, 19, 256)  131072      conv4_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 19, 19, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 19, 19, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 19, 19, 1024) 525312      conv4_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Add)          (None, 19, 19, 1024) 0           conv4_block1_0_conv[0][0]        \n",
            "                                                                 conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 19, 19, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 19, 19, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Add)          (None, 19, 19, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 19, 19, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 19, 19, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Add)          (None, 19, 19, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block4_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block4_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 19, 19, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block4_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 19, 19, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Add)          (None, 19, 19, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block5_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block5_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 19, 19, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block5_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 19, 19, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Add)          (None, 19, 19, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block6_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block6_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 19, 19, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 10, 10, 256)  589824      conv4_block6_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 10, 10, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 10, 10, 1024) 0           conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 10, 10, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Add)          (None, 10, 10, 1024) 0           max_pooling2d_2[0][0]            \n",
            "                                                                 conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_preact_bn (BatchNo (None, 10, 10, 1024) 4096        conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_preact_relu (Activ (None, 10, 10, 1024) 0           conv5_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 10, 10, 512)  524288      conv5_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 10, 10, 512)  0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_pad (ZeroPadding (None, 12, 12, 512)  0           conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 10, 10, 512)  2359296     conv5_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 10, 10, 512)  0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 10, 10, 2048) 2099200     conv5_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 10, 10, 2048) 1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Add)          (None, 10, 10, 2048) 0           conv5_block1_0_conv[0][0]        \n",
            "                                                                 conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_preact_bn (BatchNo (None, 10, 10, 2048) 8192        conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_preact_relu (Activ (None, 10, 10, 2048) 0           conv5_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 10, 10, 512)  1048576     conv5_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 10, 10, 512)  0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_pad (ZeroPadding (None, 12, 12, 512)  0           conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 10, 10, 512)  2359296     conv5_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 10, 10, 512)  0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 10, 10, 2048) 1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Add)          (None, 10, 10, 2048) 0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_preact_bn (BatchNo (None, 10, 10, 2048) 8192        conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_preact_relu (Activ (None, 10, 10, 2048) 0           conv5_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 10, 10, 512)  1048576     conv5_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 10, 10, 512)  0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_pad (ZeroPadding (None, 12, 12, 512)  0           conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 10, 10, 512)  2359296     conv5_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 10, 10, 512)  0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 10, 10, 2048) 1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Add)          (None, 10, 10, 2048) 0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "post_bn (BatchNormalization)    (None, 10, 10, 2048) 8192        conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "post_relu (Activation)          (None, 10, 10, 2048) 0           post_bn[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 23,564,800\n",
            "Trainable params: 0\n",
            "Non-trainable params: 23,564,800\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzhaWb2aX2if"
      },
      "source": [
        "# set a low learning rate to avoid overfitting too quickly\n",
        "base_learning_rate = 0.0001\n",
        "\n",
        "# create the model to train using the pre-trained model as base model\n",
        "def create_model():\n",
        "  # generate additional training data from input training data by augmenting them using random flip, rotation & zoom\n",
        "  data_augmentation = keras.Sequential(\n",
        "    [\n",
        "      layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n",
        "                                                  input_shape=(img_height, \n",
        "                                                                img_width,\n",
        "                                                                3)),\n",
        "      layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "      layers.experimental.preprocessing.RandomZoom(0.1),\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  # average over the spatial locations to convert the features to a single vector per image\n",
        "  global_average_layer = keras.layers.GlobalAveragePooling2D()\n",
        "  # convert these features into a single prediction per image\n",
        "  prediction_layer = keras.layers.Dense(10)\n",
        "\n",
        "  # Build a model by chaining together the layers using the Keras Functional API.\n",
        "  inputs = keras.Input(shape=(img_height, img_width, 3))\n",
        "  x = data_augmentation(inputs)\n",
        "  x = preprocess_input(x)\n",
        "  x = base_model(x, training=False) # use training=False as the base model contains a BatchNormalization layer\n",
        "  x = global_average_layer(x)\n",
        "  x = keras.layers.Dropout(0.2)(x) # add dropout to fully connected layer to reduce overfitting\n",
        "  outputs = prediction_layer(x)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  optimizer = keras.optimizers.Adam(lr=base_learning_rate)\n",
        "  loss= keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  # compile model with Adam optimizer with specified learning rate\n",
        "  model.compile(optimizer=optimizer,\n",
        "                loss=loss,\n",
        "                metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-5TEZRgY2l_"
      },
      "source": [
        "no_epochs = 100"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya698zRbu7Ep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52f359cd-763f-49bf-8623-bb8fe0e97cf2"
      },
      "source": [
        "# store results to plot graphs and get cross-validated accuracies\n",
        "history_map = {}\n",
        "base_model_acc_list = []\n",
        "final_acc_list = []\n",
        "\n",
        "# do 5-fold cross-validation\n",
        "for i in range(5):\n",
        "  print('fold', i + 1)\n",
        "  temp_dict = ds_fold_dict.copy()\n",
        "  # get test set for this iteration\n",
        "  current_val_ds = temp_dict[i]\n",
        "\n",
        "  # get training set for this iteration from remaining data samples\n",
        "  del temp_dict[i]\n",
        "  current_train_ds = None\n",
        "  for ds_shard in temp_dict.values():\n",
        "    if current_train_ds is None:\n",
        "      current_train_ds = ds_shard\n",
        "    else:\n",
        "      current_train_ds = current_train_ds.concatenate(ds_shard)\n",
        "  \n",
        "  # configure both training and test sets to improve performance\n",
        "  current_train_ds = configure_for_performance(current_train_ds)\n",
        "  current_val_ds = configure_for_performance(current_val_ds)\n",
        "\n",
        "  # create a new model\n",
        "  model = create_model()\n",
        "  # get initial test accuracy\n",
        "  base_model_acc_list.append(model.evaluate(current_val_ds)[1])\n",
        "  # train for specified epochs\n",
        "  history = model.fit(current_train_ds,\n",
        "                    epochs=no_epochs,\n",
        "                    validation_data=current_val_ds)\n",
        "  \n",
        "  # save results\n",
        "  if i == 0:\n",
        "    history_map['accuracy'] = [history.history['accuracy']]\n",
        "    history_map['val_accuracy'] = [history.history['val_accuracy']]\n",
        "    history_map['loss'] = [history.history['loss']]\n",
        "    history_map['val_loss'] = [history.history['val_loss']]\n",
        "  else:\n",
        "    history_map['accuracy'].append(history.history['accuracy'])\n",
        "    history_map['val_accuracy'].append(history.history['val_accuracy'])\n",
        "    history_map['loss'].append(history.history['loss'])\n",
        "    history_map['val_loss'].append(history.history['val_loss'])\n",
        "  \n",
        "  # get final test accuracy by taking the max accuracy over the whole training\n",
        "  final_acc_list.append(np.amax(history.history['val_accuracy']))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fold 1\n",
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential (Sequential)      (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv (TensorF [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub (TensorFlowO [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "resnet50v2 (Functional)      (None, 10, 10, 2048)      23564800  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 23,585,290\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 23,564,800\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 3s 231ms/step - loss: 2.6333 - accuracy: 0.1000\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 12s 245ms/step - loss: 2.6525 - accuracy: 0.1138 - val_loss: 2.2931 - val_accuracy: 0.1750\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 12s 244ms/step - loss: 2.3320 - accuracy: 0.1637 - val_loss: 2.0288 - val_accuracy: 0.2700\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 12s 245ms/step - loss: 2.0200 - accuracy: 0.2688 - val_loss: 1.7997 - val_accuracy: 0.3650\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 12s 246ms/step - loss: 1.8281 - accuracy: 0.3575 - val_loss: 1.6181 - val_accuracy: 0.4400\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 12s 247ms/step - loss: 1.6195 - accuracy: 0.4500 - val_loss: 1.4678 - val_accuracy: 0.5200\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 12s 248ms/step - loss: 1.4800 - accuracy: 0.5125 - val_loss: 1.3409 - val_accuracy: 0.5600\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 12s 249ms/step - loss: 1.3762 - accuracy: 0.5788 - val_loss: 1.2378 - val_accuracy: 0.6100\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 12s 249ms/step - loss: 1.2877 - accuracy: 0.6087 - val_loss: 1.1513 - val_accuracy: 0.6300\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 12s 250ms/step - loss: 1.1820 - accuracy: 0.6475 - val_loss: 1.0808 - val_accuracy: 0.6600\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 1.0992 - accuracy: 0.6762 - val_loss: 1.0240 - val_accuracy: 0.6650\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 1.0236 - accuracy: 0.7250 - val_loss: 0.9717 - val_accuracy: 0.6900\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.9763 - accuracy: 0.7275 - val_loss: 0.9255 - val_accuracy: 0.6950\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.9329 - accuracy: 0.7287 - val_loss: 0.8894 - val_accuracy: 0.7050\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.8908 - accuracy: 0.7525 - val_loss: 0.8552 - val_accuracy: 0.7200\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.8629 - accuracy: 0.7688 - val_loss: 0.8266 - val_accuracy: 0.7300\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.8337 - accuracy: 0.7887 - val_loss: 0.8026 - val_accuracy: 0.7350\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.7502 - accuracy: 0.7937 - val_loss: 0.7779 - val_accuracy: 0.7650\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.7810 - accuracy: 0.7775 - val_loss: 0.7592 - val_accuracy: 0.7600\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.7261 - accuracy: 0.8075 - val_loss: 0.7427 - val_accuracy: 0.7850\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.7119 - accuracy: 0.8050 - val_loss: 0.7241 - val_accuracy: 0.7800\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.6563 - accuracy: 0.8288 - val_loss: 0.7126 - val_accuracy: 0.7950\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.6578 - accuracy: 0.8250 - val_loss: 0.6992 - val_accuracy: 0.8000\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.6151 - accuracy: 0.8425 - val_loss: 0.6876 - val_accuracy: 0.7950\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.6198 - accuracy: 0.8388 - val_loss: 0.6726 - val_accuracy: 0.7950\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.6115 - accuracy: 0.8363 - val_loss: 0.6667 - val_accuracy: 0.8000\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.5726 - accuracy: 0.8338 - val_loss: 0.6570 - val_accuracy: 0.8000\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.5663 - accuracy: 0.8512 - val_loss: 0.6460 - val_accuracy: 0.8050\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.5429 - accuracy: 0.8537 - val_loss: 0.6424 - val_accuracy: 0.8050\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.5299 - accuracy: 0.8537 - val_loss: 0.6349 - val_accuracy: 0.8050\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.5137 - accuracy: 0.8600 - val_loss: 0.6302 - val_accuracy: 0.8050\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.5033 - accuracy: 0.8750 - val_loss: 0.6223 - val_accuracy: 0.8100\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.5088 - accuracy: 0.8562 - val_loss: 0.6161 - val_accuracy: 0.8150\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.4981 - accuracy: 0.8600 - val_loss: 0.6080 - val_accuracy: 0.8100\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.4723 - accuracy: 0.8675 - val_loss: 0.6054 - val_accuracy: 0.8150\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.4668 - accuracy: 0.8587 - val_loss: 0.5991 - val_accuracy: 0.8250\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.4513 - accuracy: 0.8725 - val_loss: 0.5959 - val_accuracy: 0.8200\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.4482 - accuracy: 0.8637 - val_loss: 0.5902 - val_accuracy: 0.8300\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.4458 - accuracy: 0.8687 - val_loss: 0.5888 - val_accuracy: 0.8250\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.4325 - accuracy: 0.8838 - val_loss: 0.5860 - val_accuracy: 0.8300\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.4163 - accuracy: 0.8900 - val_loss: 0.5808 - val_accuracy: 0.8250\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.4156 - accuracy: 0.8900 - val_loss: 0.5814 - val_accuracy: 0.8300\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.4049 - accuracy: 0.8825 - val_loss: 0.5760 - val_accuracy: 0.8250\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.3710 - accuracy: 0.9075 - val_loss: 0.5749 - val_accuracy: 0.8300\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.3790 - accuracy: 0.8950 - val_loss: 0.5686 - val_accuracy: 0.8300\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.3821 - accuracy: 0.8975 - val_loss: 0.5657 - val_accuracy: 0.8300\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.3829 - accuracy: 0.8888 - val_loss: 0.5697 - val_accuracy: 0.8150\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.3796 - accuracy: 0.9087 - val_loss: 0.5649 - val_accuracy: 0.8250\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.3572 - accuracy: 0.9150 - val_loss: 0.5586 - val_accuracy: 0.8250\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.3426 - accuracy: 0.9150 - val_loss: 0.5558 - val_accuracy: 0.8200\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.3229 - accuracy: 0.9187 - val_loss: 0.5543 - val_accuracy: 0.8250\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.3541 - accuracy: 0.8975 - val_loss: 0.5506 - val_accuracy: 0.8150\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.3181 - accuracy: 0.9325 - val_loss: 0.5476 - val_accuracy: 0.8200\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.3114 - accuracy: 0.9212 - val_loss: 0.5480 - val_accuracy: 0.8250\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.2984 - accuracy: 0.9250 - val_loss: 0.5486 - val_accuracy: 0.8150\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.3038 - accuracy: 0.9300 - val_loss: 0.5489 - val_accuracy: 0.8250\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.3109 - accuracy: 0.9225 - val_loss: 0.5461 - val_accuracy: 0.8250\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.3013 - accuracy: 0.9225 - val_loss: 0.5485 - val_accuracy: 0.8250\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 13s 250ms/step - loss: 0.3115 - accuracy: 0.9237 - val_loss: 0.5479 - val_accuracy: 0.8300\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.2791 - accuracy: 0.9388 - val_loss: 0.5454 - val_accuracy: 0.8250\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.3063 - accuracy: 0.9225 - val_loss: 0.5430 - val_accuracy: 0.8250\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.2950 - accuracy: 0.9237 - val_loss: 0.5415 - val_accuracy: 0.8200\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.3029 - accuracy: 0.9275 - val_loss: 0.5431 - val_accuracy: 0.8200\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.2865 - accuracy: 0.9275 - val_loss: 0.5415 - val_accuracy: 0.8250\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.2667 - accuracy: 0.9362 - val_loss: 0.5368 - val_accuracy: 0.8250\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2661 - accuracy: 0.9337 - val_loss: 0.5370 - val_accuracy: 0.8250\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2570 - accuracy: 0.9425 - val_loss: 0.5300 - val_accuracy: 0.8250\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.2607 - accuracy: 0.9400 - val_loss: 0.5306 - val_accuracy: 0.8250\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.2614 - accuracy: 0.9350 - val_loss: 0.5319 - val_accuracy: 0.8200\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2514 - accuracy: 0.9438 - val_loss: 0.5310 - val_accuracy: 0.8250\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.2450 - accuracy: 0.9388 - val_loss: 0.5332 - val_accuracy: 0.8300\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2642 - accuracy: 0.9312 - val_loss: 0.5327 - val_accuracy: 0.8300\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2556 - accuracy: 0.9400 - val_loss: 0.5297 - val_accuracy: 0.8250\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2411 - accuracy: 0.9400 - val_loss: 0.5353 - val_accuracy: 0.8300\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.2197 - accuracy: 0.9488 - val_loss: 0.5311 - val_accuracy: 0.8300\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2474 - accuracy: 0.9337 - val_loss: 0.5267 - val_accuracy: 0.8300\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.2136 - accuracy: 0.9463 - val_loss: 0.5305 - val_accuracy: 0.8250\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.2448 - accuracy: 0.9450 - val_loss: 0.5300 - val_accuracy: 0.8250\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2214 - accuracy: 0.9588 - val_loss: 0.5248 - val_accuracy: 0.8250\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.2207 - accuracy: 0.9488 - val_loss: 0.5282 - val_accuracy: 0.8250\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.2338 - accuracy: 0.9388 - val_loss: 0.5275 - val_accuracy: 0.8300\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2228 - accuracy: 0.9500 - val_loss: 0.5271 - val_accuracy: 0.8250\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2032 - accuracy: 0.9613 - val_loss: 0.5270 - val_accuracy: 0.8300\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2191 - accuracy: 0.9538 - val_loss: 0.5257 - val_accuracy: 0.8250\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2125 - accuracy: 0.9413 - val_loss: 0.5215 - val_accuracy: 0.8300\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.1935 - accuracy: 0.9650 - val_loss: 0.5252 - val_accuracy: 0.8300\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.1932 - accuracy: 0.9625 - val_loss: 0.5267 - val_accuracy: 0.8200\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.2046 - accuracy: 0.9625 - val_loss: 0.5274 - val_accuracy: 0.8150\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.1972 - accuracy: 0.9513 - val_loss: 0.5217 - val_accuracy: 0.8200\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.1813 - accuracy: 0.9712 - val_loss: 0.5226 - val_accuracy: 0.8250\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.1908 - accuracy: 0.9650 - val_loss: 0.5237 - val_accuracy: 0.8250\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2019 - accuracy: 0.9575 - val_loss: 0.5192 - val_accuracy: 0.8250\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.1904 - accuracy: 0.9563 - val_loss: 0.5237 - val_accuracy: 0.8250\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.1999 - accuracy: 0.9575 - val_loss: 0.5253 - val_accuracy: 0.8250\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.1794 - accuracy: 0.9600 - val_loss: 0.5269 - val_accuracy: 0.8250\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.1699 - accuracy: 0.9625 - val_loss: 0.5277 - val_accuracy: 0.8250\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.1793 - accuracy: 0.9563 - val_loss: 0.5287 - val_accuracy: 0.8250\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.1737 - accuracy: 0.9625 - val_loss: 0.5236 - val_accuracy: 0.8250\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.1796 - accuracy: 0.9638 - val_loss: 0.5213 - val_accuracy: 0.8300\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.1602 - accuracy: 0.9737 - val_loss: 0.5253 - val_accuracy: 0.8250\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.1808 - accuracy: 0.9650 - val_loss: 0.5248 - val_accuracy: 0.8250\n",
            "fold 2\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_1 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_1 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_1 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "resnet50v2 (Functional)      (None, 10, 10, 2048)      23564800  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 23,585,290\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 23,564,800\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 2s 177ms/step - loss: 2.6377 - accuracy: 0.1050\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 13s 250ms/step - loss: 2.5349 - accuracy: 0.1325 - val_loss: 2.3352 - val_accuracy: 0.1700\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 2.2127 - accuracy: 0.2113 - val_loss: 2.0927 - val_accuracy: 0.2950\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 1.9397 - accuracy: 0.3125 - val_loss: 1.8781 - val_accuracy: 0.3500\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 13s 250ms/step - loss: 1.8002 - accuracy: 0.3938 - val_loss: 1.6957 - val_accuracy: 0.4250\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 1.6044 - accuracy: 0.4575 - val_loss: 1.5512 - val_accuracy: 0.5350\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 1.4396 - accuracy: 0.5487 - val_loss: 1.4284 - val_accuracy: 0.5600\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 1.3579 - accuracy: 0.5750 - val_loss: 1.3239 - val_accuracy: 0.6050\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 1.2347 - accuracy: 0.6187 - val_loss: 1.2373 - val_accuracy: 0.6350\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 1.1505 - accuracy: 0.6475 - val_loss: 1.1686 - val_accuracy: 0.6550\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 1.0630 - accuracy: 0.6888 - val_loss: 1.1110 - val_accuracy: 0.6800\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 1.0200 - accuracy: 0.7088 - val_loss: 1.0620 - val_accuracy: 0.6900\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.9681 - accuracy: 0.7138 - val_loss: 1.0149 - val_accuracy: 0.7050\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.9193 - accuracy: 0.7337 - val_loss: 0.9761 - val_accuracy: 0.7150\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.8746 - accuracy: 0.7437 - val_loss: 0.9430 - val_accuracy: 0.7100\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.8111 - accuracy: 0.7625 - val_loss: 0.9179 - val_accuracy: 0.7150\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.8230 - accuracy: 0.7613 - val_loss: 0.8865 - val_accuracy: 0.7250\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.7837 - accuracy: 0.7650 - val_loss: 0.8617 - val_accuracy: 0.7300\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.7233 - accuracy: 0.7850 - val_loss: 0.8430 - val_accuracy: 0.7350\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.7313 - accuracy: 0.7850 - val_loss: 0.8269 - val_accuracy: 0.7400\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.6763 - accuracy: 0.8075 - val_loss: 0.8068 - val_accuracy: 0.7550\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.6623 - accuracy: 0.8087 - val_loss: 0.7944 - val_accuracy: 0.7750\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.6569 - accuracy: 0.8050 - val_loss: 0.7805 - val_accuracy: 0.7600\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.6520 - accuracy: 0.8125 - val_loss: 0.7688 - val_accuracy: 0.7600\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.6071 - accuracy: 0.8263 - val_loss: 0.7576 - val_accuracy: 0.7700\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.6008 - accuracy: 0.8388 - val_loss: 0.7424 - val_accuracy: 0.7900\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.6031 - accuracy: 0.8138 - val_loss: 0.7303 - val_accuracy: 0.7850\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.5311 - accuracy: 0.8562 - val_loss: 0.7229 - val_accuracy: 0.7900\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.5506 - accuracy: 0.8388 - val_loss: 0.7121 - val_accuracy: 0.7850\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.5102 - accuracy: 0.8700 - val_loss: 0.7036 - val_accuracy: 0.7850\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.5261 - accuracy: 0.8388 - val_loss: 0.6942 - val_accuracy: 0.7900\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.5268 - accuracy: 0.8550 - val_loss: 0.6867 - val_accuracy: 0.7850\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.4847 - accuracy: 0.8575 - val_loss: 0.6803 - val_accuracy: 0.7900\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.4796 - accuracy: 0.8700 - val_loss: 0.6732 - val_accuracy: 0.7950\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.4833 - accuracy: 0.8637 - val_loss: 0.6665 - val_accuracy: 0.7950\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.4623 - accuracy: 0.8725 - val_loss: 0.6593 - val_accuracy: 0.7950\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.4464 - accuracy: 0.8737 - val_loss: 0.6543 - val_accuracy: 0.7900\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.4425 - accuracy: 0.8813 - val_loss: 0.6510 - val_accuracy: 0.7950\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.4213 - accuracy: 0.8863 - val_loss: 0.6455 - val_accuracy: 0.7950\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.4099 - accuracy: 0.8875 - val_loss: 0.6443 - val_accuracy: 0.8000\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.3985 - accuracy: 0.8925 - val_loss: 0.6372 - val_accuracy: 0.7950\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.3895 - accuracy: 0.8925 - val_loss: 0.6346 - val_accuracy: 0.7900\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.3974 - accuracy: 0.8975 - val_loss: 0.6294 - val_accuracy: 0.8000\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.4104 - accuracy: 0.8838 - val_loss: 0.6231 - val_accuracy: 0.7950\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.3829 - accuracy: 0.9050 - val_loss: 0.6235 - val_accuracy: 0.8000\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.3700 - accuracy: 0.9062 - val_loss: 0.6176 - val_accuracy: 0.8000\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.3606 - accuracy: 0.9062 - val_loss: 0.6167 - val_accuracy: 0.7950\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.3710 - accuracy: 0.9038 - val_loss: 0.6124 - val_accuracy: 0.8000\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.3408 - accuracy: 0.9087 - val_loss: 0.6088 - val_accuracy: 0.8000\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.3493 - accuracy: 0.9237 - val_loss: 0.6052 - val_accuracy: 0.8050\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.3530 - accuracy: 0.8913 - val_loss: 0.6033 - val_accuracy: 0.8000\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.3365 - accuracy: 0.9075 - val_loss: 0.6030 - val_accuracy: 0.8050\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.3455 - accuracy: 0.9075 - val_loss: 0.5990 - val_accuracy: 0.8050\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.3322 - accuracy: 0.9150 - val_loss: 0.5974 - val_accuracy: 0.8050\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.3302 - accuracy: 0.9075 - val_loss: 0.5975 - val_accuracy: 0.8000\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.3033 - accuracy: 0.9262 - val_loss: 0.5931 - val_accuracy: 0.8050\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.3265 - accuracy: 0.9125 - val_loss: 0.5913 - val_accuracy: 0.8000\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2988 - accuracy: 0.9175 - val_loss: 0.5892 - val_accuracy: 0.8050\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2941 - accuracy: 0.9350 - val_loss: 0.5875 - val_accuracy: 0.8000\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2984 - accuracy: 0.9237 - val_loss: 0.5826 - val_accuracy: 0.8000\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2995 - accuracy: 0.9388 - val_loss: 0.5812 - val_accuracy: 0.8050\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.3001 - accuracy: 0.9187 - val_loss: 0.5793 - val_accuracy: 0.8000\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2767 - accuracy: 0.9362 - val_loss: 0.5786 - val_accuracy: 0.8000\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2796 - accuracy: 0.9287 - val_loss: 0.5800 - val_accuracy: 0.8000\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2714 - accuracy: 0.9325 - val_loss: 0.5775 - val_accuracy: 0.8000\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2624 - accuracy: 0.9362 - val_loss: 0.5762 - val_accuracy: 0.7950\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2740 - accuracy: 0.9350 - val_loss: 0.5742 - val_accuracy: 0.7900\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2556 - accuracy: 0.9400 - val_loss: 0.5738 - val_accuracy: 0.7900\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2561 - accuracy: 0.9450 - val_loss: 0.5745 - val_accuracy: 0.8000\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2662 - accuracy: 0.9362 - val_loss: 0.5719 - val_accuracy: 0.8050\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.2563 - accuracy: 0.9375 - val_loss: 0.5680 - val_accuracy: 0.7850\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2511 - accuracy: 0.9438 - val_loss: 0.5680 - val_accuracy: 0.8050\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2537 - accuracy: 0.9350 - val_loss: 0.5686 - val_accuracy: 0.7950\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2333 - accuracy: 0.9463 - val_loss: 0.5688 - val_accuracy: 0.7950\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.2565 - accuracy: 0.9362 - val_loss: 0.5665 - val_accuracy: 0.7900\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2561 - accuracy: 0.9287 - val_loss: 0.5648 - val_accuracy: 0.7950\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2407 - accuracy: 0.9438 - val_loss: 0.5608 - val_accuracy: 0.7850\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2233 - accuracy: 0.9438 - val_loss: 0.5608 - val_accuracy: 0.7950\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2303 - accuracy: 0.9450 - val_loss: 0.5619 - val_accuracy: 0.7950\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2133 - accuracy: 0.9550 - val_loss: 0.5623 - val_accuracy: 0.7900\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2197 - accuracy: 0.9475 - val_loss: 0.5613 - val_accuracy: 0.7950\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2258 - accuracy: 0.9488 - val_loss: 0.5606 - val_accuracy: 0.7950\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2091 - accuracy: 0.9600 - val_loss: 0.5575 - val_accuracy: 0.7950\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2006 - accuracy: 0.9575 - val_loss: 0.5593 - val_accuracy: 0.8000\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2154 - accuracy: 0.9488 - val_loss: 0.5567 - val_accuracy: 0.7950\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2191 - accuracy: 0.9362 - val_loss: 0.5575 - val_accuracy: 0.7950\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.1918 - accuracy: 0.9575 - val_loss: 0.5577 - val_accuracy: 0.7900\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2036 - accuracy: 0.9538 - val_loss: 0.5578 - val_accuracy: 0.7900\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2003 - accuracy: 0.9550 - val_loss: 0.5609 - val_accuracy: 0.7950\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.2054 - accuracy: 0.9563 - val_loss: 0.5580 - val_accuracy: 0.7950\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.1981 - accuracy: 0.9600 - val_loss: 0.5603 - val_accuracy: 0.7950\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.1904 - accuracy: 0.9613 - val_loss: 0.5597 - val_accuracy: 0.7900\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.1893 - accuracy: 0.9600 - val_loss: 0.5573 - val_accuracy: 0.7950\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.1946 - accuracy: 0.9538 - val_loss: 0.5557 - val_accuracy: 0.8000\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.1960 - accuracy: 0.9513 - val_loss: 0.5536 - val_accuracy: 0.8000\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.1979 - accuracy: 0.9513 - val_loss: 0.5527 - val_accuracy: 0.7950\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 0.1868 - accuracy: 0.9663 - val_loss: 0.5535 - val_accuracy: 0.7950\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.1797 - accuracy: 0.9550 - val_loss: 0.5523 - val_accuracy: 0.7900\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.1750 - accuracy: 0.9663 - val_loss: 0.5520 - val_accuracy: 0.7900\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.1809 - accuracy: 0.9600 - val_loss: 0.5534 - val_accuracy: 0.7900\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.1756 - accuracy: 0.9625 - val_loss: 0.5555 - val_accuracy: 0.7900\n",
            "fold 3\n",
            "Model: \"functional_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_2 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_2 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_2 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "resnet50v2 (Functional)      (None, 10, 10, 2048)      23564800  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 23,585,290\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 23,564,800\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 2s 176ms/step - loss: 2.7507 - accuracy: 0.0950\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 2.8081 - accuracy: 0.0962 - val_loss: 2.4107 - val_accuracy: 0.1400\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 13s 251ms/step - loss: 2.4169 - accuracy: 0.1562 - val_loss: 2.1436 - val_accuracy: 0.1750\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 2.1665 - accuracy: 0.2275 - val_loss: 1.9177 - val_accuracy: 0.2800\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 1.9176 - accuracy: 0.3350 - val_loss: 1.7324 - val_accuracy: 0.4000\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 1.7318 - accuracy: 0.4087 - val_loss: 1.5793 - val_accuracy: 0.4650\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 1.5789 - accuracy: 0.4700 - val_loss: 1.4489 - val_accuracy: 0.5600\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 1.4300 - accuracy: 0.5475 - val_loss: 1.3418 - val_accuracy: 0.5900\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 1.3225 - accuracy: 0.5863 - val_loss: 1.2547 - val_accuracy: 0.6250\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 1.2132 - accuracy: 0.6425 - val_loss: 1.1804 - val_accuracy: 0.6500\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 1.1450 - accuracy: 0.6575 - val_loss: 1.1141 - val_accuracy: 0.6700\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 1.0487 - accuracy: 0.6988 - val_loss: 1.0644 - val_accuracy: 0.7050\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.9885 - accuracy: 0.7163 - val_loss: 1.0214 - val_accuracy: 0.7100\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.9467 - accuracy: 0.7487 - val_loss: 0.9826 - val_accuracy: 0.7200\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.8933 - accuracy: 0.7325 - val_loss: 0.9491 - val_accuracy: 0.7350\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.8491 - accuracy: 0.7625 - val_loss: 0.9162 - val_accuracy: 0.7400\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.8066 - accuracy: 0.7663 - val_loss: 0.8898 - val_accuracy: 0.7500\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.7280 - accuracy: 0.8100 - val_loss: 0.8663 - val_accuracy: 0.7500\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.7323 - accuracy: 0.7925 - val_loss: 0.8464 - val_accuracy: 0.7600\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.7387 - accuracy: 0.7862 - val_loss: 0.8250 - val_accuracy: 0.7700\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.6842 - accuracy: 0.8112 - val_loss: 0.8094 - val_accuracy: 0.7650\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.6618 - accuracy: 0.8087 - val_loss: 0.7953 - val_accuracy: 0.7850\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.6628 - accuracy: 0.8138 - val_loss: 0.7839 - val_accuracy: 0.7800\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.6169 - accuracy: 0.8413 - val_loss: 0.7670 - val_accuracy: 0.7900\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.6093 - accuracy: 0.8462 - val_loss: 0.7588 - val_accuracy: 0.7850\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.5965 - accuracy: 0.8400 - val_loss: 0.7494 - val_accuracy: 0.7750\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.5911 - accuracy: 0.8288 - val_loss: 0.7407 - val_accuracy: 0.7750\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.5556 - accuracy: 0.8475 - val_loss: 0.7300 - val_accuracy: 0.7850\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.5456 - accuracy: 0.8537 - val_loss: 0.7217 - val_accuracy: 0.7900\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.5300 - accuracy: 0.8438 - val_loss: 0.7136 - val_accuracy: 0.7900\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.5142 - accuracy: 0.8712 - val_loss: 0.7078 - val_accuracy: 0.7850\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.4968 - accuracy: 0.8725 - val_loss: 0.7030 - val_accuracy: 0.7850\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.4619 - accuracy: 0.8775 - val_loss: 0.6963 - val_accuracy: 0.7900\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.4713 - accuracy: 0.8700 - val_loss: 0.6906 - val_accuracy: 0.7850\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.4740 - accuracy: 0.8675 - val_loss: 0.6885 - val_accuracy: 0.7700\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.4359 - accuracy: 0.8875 - val_loss: 0.6846 - val_accuracy: 0.7700\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.4492 - accuracy: 0.8712 - val_loss: 0.6770 - val_accuracy: 0.7800\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.4345 - accuracy: 0.8763 - val_loss: 0.6753 - val_accuracy: 0.7850\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.4265 - accuracy: 0.8863 - val_loss: 0.6726 - val_accuracy: 0.7850\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.4166 - accuracy: 0.8850 - val_loss: 0.6660 - val_accuracy: 0.7850\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.4176 - accuracy: 0.9038 - val_loss: 0.6662 - val_accuracy: 0.7800\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.3953 - accuracy: 0.8938 - val_loss: 0.6611 - val_accuracy: 0.7800\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.3938 - accuracy: 0.8975 - val_loss: 0.6612 - val_accuracy: 0.7750\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.3746 - accuracy: 0.8975 - val_loss: 0.6576 - val_accuracy: 0.7800\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.3883 - accuracy: 0.8863 - val_loss: 0.6544 - val_accuracy: 0.7850\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.3783 - accuracy: 0.8900 - val_loss: 0.6530 - val_accuracy: 0.7900\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.3755 - accuracy: 0.9025 - val_loss: 0.6463 - val_accuracy: 0.7850\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.3595 - accuracy: 0.9025 - val_loss: 0.6452 - val_accuracy: 0.7850\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.3444 - accuracy: 0.9150 - val_loss: 0.6474 - val_accuracy: 0.7800\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.3606 - accuracy: 0.9025 - val_loss: 0.6424 - val_accuracy: 0.7800\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.3334 - accuracy: 0.9137 - val_loss: 0.6398 - val_accuracy: 0.7750\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.3122 - accuracy: 0.9200 - val_loss: 0.6384 - val_accuracy: 0.7750\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.3264 - accuracy: 0.9162 - val_loss: 0.6383 - val_accuracy: 0.7850\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.3241 - accuracy: 0.9112 - val_loss: 0.6364 - val_accuracy: 0.7850\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.3357 - accuracy: 0.8963 - val_loss: 0.6368 - val_accuracy: 0.7750\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2992 - accuracy: 0.9312 - val_loss: 0.6322 - val_accuracy: 0.7800\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2932 - accuracy: 0.9312 - val_loss: 0.6347 - val_accuracy: 0.7800\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.3118 - accuracy: 0.9187 - val_loss: 0.6298 - val_accuracy: 0.7800\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2970 - accuracy: 0.9175 - val_loss: 0.6269 - val_accuracy: 0.7800\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2903 - accuracy: 0.9162 - val_loss: 0.6229 - val_accuracy: 0.7800\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2917 - accuracy: 0.9200 - val_loss: 0.6229 - val_accuracy: 0.7850\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2904 - accuracy: 0.9287 - val_loss: 0.6252 - val_accuracy: 0.7750\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2876 - accuracy: 0.9262 - val_loss: 0.6272 - val_accuracy: 0.7800\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2796 - accuracy: 0.9162 - val_loss: 0.6301 - val_accuracy: 0.7750\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2696 - accuracy: 0.9287 - val_loss: 0.6261 - val_accuracy: 0.7800\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2679 - accuracy: 0.9337 - val_loss: 0.6249 - val_accuracy: 0.7800\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2717 - accuracy: 0.9287 - val_loss: 0.6219 - val_accuracy: 0.7800\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2545 - accuracy: 0.9350 - val_loss: 0.6256 - val_accuracy: 0.7800\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2881 - accuracy: 0.9250 - val_loss: 0.6235 - val_accuracy: 0.7800\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2426 - accuracy: 0.9438 - val_loss: 0.6237 - val_accuracy: 0.7800\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2589 - accuracy: 0.9425 - val_loss: 0.6264 - val_accuracy: 0.7800\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2460 - accuracy: 0.9463 - val_loss: 0.6252 - val_accuracy: 0.7800\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2371 - accuracy: 0.9475 - val_loss: 0.6295 - val_accuracy: 0.7900\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2510 - accuracy: 0.9375 - val_loss: 0.6233 - val_accuracy: 0.7800\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2290 - accuracy: 0.9538 - val_loss: 0.6247 - val_accuracy: 0.7850\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2276 - accuracy: 0.9488 - val_loss: 0.6268 - val_accuracy: 0.7800\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2228 - accuracy: 0.9488 - val_loss: 0.6221 - val_accuracy: 0.7850\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2321 - accuracy: 0.9375 - val_loss: 0.6229 - val_accuracy: 0.7800\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2281 - accuracy: 0.9475 - val_loss: 0.6210 - val_accuracy: 0.7800\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2145 - accuracy: 0.9475 - val_loss: 0.6216 - val_accuracy: 0.7800\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2499 - accuracy: 0.9325 - val_loss: 0.6234 - val_accuracy: 0.7850\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2107 - accuracy: 0.9438 - val_loss: 0.6262 - val_accuracy: 0.7800\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2146 - accuracy: 0.9575 - val_loss: 0.6304 - val_accuracy: 0.7800\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2167 - accuracy: 0.9575 - val_loss: 0.6299 - val_accuracy: 0.7800\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2011 - accuracy: 0.9500 - val_loss: 0.6286 - val_accuracy: 0.7800\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.1986 - accuracy: 0.9600 - val_loss: 0.6280 - val_accuracy: 0.7800\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.1973 - accuracy: 0.9575 - val_loss: 0.6271 - val_accuracy: 0.7800\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.1886 - accuracy: 0.9625 - val_loss: 0.6270 - val_accuracy: 0.7750\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.1991 - accuracy: 0.9600 - val_loss: 0.6286 - val_accuracy: 0.7750\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2075 - accuracy: 0.9513 - val_loss: 0.6272 - val_accuracy: 0.7750\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.1854 - accuracy: 0.9650 - val_loss: 0.6290 - val_accuracy: 0.7750\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.1907 - accuracy: 0.9538 - val_loss: 0.6229 - val_accuracy: 0.7700\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.1918 - accuracy: 0.9538 - val_loss: 0.6276 - val_accuracy: 0.7750\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.1918 - accuracy: 0.9538 - val_loss: 0.6339 - val_accuracy: 0.7750\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.1929 - accuracy: 0.9513 - val_loss: 0.6258 - val_accuracy: 0.7700\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.1682 - accuracy: 0.9675 - val_loss: 0.6217 - val_accuracy: 0.7750\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.1705 - accuracy: 0.9663 - val_loss: 0.6266 - val_accuracy: 0.7750\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.1796 - accuracy: 0.9588 - val_loss: 0.6221 - val_accuracy: 0.7800\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 0.1679 - accuracy: 0.9613 - val_loss: 0.6204 - val_accuracy: 0.7800\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.1819 - accuracy: 0.9638 - val_loss: 0.6242 - val_accuracy: 0.7850\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.1580 - accuracy: 0.9737 - val_loss: 0.6259 - val_accuracy: 0.7850\n",
            "fold 4\n",
            "Model: \"functional_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_3 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_3 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_3 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "resnet50v2 (Functional)      (None, 10, 10, 2048)      23564800  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_3 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 23,585,290\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 23,564,800\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 2s 179ms/step - loss: 2.5996 - accuracy: 0.1150\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 2.5138 - accuracy: 0.1488 - val_loss: 2.2384 - val_accuracy: 0.1800\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 2.2232 - accuracy: 0.2250 - val_loss: 2.0012 - val_accuracy: 0.2700\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 1.9692 - accuracy: 0.3075 - val_loss: 1.7936 - val_accuracy: 0.3500\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 13s 252ms/step - loss: 1.8167 - accuracy: 0.3613 - val_loss: 1.6264 - val_accuracy: 0.4500\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 1.6036 - accuracy: 0.4675 - val_loss: 1.4859 - val_accuracy: 0.5350\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 1.4397 - accuracy: 0.5400 - val_loss: 1.3673 - val_accuracy: 0.6000\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 1.3025 - accuracy: 0.6012 - val_loss: 1.2737 - val_accuracy: 0.6350\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 1.2395 - accuracy: 0.6338 - val_loss: 1.1911 - val_accuracy: 0.6650\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 1.1359 - accuracy: 0.6800 - val_loss: 1.1195 - val_accuracy: 0.6950\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 1.0627 - accuracy: 0.6875 - val_loss: 1.0596 - val_accuracy: 0.7100\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 1.0022 - accuracy: 0.6938 - val_loss: 1.0059 - val_accuracy: 0.7500\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.9498 - accuracy: 0.7175 - val_loss: 0.9638 - val_accuracy: 0.7550\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.8982 - accuracy: 0.7312 - val_loss: 0.9235 - val_accuracy: 0.7600\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.8303 - accuracy: 0.7538 - val_loss: 0.8857 - val_accuracy: 0.7750\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.8303 - accuracy: 0.7563 - val_loss: 0.8591 - val_accuracy: 0.7750\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.7917 - accuracy: 0.7638 - val_loss: 0.8297 - val_accuracy: 0.7750\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.7758 - accuracy: 0.7887 - val_loss: 0.8019 - val_accuracy: 0.7800\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.7295 - accuracy: 0.7875 - val_loss: 0.7801 - val_accuracy: 0.7850\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.6919 - accuracy: 0.7962 - val_loss: 0.7626 - val_accuracy: 0.7850\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.6715 - accuracy: 0.8012 - val_loss: 0.7465 - val_accuracy: 0.7850\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.6479 - accuracy: 0.8163 - val_loss: 0.7298 - val_accuracy: 0.7750\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.6288 - accuracy: 0.8225 - val_loss: 0.7144 - val_accuracy: 0.7850\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.6015 - accuracy: 0.8313 - val_loss: 0.6977 - val_accuracy: 0.7950\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.5870 - accuracy: 0.8375 - val_loss: 0.6890 - val_accuracy: 0.7900\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.5800 - accuracy: 0.8413 - val_loss: 0.6782 - val_accuracy: 0.7950\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.5832 - accuracy: 0.8475 - val_loss: 0.6618 - val_accuracy: 0.8050\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.5500 - accuracy: 0.8487 - val_loss: 0.6471 - val_accuracy: 0.8050\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.5536 - accuracy: 0.8400 - val_loss: 0.6388 - val_accuracy: 0.8050\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.5195 - accuracy: 0.8500 - val_loss: 0.6334 - val_accuracy: 0.8050\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.5114 - accuracy: 0.8625 - val_loss: 0.6203 - val_accuracy: 0.8050\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.5114 - accuracy: 0.8425 - val_loss: 0.6138 - val_accuracy: 0.8150\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.4516 - accuracy: 0.8938 - val_loss: 0.6055 - val_accuracy: 0.8300\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.4555 - accuracy: 0.8788 - val_loss: 0.6007 - val_accuracy: 0.8250\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.4641 - accuracy: 0.8875 - val_loss: 0.5937 - val_accuracy: 0.8250\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.4407 - accuracy: 0.8838 - val_loss: 0.5870 - val_accuracy: 0.8350\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.4486 - accuracy: 0.8775 - val_loss: 0.5815 - val_accuracy: 0.8300\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.4341 - accuracy: 0.8825 - val_loss: 0.5735 - val_accuracy: 0.8400\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.4285 - accuracy: 0.8938 - val_loss: 0.5769 - val_accuracy: 0.8250\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.4106 - accuracy: 0.8900 - val_loss: 0.5754 - val_accuracy: 0.8300\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.4075 - accuracy: 0.8788 - val_loss: 0.5671 - val_accuracy: 0.8250\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.3892 - accuracy: 0.8888 - val_loss: 0.5594 - val_accuracy: 0.8400\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.3756 - accuracy: 0.9125 - val_loss: 0.5563 - val_accuracy: 0.8350\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.3782 - accuracy: 0.8938 - val_loss: 0.5502 - val_accuracy: 0.8400\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.3874 - accuracy: 0.8875 - val_loss: 0.5453 - val_accuracy: 0.8300\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.3656 - accuracy: 0.9087 - val_loss: 0.5461 - val_accuracy: 0.8400\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.3781 - accuracy: 0.9062 - val_loss: 0.5438 - val_accuracy: 0.8450\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.3624 - accuracy: 0.9013 - val_loss: 0.5371 - val_accuracy: 0.8500\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.3599 - accuracy: 0.9050 - val_loss: 0.5314 - val_accuracy: 0.8400\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.3466 - accuracy: 0.9187 - val_loss: 0.5302 - val_accuracy: 0.8400\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.3499 - accuracy: 0.9162 - val_loss: 0.5287 - val_accuracy: 0.8500\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.3411 - accuracy: 0.9150 - val_loss: 0.5280 - val_accuracy: 0.8400\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.3279 - accuracy: 0.9150 - val_loss: 0.5232 - val_accuracy: 0.8400\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.3057 - accuracy: 0.9287 - val_loss: 0.5238 - val_accuracy: 0.8400\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.3239 - accuracy: 0.9075 - val_loss: 0.5177 - val_accuracy: 0.8450\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.3169 - accuracy: 0.9075 - val_loss: 0.5152 - val_accuracy: 0.8400\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2991 - accuracy: 0.9312 - val_loss: 0.5099 - val_accuracy: 0.8550\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2647 - accuracy: 0.9463 - val_loss: 0.5072 - val_accuracy: 0.8550\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.3014 - accuracy: 0.9212 - val_loss: 0.5056 - val_accuracy: 0.8600\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.3009 - accuracy: 0.9200 - val_loss: 0.5061 - val_accuracy: 0.8500\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.3030 - accuracy: 0.9225 - val_loss: 0.5027 - val_accuracy: 0.8600\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.2759 - accuracy: 0.9287 - val_loss: 0.4991 - val_accuracy: 0.8600\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2934 - accuracy: 0.9300 - val_loss: 0.4989 - val_accuracy: 0.8550\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2763 - accuracy: 0.9325 - val_loss: 0.4966 - val_accuracy: 0.8500\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2592 - accuracy: 0.9375 - val_loss: 0.4932 - val_accuracy: 0.8500\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2798 - accuracy: 0.9250 - val_loss: 0.4900 - val_accuracy: 0.8500\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2591 - accuracy: 0.9513 - val_loss: 0.4896 - val_accuracy: 0.8450\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2664 - accuracy: 0.9325 - val_loss: 0.4859 - val_accuracy: 0.8550\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2468 - accuracy: 0.9500 - val_loss: 0.4841 - val_accuracy: 0.8550\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2475 - accuracy: 0.9475 - val_loss: 0.4832 - val_accuracy: 0.8550\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2671 - accuracy: 0.9350 - val_loss: 0.4823 - val_accuracy: 0.8550\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2410 - accuracy: 0.9450 - val_loss: 0.4850 - val_accuracy: 0.8500\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2305 - accuracy: 0.9438 - val_loss: 0.4782 - val_accuracy: 0.8550\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2525 - accuracy: 0.9388 - val_loss: 0.4781 - val_accuracy: 0.8650\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2197 - accuracy: 0.9575 - val_loss: 0.4755 - val_accuracy: 0.8500\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.2343 - accuracy: 0.9413 - val_loss: 0.4781 - val_accuracy: 0.8450\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2274 - accuracy: 0.9463 - val_loss: 0.4785 - val_accuracy: 0.8500\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2183 - accuracy: 0.9463 - val_loss: 0.4817 - val_accuracy: 0.8550\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2133 - accuracy: 0.9600 - val_loss: 0.4767 - val_accuracy: 0.8500\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2246 - accuracy: 0.9450 - val_loss: 0.4751 - val_accuracy: 0.8700\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2173 - accuracy: 0.9362 - val_loss: 0.4706 - val_accuracy: 0.8550\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2247 - accuracy: 0.9500 - val_loss: 0.4696 - val_accuracy: 0.8600\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.2080 - accuracy: 0.9575 - val_loss: 0.4702 - val_accuracy: 0.8500\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.1985 - accuracy: 0.9625 - val_loss: 0.4645 - val_accuracy: 0.8600\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2078 - accuracy: 0.9600 - val_loss: 0.4686 - val_accuracy: 0.8550\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.1948 - accuracy: 0.9563 - val_loss: 0.4681 - val_accuracy: 0.8650\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2085 - accuracy: 0.9538 - val_loss: 0.4658 - val_accuracy: 0.8450\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.1948 - accuracy: 0.9550 - val_loss: 0.4591 - val_accuracy: 0.8550\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.1852 - accuracy: 0.9650 - val_loss: 0.4633 - val_accuracy: 0.8450\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.1948 - accuracy: 0.9513 - val_loss: 0.4635 - val_accuracy: 0.8500\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.1918 - accuracy: 0.9663 - val_loss: 0.4604 - val_accuracy: 0.8550\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.1826 - accuracy: 0.9625 - val_loss: 0.4613 - val_accuracy: 0.8600\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.1744 - accuracy: 0.9650 - val_loss: 0.4588 - val_accuracy: 0.8500\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.1760 - accuracy: 0.9712 - val_loss: 0.4594 - val_accuracy: 0.8500\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.1805 - accuracy: 0.9638 - val_loss: 0.4631 - val_accuracy: 0.8450\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.1701 - accuracy: 0.9737 - val_loss: 0.4645 - val_accuracy: 0.8500\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.1751 - accuracy: 0.9638 - val_loss: 0.4583 - val_accuracy: 0.8500\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.1615 - accuracy: 0.9737 - val_loss: 0.4604 - val_accuracy: 0.8450\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.1698 - accuracy: 0.9750 - val_loss: 0.4584 - val_accuracy: 0.8450\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.1765 - accuracy: 0.9712 - val_loss: 0.4595 - val_accuracy: 0.8550\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.1687 - accuracy: 0.9650 - val_loss: 0.4618 - val_accuracy: 0.8450\n",
            "fold 5\n",
            "Model: \"functional_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_4 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_4 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_4 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "resnet50v2 (Functional)      (None, 10, 10, 2048)      23564800  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_4 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 23,585,290\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 23,564,800\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 2s 178ms/step - loss: 2.6778 - accuracy: 0.0750\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 2.5838 - accuracy: 0.1213 - val_loss: 2.2884 - val_accuracy: 0.2050\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 2.2402 - accuracy: 0.1988 - val_loss: 2.0224 - val_accuracy: 0.2850\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 1.9921 - accuracy: 0.2925 - val_loss: 1.8080 - val_accuracy: 0.3700\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 1.7717 - accuracy: 0.3887 - val_loss: 1.6266 - val_accuracy: 0.4750\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 1.6083 - accuracy: 0.4712 - val_loss: 1.4769 - val_accuracy: 0.5450\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 1.4529 - accuracy: 0.5362 - val_loss: 1.3607 - val_accuracy: 0.5950\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 1.3266 - accuracy: 0.5738 - val_loss: 1.2547 - val_accuracy: 0.6300\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 1.2424 - accuracy: 0.6450 - val_loss: 1.1674 - val_accuracy: 0.6550\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 1.1603 - accuracy: 0.6600 - val_loss: 1.0950 - val_accuracy: 0.6850\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 1.0863 - accuracy: 0.6762 - val_loss: 1.0345 - val_accuracy: 0.6950\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 1.0519 - accuracy: 0.7013 - val_loss: 0.9799 - val_accuracy: 0.7200\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.9855 - accuracy: 0.7088 - val_loss: 0.9359 - val_accuracy: 0.7200\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.9038 - accuracy: 0.7412 - val_loss: 0.8970 - val_accuracy: 0.7250\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.8701 - accuracy: 0.7513 - val_loss: 0.8684 - val_accuracy: 0.7250\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.8183 - accuracy: 0.7688 - val_loss: 0.8341 - val_accuracy: 0.7400\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.7843 - accuracy: 0.7738 - val_loss: 0.8052 - val_accuracy: 0.7450\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.7535 - accuracy: 0.7862 - val_loss: 0.7833 - val_accuracy: 0.7550\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.7257 - accuracy: 0.7837 - val_loss: 0.7613 - val_accuracy: 0.7650\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.7001 - accuracy: 0.8062 - val_loss: 0.7420 - val_accuracy: 0.7700\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.6924 - accuracy: 0.7975 - val_loss: 0.7298 - val_accuracy: 0.7750\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.6539 - accuracy: 0.8300 - val_loss: 0.7108 - val_accuracy: 0.7900\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.6736 - accuracy: 0.8000 - val_loss: 0.6953 - val_accuracy: 0.7900\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.6146 - accuracy: 0.8500 - val_loss: 0.6823 - val_accuracy: 0.8000\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.5910 - accuracy: 0.8300 - val_loss: 0.6719 - val_accuracy: 0.7800\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.6090 - accuracy: 0.8238 - val_loss: 0.6577 - val_accuracy: 0.7950\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.5676 - accuracy: 0.8438 - val_loss: 0.6476 - val_accuracy: 0.7900\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.5730 - accuracy: 0.8487 - val_loss: 0.6383 - val_accuracy: 0.8050\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.5155 - accuracy: 0.8625 - val_loss: 0.6320 - val_accuracy: 0.7900\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.5537 - accuracy: 0.8375 - val_loss: 0.6225 - val_accuracy: 0.8000\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.5077 - accuracy: 0.8687 - val_loss: 0.6134 - val_accuracy: 0.8000\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.5176 - accuracy: 0.8537 - val_loss: 0.6089 - val_accuracy: 0.8050\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.5004 - accuracy: 0.8600 - val_loss: 0.5999 - val_accuracy: 0.8050\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.4902 - accuracy: 0.8612 - val_loss: 0.5972 - val_accuracy: 0.8100\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.4647 - accuracy: 0.8800 - val_loss: 0.5928 - val_accuracy: 0.8050\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.4741 - accuracy: 0.8637 - val_loss: 0.5852 - val_accuracy: 0.8050\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.4529 - accuracy: 0.8687 - val_loss: 0.5832 - val_accuracy: 0.8150\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.4416 - accuracy: 0.8800 - val_loss: 0.5753 - val_accuracy: 0.8100\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.4324 - accuracy: 0.8850 - val_loss: 0.5697 - val_accuracy: 0.8200\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.4313 - accuracy: 0.8788 - val_loss: 0.5660 - val_accuracy: 0.8100\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 13s 253ms/step - loss: 0.4173 - accuracy: 0.8875 - val_loss: 0.5649 - val_accuracy: 0.8100\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.3916 - accuracy: 0.9000 - val_loss: 0.5595 - val_accuracy: 0.8150\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.4008 - accuracy: 0.8875 - val_loss: 0.5553 - val_accuracy: 0.8200\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.3728 - accuracy: 0.9000 - val_loss: 0.5543 - val_accuracy: 0.8200\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.4179 - accuracy: 0.8900 - val_loss: 0.5464 - val_accuracy: 0.8150\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.3754 - accuracy: 0.9000 - val_loss: 0.5438 - val_accuracy: 0.8400\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.3788 - accuracy: 0.9050 - val_loss: 0.5405 - val_accuracy: 0.8350\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.3539 - accuracy: 0.9175 - val_loss: 0.5401 - val_accuracy: 0.8400\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.3551 - accuracy: 0.9200 - val_loss: 0.5347 - val_accuracy: 0.8350\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 13s 256ms/step - loss: 0.3471 - accuracy: 0.9100 - val_loss: 0.5326 - val_accuracy: 0.8250\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.3431 - accuracy: 0.9112 - val_loss: 0.5296 - val_accuracy: 0.8400\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.3280 - accuracy: 0.9200 - val_loss: 0.5285 - val_accuracy: 0.8400\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.3419 - accuracy: 0.9162 - val_loss: 0.5248 - val_accuracy: 0.8350\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.3499 - accuracy: 0.8975 - val_loss: 0.5244 - val_accuracy: 0.8450\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.3349 - accuracy: 0.9125 - val_loss: 0.5224 - val_accuracy: 0.8350\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.3175 - accuracy: 0.9250 - val_loss: 0.5222 - val_accuracy: 0.8300\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.3414 - accuracy: 0.9025 - val_loss: 0.5202 - val_accuracy: 0.8350\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.3028 - accuracy: 0.9200 - val_loss: 0.5184 - val_accuracy: 0.8300\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.3118 - accuracy: 0.9062 - val_loss: 0.5177 - val_accuracy: 0.8250\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.2962 - accuracy: 0.9300 - val_loss: 0.5150 - val_accuracy: 0.8400\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.3169 - accuracy: 0.9175 - val_loss: 0.5133 - val_accuracy: 0.8300\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.3056 - accuracy: 0.9125 - val_loss: 0.5117 - val_accuracy: 0.8350\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.2948 - accuracy: 0.9187 - val_loss: 0.5107 - val_accuracy: 0.8250\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.2839 - accuracy: 0.9287 - val_loss: 0.5071 - val_accuracy: 0.8450\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2846 - accuracy: 0.9325 - val_loss: 0.5070 - val_accuracy: 0.8300\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.2667 - accuracy: 0.9325 - val_loss: 0.5027 - val_accuracy: 0.8450\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2662 - accuracy: 0.9400 - val_loss: 0.5046 - val_accuracy: 0.8300\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.2487 - accuracy: 0.9438 - val_loss: 0.5049 - val_accuracy: 0.8350\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2559 - accuracy: 0.9350 - val_loss: 0.5013 - val_accuracy: 0.8350\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.2631 - accuracy: 0.9312 - val_loss: 0.4992 - val_accuracy: 0.8350\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.2501 - accuracy: 0.9450 - val_loss: 0.4964 - val_accuracy: 0.8350\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.2410 - accuracy: 0.9350 - val_loss: 0.4968 - val_accuracy: 0.8350\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.2622 - accuracy: 0.9237 - val_loss: 0.4940 - val_accuracy: 0.8350\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.2551 - accuracy: 0.9337 - val_loss: 0.4932 - val_accuracy: 0.8350\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2631 - accuracy: 0.9325 - val_loss: 0.4975 - val_accuracy: 0.8250\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.2347 - accuracy: 0.9450 - val_loss: 0.4941 - val_accuracy: 0.8400\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2453 - accuracy: 0.9400 - val_loss: 0.4913 - val_accuracy: 0.8250\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2440 - accuracy: 0.9362 - val_loss: 0.4892 - val_accuracy: 0.8350\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.2428 - accuracy: 0.9425 - val_loss: 0.4914 - val_accuracy: 0.8300\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2092 - accuracy: 0.9538 - val_loss: 0.4919 - val_accuracy: 0.8300\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.2243 - accuracy: 0.9413 - val_loss: 0.4911 - val_accuracy: 0.8300\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.2202 - accuracy: 0.9538 - val_loss: 0.4916 - val_accuracy: 0.8250\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.2169 - accuracy: 0.9450 - val_loss: 0.4897 - val_accuracy: 0.8250\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.2144 - accuracy: 0.9550 - val_loss: 0.4867 - val_accuracy: 0.8250\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.2128 - accuracy: 0.9500 - val_loss: 0.4860 - val_accuracy: 0.8250\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.2076 - accuracy: 0.9513 - val_loss: 0.4882 - val_accuracy: 0.8250\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.2040 - accuracy: 0.9538 - val_loss: 0.4863 - val_accuracy: 0.8250\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.2067 - accuracy: 0.9525 - val_loss: 0.4860 - val_accuracy: 0.8250\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.1908 - accuracy: 0.9563 - val_loss: 0.4890 - val_accuracy: 0.8250\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.2114 - accuracy: 0.9475 - val_loss: 0.4883 - val_accuracy: 0.8250\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.1894 - accuracy: 0.9588 - val_loss: 0.4883 - val_accuracy: 0.8250\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.2154 - accuracy: 0.9500 - val_loss: 0.4880 - val_accuracy: 0.8250\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.1930 - accuracy: 0.9600 - val_loss: 0.4891 - val_accuracy: 0.8250\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.1804 - accuracy: 0.9688 - val_loss: 0.4873 - val_accuracy: 0.8250\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.1903 - accuracy: 0.9550 - val_loss: 0.4897 - val_accuracy: 0.8250\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 13s 255ms/step - loss: 0.1930 - accuracy: 0.9625 - val_loss: 0.4880 - val_accuracy: 0.8250\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.1776 - accuracy: 0.9688 - val_loss: 0.4852 - val_accuracy: 0.8250\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 13s 256ms/step - loss: 0.1780 - accuracy: 0.9675 - val_loss: 0.4841 - val_accuracy: 0.8250\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.1624 - accuracy: 0.9675 - val_loss: 0.4836 - val_accuracy: 0.8250\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.1828 - accuracy: 0.9638 - val_loss: 0.4851 - val_accuracy: 0.8250\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 13s 254ms/step - loss: 0.1715 - accuracy: 0.9688 - val_loss: 0.4837 - val_accuracy: 0.8250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2E72C3O30fv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82a2417e-5a9f-4e97-a1db-9c27e9941f67"
      },
      "source": [
        "# cross-validated accuracy for pre-trained model before training\n",
        "print(\"Base model accuracy:\", np.mean(base_model_acc_list))\n",
        "# cross-validated accuracy after training\n",
        "print(\"Final accuracy:\", np.mean(final_acc_list))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Base model accuracy: 0.09800000041723252\n",
            "Final accuracy: 0.828000009059906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dtn-03T_ZaRX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "738dffdd-2c59-4b77-b6c8-c525feaaf4a3"
      },
      "source": [
        "# plot graph of cross-validated accuracies\n",
        "acc = np.mean(history_map['accuracy'], axis=0)\n",
        "val_acc = np.mean(history_map['val_accuracy'], axis=0)\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftCYvBkKZmGa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "b17006c9-7e59-47d4-9167-0f2f5113c063"
      },
      "source": [
        "# plot graph of cross-validated losses\n",
        "loss = np.mean(history_map['loss'], axis=0)\n",
        "val_loss = np.mean(history_map['val_loss'], axis=0)\n",
        "\n",
        "plt.figure(2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "#plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}