{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "materials-xception-balanced-cross-validated.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNC8CRXLeHRTwcZePdzr3+G"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERKJLyg2z-Uq"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fIbsvD01EQm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34699ea0-4aa0-4d2e-e576-4cc2a7b8c4b2"
      },
      "source": [
        "import pathlib\n",
        "\n",
        "# set path to FMD image directory\n",
        "data_dir = pathlib.Path(\"image\")\n",
        "\n",
        "# total no. of images in FMD dataset\n",
        "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
        "print(image_count)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL3rBqoe3sK5"
      },
      "source": [
        "# set batch size for training\n",
        "batch_size = 16\n",
        "\n",
        "# set dimensions to change images into for training\n",
        "# 299x299 images is used to train for consistency between different pre-trained models\n",
        "img_height = 299\n",
        "img_width = 299"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFKuhNRxqxcB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87e7c1f8-26bf-4846-a76e-00c8975aed26"
      },
      "source": [
        "# get class names from the folder names in data_dir\n",
        "class_names = np.array(sorted([item.name for item in data_dir.glob('*') if item.name != \"LICENSE.txt\"]))\n",
        "print(class_names)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['fabric' 'foliage' 'glass' 'leather' 'metal' 'paper' 'plastic' 'stone'\n",
            " 'water' 'wood']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap_FpvbEqWrS"
      },
      "source": [
        "# create list containing the dataset for each class\n",
        "ds_each_class = [tf.data.Dataset.list_files(str(data_dir/f'{class_name}/*.jpg'), shuffle=False) for class_name in class_names]\n",
        "\n",
        "# shuffle the 100 images in each class with the random seed value of 123 before training\n",
        "for index, ds in enumerate(ds_each_class):\n",
        "  ds_each_class[index] = ds.shuffle(image_count//10, seed=123, reshuffle_each_iteration=False)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty_LijJpqbEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f3110cc-e5b7-49d7-8ed3-d1c2aae2d289"
      },
      "source": [
        "# display some samples from a class to verify each class dataset contains only the class images\n",
        "for f in ds_each_class[0].take(10):\n",
        "  print(f.numpy())\n",
        "\n",
        "for f in ds_each_class[1].take(10):\n",
        "  print(f.numpy())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'FMD/image/fabric/fabric_moderate_037_new.jpg'\n",
            "b'FMD/image/fabric/fabric_moderate_004_new.jpg'\n",
            "b'FMD/image/fabric/fabric_moderate_008_new.jpg'\n",
            "b'FMD/image/fabric/fabric_object_003_new.jpg'\n",
            "b'FMD/image/fabric/fabric_object_017_new.jpg'\n",
            "b'FMD/image/fabric/fabric_object_001_new.jpg'\n",
            "b'FMD/image/fabric/fabric_moderate_032_new.jpg'\n",
            "b'FMD/image/fabric/fabric_moderate_030_new.jpg'\n",
            "b'FMD/image/fabric/fabric_moderate_038_new.jpg'\n",
            "b'FMD/image/fabric/fabric_object_009_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_037_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_004_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_008_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_053_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_067_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_051_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_032_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_030_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_038_new.jpg'\n",
            "b'FMD/image/foliage/foliage_final_059_new.jpg'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACILObxurdXN"
      },
      "source": [
        "# split first class dataset into 5 equal sized partitions\n",
        "# then for remaining classes' datasets do the same and add to corresponding partition\n",
        "# for 5-fold cross validation\n",
        "A = ds_each_class[0].shard(num_shards=5, index=0)\n",
        "B = ds_each_class[0].shard(num_shards=5, index=1)\n",
        "C = ds_each_class[0].shard(num_shards=5, index=2)\n",
        "D = ds_each_class[0].shard(num_shards=5, index=3)\n",
        "E = ds_each_class[0].shard(num_shards=5, index=4)\n",
        "for i in range(1, 10):\n",
        "  A = A.concatenate(ds_each_class[i].shard(num_shards=5, index=0))\n",
        "  B = B.concatenate(ds_each_class[i].shard(num_shards=5, index=1))\n",
        "  C = C.concatenate(ds_each_class[i].shard(num_shards=5, index=2))\n",
        "  D = D.concatenate(ds_each_class[i].shard(num_shards=5, index=3))\n",
        "  E = E.concatenate(ds_each_class[i].shard(num_shards=5, index=4))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9oYdHkhrwdz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "874eb196-4498-421a-93ed-7048b721d1e2"
      },
      "source": [
        "# check no. of samples in each partition is the same\n",
        "print(A.cardinality().numpy())\n",
        "print(B.cardinality().numpy())\n",
        "print(C.cardinality().numpy())\n",
        "print(D.cardinality().numpy())\n",
        "print(E.cardinality().numpy())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BdD3FMHtGRV"
      },
      "source": [
        "def get_label(file_path):\n",
        "  # convert the path to a list of path components\n",
        "  parts = tf.strings.split(file_path, os.path.sep)\n",
        "  # The second to last is the class-directory\n",
        "  one_hot = parts[-2] == class_names\n",
        "  # Integer encode the label\n",
        "  return tf.argmax(one_hot)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfWduaL4tKDV"
      },
      "source": [
        "def decode_img(img):\n",
        "  # convert the compressed string to a 3D uint8 tensor\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\n",
        "  # resize the image to the desired size\n",
        "  return tf.image.resize(img, [img_height, img_width])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bGGe0KltMTC"
      },
      "source": [
        "def process_path(file_path):\n",
        "  label = get_label(file_path)\n",
        "  # load the raw data from the file as a string\n",
        "  img = tf.io.read_file(file_path)\n",
        "  img = decode_img(img)\n",
        "  return img, label"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcyZLwRxt1dy"
      },
      "source": [
        "# prompt the tf.data runtime to tune the number of elements to prefetch dynamically at runtime\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkBqAQlHtblh"
      },
      "source": [
        "# use the path of image to load the image into each partition of the dataset\n",
        "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
        "A = A.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "B = B.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "C = C.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "D = D.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "E = E.map(process_path, num_parallel_calls=AUTOTUNE)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc9pmaQ5t9H5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "323e1fd1-4c3a-4ceb-88ef-9b0a6899ca4f"
      },
      "source": [
        "for image, label in A.take(1):\n",
        "  print(\"Image shape: \", image.numpy().shape)\n",
        "  print(\"Label: \", label.numpy())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image shape:  (299, 299, 3)\n",
            "Label:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_T2KNdGub1X"
      },
      "source": [
        "# shuffle, batch, and prefetch the dataset\n",
        "def configure_for_performance(ds):\n",
        "  ds = ds.cache()\n",
        "  ds = ds.shuffle(buffer_size=1000)\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
        "  return ds"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWcojoVRuok5"
      },
      "source": [
        "# map the dataset partitions to integers for building training/test sets during cross-validation\n",
        "ds_fold_dict = {0:A, 1:B, 2:C, 3:D, 4:E}"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx3xKoMZU9Yv"
      },
      "source": [
        "# normalise the input values to the pre-trained model's required range of values\n",
        "preprocess_input = keras.applications.xception.preprocess_input"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVOP5fIPwEx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89ca66ae-d5af-465c-9b49-923ca124c578"
      },
      "source": [
        "# get pre-trained model\n",
        "base_model = keras.applications.Xception(include_top=False, input_shape=(img_height, img_width, 3))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS2kAceGVW0e"
      },
      "source": [
        "# don't train base model weights\n",
        "base_model.trainable = False"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGkReMX60ScJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0e4d90d-a07c-404d-8836-e0242aad1938"
      },
      "source": [
        "base_model.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"xception\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 299, 299, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1 (Conv2D)           (None, 149, 149, 32) 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_bn (BatchNormaliza (None, 149, 149, 32) 128         block1_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_act (Activation)   (None, 149, 149, 32) 0           block1_conv1_bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2 (Conv2D)           (None, 147, 147, 64) 18432       block1_conv1_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_bn (BatchNormaliza (None, 147, 147, 64) 256         block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_act (Activation)   (None, 147, 147, 64) 0           block1_conv2_bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv1 (SeparableConv2 (None, 147, 147, 128 8768        block1_conv2_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv1_bn (BatchNormal (None, 147, 147, 128 512         block2_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2_act (Activation (None, 147, 147, 128 0           block2_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2 (SeparableConv2 (None, 147, 147, 128 17536       block2_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2_bn (BatchNormal (None, 147, 147, 128 512         block2_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 74, 74, 128)  8192        block1_conv2_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool (MaxPooling2D)      (None, 74, 74, 128)  0           block2_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 74, 74, 128)  512         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 74, 74, 128)  0           block2_pool[0][0]                \n",
            "                                                                 batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1_act (Activation (None, 74, 74, 128)  0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1 (SeparableConv2 (None, 74, 74, 256)  33920       block3_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1_bn (BatchNormal (None, 74, 74, 256)  1024        block3_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2_act (Activation (None, 74, 74, 256)  0           block3_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2 (SeparableConv2 (None, 74, 74, 256)  67840       block3_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2_bn (BatchNormal (None, 74, 74, 256)  1024        block3_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 37, 37, 256)  32768       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool (MaxPooling2D)      (None, 37, 37, 256)  0           block3_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 37, 37, 256)  1024        conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 37, 37, 256)  0           block3_pool[0][0]                \n",
            "                                                                 batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1_act (Activation (None, 37, 37, 256)  0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1 (SeparableConv2 (None, 37, 37, 728)  188672      block4_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1_bn (BatchNormal (None, 37, 37, 728)  2912        block4_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2_act (Activation (None, 37, 37, 728)  0           block4_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2 (SeparableConv2 (None, 37, 37, 728)  536536      block4_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2_bn (BatchNormal (None, 37, 37, 728)  2912        block4_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 19, 19, 728)  186368      add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool (MaxPooling2D)      (None, 19, 19, 728)  0           block4_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 19, 19, 728)  2912        conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 19, 19, 728)  0           block4_pool[0][0]                \n",
            "                                                                 batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1_act (Activation (None, 19, 19, 728)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block5_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block5_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2_act (Activation (None, 19, 19, 728)  0           block5_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block5_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block5_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3_act (Activation (None, 19, 19, 728)  0           block5_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block5_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block5_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 19, 19, 728)  0           block5_sepconv3_bn[0][0]         \n",
            "                                                                 add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1_act (Activation (None, 19, 19, 728)  0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block6_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block6_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2_act (Activation (None, 19, 19, 728)  0           block6_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block6_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block6_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3_act (Activation (None, 19, 19, 728)  0           block6_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block6_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block6_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 19, 19, 728)  0           block6_sepconv3_bn[0][0]         \n",
            "                                                                 add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1_act (Activation (None, 19, 19, 728)  0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block7_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block7_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2_act (Activation (None, 19, 19, 728)  0           block7_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block7_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block7_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3_act (Activation (None, 19, 19, 728)  0           block7_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block7_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block7_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 19, 19, 728)  0           block7_sepconv3_bn[0][0]         \n",
            "                                                                 add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1_act (Activation (None, 19, 19, 728)  0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block8_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block8_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2_act (Activation (None, 19, 19, 728)  0           block8_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block8_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block8_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3_act (Activation (None, 19, 19, 728)  0           block8_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block8_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block8_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 19, 19, 728)  0           block8_sepconv3_bn[0][0]         \n",
            "                                                                 add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1_act (Activation (None, 19, 19, 728)  0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block9_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block9_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2_act (Activation (None, 19, 19, 728)  0           block9_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block9_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block9_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3_act (Activation (None, 19, 19, 728)  0           block9_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block9_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block9_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 19, 19, 728)  0           block9_sepconv3_bn[0][0]         \n",
            "                                                                 add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block10_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block10_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2_act (Activatio (None, 19, 19, 728)  0           block10_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2 (SeparableConv (None, 19, 19, 728)  536536      block10_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2_bn (BatchNorma (None, 19, 19, 728)  2912        block10_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3_act (Activatio (None, 19, 19, 728)  0           block10_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3 (SeparableConv (None, 19, 19, 728)  536536      block10_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3_bn (BatchNorma (None, 19, 19, 728)  2912        block10_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 19, 19, 728)  0           block10_sepconv3_bn[0][0]        \n",
            "                                                                 add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block11_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block11_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2_act (Activatio (None, 19, 19, 728)  0           block11_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2 (SeparableConv (None, 19, 19, 728)  536536      block11_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2_bn (BatchNorma (None, 19, 19, 728)  2912        block11_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3_act (Activatio (None, 19, 19, 728)  0           block11_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3 (SeparableConv (None, 19, 19, 728)  536536      block11_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3_bn (BatchNorma (None, 19, 19, 728)  2912        block11_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 19, 19, 728)  0           block11_sepconv3_bn[0][0]        \n",
            "                                                                 add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block12_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block12_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2_act (Activatio (None, 19, 19, 728)  0           block12_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2 (SeparableConv (None, 19, 19, 728)  536536      block12_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2_bn (BatchNorma (None, 19, 19, 728)  2912        block12_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3_act (Activatio (None, 19, 19, 728)  0           block12_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3 (SeparableConv (None, 19, 19, 728)  536536      block12_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3_bn (BatchNorma (None, 19, 19, 728)  2912        block12_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 19, 19, 728)  0           block12_sepconv3_bn[0][0]        \n",
            "                                                                 add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block13_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block13_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2_act (Activatio (None, 19, 19, 728)  0           block13_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2 (SeparableConv (None, 19, 19, 1024) 752024      block13_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2_bn (BatchNorma (None, 19, 19, 1024) 4096        block13_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 10, 10, 1024) 745472      add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_pool (MaxPooling2D)     (None, 10, 10, 1024) 0           block13_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 10, 10, 1024) 4096        conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 10, 10, 1024) 0           block13_pool[0][0]               \n",
            "                                                                 batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1 (SeparableConv (None, 10, 10, 1536) 1582080     add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1_bn (BatchNorma (None, 10, 10, 1536) 6144        block14_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1_act (Activatio (None, 10, 10, 1536) 0           block14_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2 (SeparableConv (None, 10, 10, 2048) 3159552     block14_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2_bn (BatchNorma (None, 10, 10, 2048) 8192        block14_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2_act (Activatio (None, 10, 10, 2048) 0           block14_sepconv2_bn[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 20,861,480\n",
            "Trainable params: 0\n",
            "Non-trainable params: 20,861,480\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzhaWb2aX2if"
      },
      "source": [
        "# set a low learning rate to avoid overfitting too quickly\n",
        "base_learning_rate = 0.0001\n",
        "\n",
        "# create the model to train using the pre-trained model as base model\n",
        "def create_model():\n",
        "  # generate additional training data from input training data by augmenting them using random flip, rotation & zoom\n",
        "  data_augmentation = keras.Sequential(\n",
        "    [\n",
        "      layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n",
        "                                                  input_shape=(img_height, \n",
        "                                                                img_width,\n",
        "                                                                3)),\n",
        "      layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "      layers.experimental.preprocessing.RandomZoom(0.1),\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  # average over the spatial locations to convert the features to a single vector per image\n",
        "  global_average_layer = keras.layers.GlobalAveragePooling2D()\n",
        "  # convert these features into a single prediction per image\n",
        "  prediction_layer = keras.layers.Dense(10)\n",
        "\n",
        "  # Build a model by chaining together the layers using the Keras Functional API.\n",
        "  inputs = keras.Input(shape=(img_height, img_width, 3))\n",
        "  x = data_augmentation(inputs)\n",
        "  x = preprocess_input(x)\n",
        "  x = base_model(x, training=False) # use training=False as the base model contains a BatchNormalization layer\n",
        "  x = global_average_layer(x)\n",
        "  x = keras.layers.Dropout(0.2)(x) # add dropout to fully connected layer to reduce overfitting\n",
        "  outputs = prediction_layer(x)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  optimizer = keras.optimizers.Adam(lr=base_learning_rate)\n",
        "  loss= keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  # compile model with Adam optimizer with specified learning rate\n",
        "  model.compile(optimizer=optimizer,\n",
        "                loss=loss,\n",
        "                metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-5TEZRgY2l_"
      },
      "source": [
        "no_epochs = 100"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya698zRbu7Ep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03ab7a80-b8b0-40d7-8973-6a4887b40db7"
      },
      "source": [
        "# store results to plot graphs and get cross-validated accuracies\n",
        "history_map = {}\n",
        "base_model_acc_list = []\n",
        "final_acc_list = []\n",
        "\n",
        "# do 5-fold cross-validation\n",
        "for i in range(5):\n",
        "  print('fold', i + 1)\n",
        "  temp_dict = ds_fold_dict.copy()\n",
        "  # get test set for this iteration\n",
        "  current_val_ds = temp_dict[i]\n",
        "\n",
        "  # get training set for this iteration from remaining data samples\n",
        "  del temp_dict[i]\n",
        "  current_train_ds = None\n",
        "  for ds_shard in temp_dict.values():\n",
        "    if current_train_ds is None:\n",
        "      current_train_ds = ds_shard\n",
        "    else:\n",
        "      current_train_ds = current_train_ds.concatenate(ds_shard)\n",
        "  \n",
        "  # configure both training and test sets to improve performance\n",
        "  current_train_ds = configure_for_performance(current_train_ds)\n",
        "  current_val_ds = configure_for_performance(current_val_ds)\n",
        "\n",
        "  # create a new model\n",
        "  model = create_model()\n",
        "  # get initial test accuracy\n",
        "  base_model_acc_list.append(model.evaluate(current_val_ds)[1])\n",
        "  # train for specified epochs\n",
        "  history = model.fit(current_train_ds,\n",
        "                    epochs=no_epochs,\n",
        "                    validation_data=current_val_ds)\n",
        "  \n",
        "  # save results\n",
        "  if i == 0:\n",
        "    history_map['accuracy'] = [history.history['accuracy']]\n",
        "    history_map['val_accuracy'] = [history.history['val_accuracy']]\n",
        "    history_map['loss'] = [history.history['loss']]\n",
        "    history_map['val_loss'] = [history.history['val_loss']]\n",
        "  else:\n",
        "    history_map['accuracy'].append(history.history['accuracy'])\n",
        "    history_map['val_accuracy'].append(history.history['val_accuracy'])\n",
        "    history_map['loss'].append(history.history['loss'])\n",
        "    history_map['val_loss'].append(history.history['val_loss'])\n",
        "  \n",
        "  # get final test accuracy by taking the max accuracy over the whole training\n",
        "  final_acc_list.append(np.amax(history.history['val_accuracy']))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fold 1\n",
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential (Sequential)      (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv (TensorF [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub (TensorFlowO [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "xception (Functional)        (None, 10, 10, 2048)      20861480  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 20,881,970\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 20,861,480\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 2.3565 - accuracy: 0.0750\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 8s 152ms/step - loss: 2.2977 - accuracy: 0.1425 - val_loss: 2.2133 - val_accuracy: 0.2200\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 8s 154ms/step - loss: 2.1457 - accuracy: 0.2663 - val_loss: 2.0843 - val_accuracy: 0.3450\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 8s 154ms/step - loss: 2.0245 - accuracy: 0.3713 - val_loss: 1.9678 - val_accuracy: 0.4900\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 8s 156ms/step - loss: 1.9089 - accuracy: 0.4837 - val_loss: 1.8576 - val_accuracy: 0.5900\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 8s 157ms/step - loss: 1.7785 - accuracy: 0.5838 - val_loss: 1.7562 - val_accuracy: 0.6600\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 8s 158ms/step - loss: 1.6774 - accuracy: 0.6313 - val_loss: 1.6627 - val_accuracy: 0.7300\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 8s 160ms/step - loss: 1.5956 - accuracy: 0.6800 - val_loss: 1.5792 - val_accuracy: 0.7450\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 8s 163ms/step - loss: 1.4974 - accuracy: 0.7075 - val_loss: 1.5015 - val_accuracy: 0.7800\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 8s 165ms/step - loss: 1.4227 - accuracy: 0.7337 - val_loss: 1.4294 - val_accuracy: 0.8000\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 1.3726 - accuracy: 0.7350 - val_loss: 1.3679 - val_accuracy: 0.8150\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 8s 169ms/step - loss: 1.2749 - accuracy: 0.7750 - val_loss: 1.3090 - val_accuracy: 0.8200\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 1.2345 - accuracy: 0.7900 - val_loss: 1.2565 - val_accuracy: 0.8300\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 1.1654 - accuracy: 0.8175 - val_loss: 1.2093 - val_accuracy: 0.8200\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 8s 164ms/step - loss: 1.1180 - accuracy: 0.8112 - val_loss: 1.1648 - val_accuracy: 0.8100\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 8s 163ms/step - loss: 1.0836 - accuracy: 0.8050 - val_loss: 1.1259 - val_accuracy: 0.8100\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 8s 163ms/step - loss: 1.0325 - accuracy: 0.8200 - val_loss: 1.0885 - val_accuracy: 0.8100\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 8s 164ms/step - loss: 1.0161 - accuracy: 0.8213 - val_loss: 1.0534 - val_accuracy: 0.8150\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.9821 - accuracy: 0.8100 - val_loss: 1.0232 - val_accuracy: 0.8200\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.9470 - accuracy: 0.8363 - val_loss: 0.9947 - val_accuracy: 0.8250\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.9226 - accuracy: 0.8250 - val_loss: 0.9704 - val_accuracy: 0.8300\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.8815 - accuracy: 0.8375 - val_loss: 0.9455 - val_accuracy: 0.8350\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 8s 164ms/step - loss: 0.8579 - accuracy: 0.8438 - val_loss: 0.9206 - val_accuracy: 0.8350\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 8s 165ms/step - loss: 0.8347 - accuracy: 0.8575 - val_loss: 0.9025 - val_accuracy: 0.8250\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 8s 165ms/step - loss: 0.8249 - accuracy: 0.8313 - val_loss: 0.8833 - val_accuracy: 0.8350\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 8s 164ms/step - loss: 0.7783 - accuracy: 0.8438 - val_loss: 0.8618 - val_accuracy: 0.8400\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 8s 165ms/step - loss: 0.7627 - accuracy: 0.8487 - val_loss: 0.8468 - val_accuracy: 0.8300\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.7443 - accuracy: 0.8562 - val_loss: 0.8295 - val_accuracy: 0.8300\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.7357 - accuracy: 0.8537 - val_loss: 0.8109 - val_accuracy: 0.8300\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 8s 165ms/step - loss: 0.7129 - accuracy: 0.8537 - val_loss: 0.8018 - val_accuracy: 0.8300\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.6862 - accuracy: 0.8612 - val_loss: 0.7855 - val_accuracy: 0.8400\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.6818 - accuracy: 0.8612 - val_loss: 0.7742 - val_accuracy: 0.8350\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.6676 - accuracy: 0.8650 - val_loss: 0.7647 - val_accuracy: 0.8350\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.6810 - accuracy: 0.8475 - val_loss: 0.7546 - val_accuracy: 0.8400\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.6356 - accuracy: 0.8825 - val_loss: 0.7431 - val_accuracy: 0.8400\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.6356 - accuracy: 0.8612 - val_loss: 0.7332 - val_accuracy: 0.8400\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.6113 - accuracy: 0.8850 - val_loss: 0.7235 - val_accuracy: 0.8450\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 8s 165ms/step - loss: 0.6115 - accuracy: 0.8750 - val_loss: 0.7130 - val_accuracy: 0.8450\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 8s 165ms/step - loss: 0.6096 - accuracy: 0.8637 - val_loss: 0.7076 - val_accuracy: 0.8450\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 8s 165ms/step - loss: 0.5871 - accuracy: 0.8775 - val_loss: 0.6969 - val_accuracy: 0.8500\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.5796 - accuracy: 0.8712 - val_loss: 0.6915 - val_accuracy: 0.8450\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.5799 - accuracy: 0.8788 - val_loss: 0.6857 - val_accuracy: 0.8550\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5574 - accuracy: 0.8875 - val_loss: 0.6796 - val_accuracy: 0.8600\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5527 - accuracy: 0.8825 - val_loss: 0.6699 - val_accuracy: 0.8550\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.5386 - accuracy: 0.8938 - val_loss: 0.6647 - val_accuracy: 0.8600\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.5416 - accuracy: 0.8775 - val_loss: 0.6610 - val_accuracy: 0.8600\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.5066 - accuracy: 0.8888 - val_loss: 0.6528 - val_accuracy: 0.8650\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 8s 165ms/step - loss: 0.5141 - accuracy: 0.8838 - val_loss: 0.6463 - val_accuracy: 0.8750\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.5152 - accuracy: 0.8900 - val_loss: 0.6432 - val_accuracy: 0.8700\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 8s 165ms/step - loss: 0.5070 - accuracy: 0.8875 - val_loss: 0.6384 - val_accuracy: 0.8650\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 8s 165ms/step - loss: 0.4796 - accuracy: 0.8963 - val_loss: 0.6311 - val_accuracy: 0.8750\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 8s 165ms/step - loss: 0.4893 - accuracy: 0.8963 - val_loss: 0.6258 - val_accuracy: 0.8700\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.4624 - accuracy: 0.8988 - val_loss: 0.6197 - val_accuracy: 0.8750\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4724 - accuracy: 0.8950 - val_loss: 0.6177 - val_accuracy: 0.8750\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.4757 - accuracy: 0.9000 - val_loss: 0.6139 - val_accuracy: 0.8750\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.4727 - accuracy: 0.8950 - val_loss: 0.6094 - val_accuracy: 0.8750\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.4508 - accuracy: 0.9087 - val_loss: 0.6026 - val_accuracy: 0.8700\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.4475 - accuracy: 0.9038 - val_loss: 0.6012 - val_accuracy: 0.8700\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.4340 - accuracy: 0.9075 - val_loss: 0.5970 - val_accuracy: 0.8750\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.4165 - accuracy: 0.9125 - val_loss: 0.5952 - val_accuracy: 0.8750\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 8s 165ms/step - loss: 0.4213 - accuracy: 0.9150 - val_loss: 0.5911 - val_accuracy: 0.8750\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.4229 - accuracy: 0.9087 - val_loss: 0.5884 - val_accuracy: 0.8700\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.4407 - accuracy: 0.8925 - val_loss: 0.5863 - val_accuracy: 0.8750\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.4199 - accuracy: 0.9050 - val_loss: 0.5802 - val_accuracy: 0.8750\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.4165 - accuracy: 0.9150 - val_loss: 0.5779 - val_accuracy: 0.8700\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3965 - accuracy: 0.9162 - val_loss: 0.5726 - val_accuracy: 0.8700\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.4084 - accuracy: 0.9013 - val_loss: 0.5699 - val_accuracy: 0.8700\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3987 - accuracy: 0.9187 - val_loss: 0.5715 - val_accuracy: 0.8750\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 8s 165ms/step - loss: 0.3847 - accuracy: 0.9250 - val_loss: 0.5691 - val_accuracy: 0.8700\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3743 - accuracy: 0.9337 - val_loss: 0.5649 - val_accuracy: 0.8700\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3852 - accuracy: 0.9187 - val_loss: 0.5627 - val_accuracy: 0.8700\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3771 - accuracy: 0.9187 - val_loss: 0.5604 - val_accuracy: 0.8700\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3745 - accuracy: 0.9212 - val_loss: 0.5569 - val_accuracy: 0.8700\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 8s 165ms/step - loss: 0.3776 - accuracy: 0.9212 - val_loss: 0.5578 - val_accuracy: 0.8750\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3658 - accuracy: 0.9175 - val_loss: 0.5529 - val_accuracy: 0.8750\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3716 - accuracy: 0.9162 - val_loss: 0.5516 - val_accuracy: 0.8750\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3500 - accuracy: 0.9325 - val_loss: 0.5472 - val_accuracy: 0.8750\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3594 - accuracy: 0.9125 - val_loss: 0.5502 - val_accuracy: 0.8800\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3364 - accuracy: 0.9287 - val_loss: 0.5454 - val_accuracy: 0.8750\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3498 - accuracy: 0.9287 - val_loss: 0.5441 - val_accuracy: 0.8750\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3459 - accuracy: 0.9275 - val_loss: 0.5419 - val_accuracy: 0.8800\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3442 - accuracy: 0.9275 - val_loss: 0.5401 - val_accuracy: 0.8800\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3338 - accuracy: 0.9275 - val_loss: 0.5373 - val_accuracy: 0.8800\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3462 - accuracy: 0.9162 - val_loss: 0.5353 - val_accuracy: 0.8800\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3234 - accuracy: 0.9275 - val_loss: 0.5339 - val_accuracy: 0.8750\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3342 - accuracy: 0.9388 - val_loss: 0.5321 - val_accuracy: 0.8750\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3191 - accuracy: 0.9262 - val_loss: 0.5313 - val_accuracy: 0.8750\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3216 - accuracy: 0.9388 - val_loss: 0.5284 - val_accuracy: 0.8700\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3072 - accuracy: 0.9275 - val_loss: 0.5315 - val_accuracy: 0.8700\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3187 - accuracy: 0.9275 - val_loss: 0.5286 - val_accuracy: 0.8650\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3023 - accuracy: 0.9413 - val_loss: 0.5271 - val_accuracy: 0.8700\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3071 - accuracy: 0.9300 - val_loss: 0.5246 - val_accuracy: 0.8700\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3151 - accuracy: 0.9275 - val_loss: 0.5250 - val_accuracy: 0.8700\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3102 - accuracy: 0.9350 - val_loss: 0.5210 - val_accuracy: 0.8700\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.2977 - accuracy: 0.9337 - val_loss: 0.5217 - val_accuracy: 0.8750\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.2982 - accuracy: 0.9325 - val_loss: 0.5185 - val_accuracy: 0.8700\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.2935 - accuracy: 0.9325 - val_loss: 0.5172 - val_accuracy: 0.8700\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3041 - accuracy: 0.9312 - val_loss: 0.5181 - val_accuracy: 0.8700\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3071 - accuracy: 0.9262 - val_loss: 0.5202 - val_accuracy: 0.8700\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.2821 - accuracy: 0.9425 - val_loss: 0.5178 - val_accuracy: 0.8700\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.2693 - accuracy: 0.9463 - val_loss: 0.5162 - val_accuracy: 0.8700\n",
            "fold 2\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_1 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_1 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_1 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "xception (Functional)        (None, 10, 10, 2048)      20861480  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 20,881,970\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 20,861,480\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 1s 115ms/step - loss: 2.3780 - accuracy: 0.1000\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 8s 164ms/step - loss: 2.2910 - accuracy: 0.1488 - val_loss: 2.2311 - val_accuracy: 0.2150\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 2.1526 - accuracy: 0.2600 - val_loss: 2.0978 - val_accuracy: 0.3300\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 2.0172 - accuracy: 0.4025 - val_loss: 1.9751 - val_accuracy: 0.4500\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 1.8905 - accuracy: 0.5263 - val_loss: 1.8629 - val_accuracy: 0.5300\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 8s 165ms/step - loss: 1.7735 - accuracy: 0.5850 - val_loss: 1.7574 - val_accuracy: 0.6000\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 8s 165ms/step - loss: 1.6846 - accuracy: 0.6350 - val_loss: 1.6613 - val_accuracy: 0.6300\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 8s 165ms/step - loss: 1.5946 - accuracy: 0.6600 - val_loss: 1.5753 - val_accuracy: 0.6750\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 8s 165ms/step - loss: 1.4941 - accuracy: 0.7262 - val_loss: 1.4952 - val_accuracy: 0.6800\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 8s 165ms/step - loss: 1.4195 - accuracy: 0.7550 - val_loss: 1.4230 - val_accuracy: 0.7050\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 1.3575 - accuracy: 0.7387 - val_loss: 1.3583 - val_accuracy: 0.7200\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 1.2878 - accuracy: 0.7763 - val_loss: 1.2999 - val_accuracy: 0.7350\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 1.2349 - accuracy: 0.7875 - val_loss: 1.2458 - val_accuracy: 0.7450\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 1.1782 - accuracy: 0.7875 - val_loss: 1.1988 - val_accuracy: 0.7450\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 1.1298 - accuracy: 0.7950 - val_loss: 1.1559 - val_accuracy: 0.7550\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 1.0847 - accuracy: 0.8050 - val_loss: 1.1164 - val_accuracy: 0.7500\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 1.0363 - accuracy: 0.8350 - val_loss: 1.0781 - val_accuracy: 0.7700\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 1.0005 - accuracy: 0.8300 - val_loss: 1.0458 - val_accuracy: 0.7700\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.9766 - accuracy: 0.8325 - val_loss: 1.0131 - val_accuracy: 0.7750\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 8s 165ms/step - loss: 0.9249 - accuracy: 0.8537 - val_loss: 0.9842 - val_accuracy: 0.7750\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.9213 - accuracy: 0.8313 - val_loss: 0.9573 - val_accuracy: 0.7750\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 8s 165ms/step - loss: 0.8675 - accuracy: 0.8500 - val_loss: 0.9316 - val_accuracy: 0.7800\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.8534 - accuracy: 0.8462 - val_loss: 0.9117 - val_accuracy: 0.7800\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.8218 - accuracy: 0.8537 - val_loss: 0.8908 - val_accuracy: 0.7850\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.8034 - accuracy: 0.8600 - val_loss: 0.8735 - val_accuracy: 0.7850\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.8082 - accuracy: 0.8525 - val_loss: 0.8539 - val_accuracy: 0.7850\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.7627 - accuracy: 0.8500 - val_loss: 0.8355 - val_accuracy: 0.7850\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.7563 - accuracy: 0.8400 - val_loss: 0.8211 - val_accuracy: 0.7850\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.7251 - accuracy: 0.8550 - val_loss: 0.8058 - val_accuracy: 0.7900\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.6989 - accuracy: 0.8800 - val_loss: 0.7927 - val_accuracy: 0.7900\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.7061 - accuracy: 0.8675 - val_loss: 0.7759 - val_accuracy: 0.7900\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.6955 - accuracy: 0.8600 - val_loss: 0.7653 - val_accuracy: 0.7950\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.6690 - accuracy: 0.8612 - val_loss: 0.7552 - val_accuracy: 0.7950\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.6540 - accuracy: 0.8662 - val_loss: 0.7451 - val_accuracy: 0.8000\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.6428 - accuracy: 0.8612 - val_loss: 0.7382 - val_accuracy: 0.7950\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.6229 - accuracy: 0.8850 - val_loss: 0.7248 - val_accuracy: 0.8000\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.6181 - accuracy: 0.8650 - val_loss: 0.7134 - val_accuracy: 0.8050\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.6152 - accuracy: 0.8737 - val_loss: 0.7062 - val_accuracy: 0.8000\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5978 - accuracy: 0.8737 - val_loss: 0.6978 - val_accuracy: 0.8000\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5891 - accuracy: 0.8825 - val_loss: 0.6878 - val_accuracy: 0.8000\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5796 - accuracy: 0.8975 - val_loss: 0.6809 - val_accuracy: 0.8050\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5708 - accuracy: 0.8888 - val_loss: 0.6739 - val_accuracy: 0.8000\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.5669 - accuracy: 0.8875 - val_loss: 0.6668 - val_accuracy: 0.8050\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5390 - accuracy: 0.8950 - val_loss: 0.6594 - val_accuracy: 0.8050\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.5445 - accuracy: 0.8850 - val_loss: 0.6527 - val_accuracy: 0.8000\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5177 - accuracy: 0.8950 - val_loss: 0.6476 - val_accuracy: 0.8000\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5109 - accuracy: 0.8938 - val_loss: 0.6402 - val_accuracy: 0.8050\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5145 - accuracy: 0.9013 - val_loss: 0.6345 - val_accuracy: 0.8000\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4971 - accuracy: 0.9050 - val_loss: 0.6286 - val_accuracy: 0.8000\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4918 - accuracy: 0.8975 - val_loss: 0.6224 - val_accuracy: 0.8000\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4780 - accuracy: 0.9087 - val_loss: 0.6157 - val_accuracy: 0.7950\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4737 - accuracy: 0.9137 - val_loss: 0.6112 - val_accuracy: 0.7950\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4888 - accuracy: 0.8913 - val_loss: 0.6045 - val_accuracy: 0.8000\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4615 - accuracy: 0.9013 - val_loss: 0.6032 - val_accuracy: 0.7950\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4688 - accuracy: 0.9075 - val_loss: 0.5973 - val_accuracy: 0.8050\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4490 - accuracy: 0.9125 - val_loss: 0.5950 - val_accuracy: 0.7950\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4604 - accuracy: 0.8913 - val_loss: 0.5906 - val_accuracy: 0.8000\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.4332 - accuracy: 0.9050 - val_loss: 0.5873 - val_accuracy: 0.7950\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4471 - accuracy: 0.9087 - val_loss: 0.5842 - val_accuracy: 0.8000\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4284 - accuracy: 0.9112 - val_loss: 0.5783 - val_accuracy: 0.8050\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.4202 - accuracy: 0.9162 - val_loss: 0.5762 - val_accuracy: 0.8000\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4334 - accuracy: 0.9225 - val_loss: 0.5723 - val_accuracy: 0.8000\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.4081 - accuracy: 0.9187 - val_loss: 0.5694 - val_accuracy: 0.8000\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4087 - accuracy: 0.9275 - val_loss: 0.5671 - val_accuracy: 0.8050\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.4116 - accuracy: 0.9112 - val_loss: 0.5628 - val_accuracy: 0.8050\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3985 - accuracy: 0.9087 - val_loss: 0.5606 - val_accuracy: 0.8050\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3883 - accuracy: 0.9237 - val_loss: 0.5594 - val_accuracy: 0.8050\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3912 - accuracy: 0.9212 - val_loss: 0.5563 - val_accuracy: 0.8050\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3958 - accuracy: 0.9237 - val_loss: 0.5535 - val_accuracy: 0.8000\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4007 - accuracy: 0.9013 - val_loss: 0.5500 - val_accuracy: 0.8000\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3705 - accuracy: 0.9150 - val_loss: 0.5450 - val_accuracy: 0.8050\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3703 - accuracy: 0.9237 - val_loss: 0.5432 - val_accuracy: 0.8050\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3636 - accuracy: 0.9262 - val_loss: 0.5428 - val_accuracy: 0.8000\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3680 - accuracy: 0.9225 - val_loss: 0.5404 - val_accuracy: 0.8000\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3728 - accuracy: 0.9162 - val_loss: 0.5366 - val_accuracy: 0.8000\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3559 - accuracy: 0.9262 - val_loss: 0.5344 - val_accuracy: 0.8050\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3565 - accuracy: 0.9312 - val_loss: 0.5311 - val_accuracy: 0.8050\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3428 - accuracy: 0.9337 - val_loss: 0.5295 - val_accuracy: 0.8050\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3387 - accuracy: 0.9388 - val_loss: 0.5291 - val_accuracy: 0.8000\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3314 - accuracy: 0.9350 - val_loss: 0.5258 - val_accuracy: 0.8050\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3423 - accuracy: 0.9262 - val_loss: 0.5247 - val_accuracy: 0.8000\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3262 - accuracy: 0.9350 - val_loss: 0.5224 - val_accuracy: 0.8050\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3358 - accuracy: 0.9325 - val_loss: 0.5220 - val_accuracy: 0.8100\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3370 - accuracy: 0.9237 - val_loss: 0.5181 - val_accuracy: 0.8100\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3452 - accuracy: 0.9275 - val_loss: 0.5187 - val_accuracy: 0.8050\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3342 - accuracy: 0.9287 - val_loss: 0.5159 - val_accuracy: 0.8150\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3113 - accuracy: 0.9488 - val_loss: 0.5123 - val_accuracy: 0.8150\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3178 - accuracy: 0.9362 - val_loss: 0.5122 - val_accuracy: 0.8150\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3223 - accuracy: 0.9388 - val_loss: 0.5108 - val_accuracy: 0.8150\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.2920 - accuracy: 0.9388 - val_loss: 0.5093 - val_accuracy: 0.8200\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3250 - accuracy: 0.9312 - val_loss: 0.5045 - val_accuracy: 0.8150\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3152 - accuracy: 0.9375 - val_loss: 0.5047 - val_accuracy: 0.8150\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3121 - accuracy: 0.9312 - val_loss: 0.5027 - val_accuracy: 0.8150\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3021 - accuracy: 0.9400 - val_loss: 0.5013 - val_accuracy: 0.8200\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3047 - accuracy: 0.9262 - val_loss: 0.5041 - val_accuracy: 0.8100\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.2993 - accuracy: 0.9337 - val_loss: 0.5019 - val_accuracy: 0.8200\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3124 - accuracy: 0.9350 - val_loss: 0.4995 - val_accuracy: 0.8250\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.2895 - accuracy: 0.9388 - val_loss: 0.4975 - val_accuracy: 0.8200\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.2974 - accuracy: 0.9413 - val_loss: 0.4969 - val_accuracy: 0.8200\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.2662 - accuracy: 0.9488 - val_loss: 0.4951 - val_accuracy: 0.8200\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.2764 - accuracy: 0.9438 - val_loss: 0.4964 - val_accuracy: 0.8150\n",
            "fold 3\n",
            "Model: \"functional_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_2 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_2 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_2 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "xception (Functional)        (None, 10, 10, 2048)      20861480  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 20,881,970\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 20,861,480\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 1s 112ms/step - loss: 2.3113 - accuracy: 0.1350\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 8s 165ms/step - loss: 2.2387 - accuracy: 0.2037 - val_loss: 2.1661 - val_accuracy: 0.2700\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 2.0983 - accuracy: 0.3088 - val_loss: 2.0492 - val_accuracy: 0.3850\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 1.9548 - accuracy: 0.4500 - val_loss: 1.9420 - val_accuracy: 0.4700\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 1.8477 - accuracy: 0.5500 - val_loss: 1.8417 - val_accuracy: 0.5650\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 1.7423 - accuracy: 0.5763 - val_loss: 1.7484 - val_accuracy: 0.6100\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 1.6527 - accuracy: 0.6112 - val_loss: 1.6648 - val_accuracy: 0.6450\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 1.5624 - accuracy: 0.6725 - val_loss: 1.5875 - val_accuracy: 0.6850\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 1.4737 - accuracy: 0.6988 - val_loss: 1.5175 - val_accuracy: 0.6800\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 1.3940 - accuracy: 0.7325 - val_loss: 1.4505 - val_accuracy: 0.7150\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 1.3257 - accuracy: 0.7613 - val_loss: 1.3909 - val_accuracy: 0.7450\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 1.2754 - accuracy: 0.7700 - val_loss: 1.3381 - val_accuracy: 0.7450\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 1.2172 - accuracy: 0.7788 - val_loss: 1.2870 - val_accuracy: 0.7550\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 1.1661 - accuracy: 0.7837 - val_loss: 1.2441 - val_accuracy: 0.7850\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 1.1000 - accuracy: 0.8075 - val_loss: 1.2026 - val_accuracy: 0.7900\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 1.0699 - accuracy: 0.7925 - val_loss: 1.1652 - val_accuracy: 0.7950\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 1.0224 - accuracy: 0.8075 - val_loss: 1.1275 - val_accuracy: 0.7850\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.9962 - accuracy: 0.8213 - val_loss: 1.0970 - val_accuracy: 0.7900\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.9617 - accuracy: 0.8100 - val_loss: 1.0678 - val_accuracy: 0.7900\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.9323 - accuracy: 0.8200 - val_loss: 1.0379 - val_accuracy: 0.8000\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.9086 - accuracy: 0.8263 - val_loss: 1.0150 - val_accuracy: 0.8000\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.8862 - accuracy: 0.8363 - val_loss: 0.9902 - val_accuracy: 0.7950\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.8673 - accuracy: 0.8325 - val_loss: 0.9700 - val_accuracy: 0.8000\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.8236 - accuracy: 0.8388 - val_loss: 0.9498 - val_accuracy: 0.8000\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.8071 - accuracy: 0.8350 - val_loss: 0.9311 - val_accuracy: 0.7950\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.7924 - accuracy: 0.8425 - val_loss: 0.9109 - val_accuracy: 0.8050\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.7622 - accuracy: 0.8388 - val_loss: 0.8912 - val_accuracy: 0.8000\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.7458 - accuracy: 0.8537 - val_loss: 0.8812 - val_accuracy: 0.7900\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.7127 - accuracy: 0.8625 - val_loss: 0.8653 - val_accuracy: 0.7900\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.6908 - accuracy: 0.8625 - val_loss: 0.8508 - val_accuracy: 0.8000\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.7178 - accuracy: 0.8413 - val_loss: 0.8372 - val_accuracy: 0.8050\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.6850 - accuracy: 0.8512 - val_loss: 0.8225 - val_accuracy: 0.8050\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.6668 - accuracy: 0.8800 - val_loss: 0.8123 - val_accuracy: 0.8100\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.6523 - accuracy: 0.8700 - val_loss: 0.8027 - val_accuracy: 0.8100\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.6317 - accuracy: 0.8838 - val_loss: 0.7901 - val_accuracy: 0.8150\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.6405 - accuracy: 0.8700 - val_loss: 0.7836 - val_accuracy: 0.8150\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.6172 - accuracy: 0.8788 - val_loss: 0.7704 - val_accuracy: 0.8150\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.6183 - accuracy: 0.8763 - val_loss: 0.7615 - val_accuracy: 0.8150\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.5963 - accuracy: 0.8863 - val_loss: 0.7515 - val_accuracy: 0.8100\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.5737 - accuracy: 0.8775 - val_loss: 0.7444 - val_accuracy: 0.8100\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5660 - accuracy: 0.8838 - val_loss: 0.7361 - val_accuracy: 0.8200\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.5660 - accuracy: 0.8888 - val_loss: 0.7273 - val_accuracy: 0.8200\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.5555 - accuracy: 0.8875 - val_loss: 0.7197 - val_accuracy: 0.8100\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5295 - accuracy: 0.8850 - val_loss: 0.7127 - val_accuracy: 0.8200\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5383 - accuracy: 0.8925 - val_loss: 0.7057 - val_accuracy: 0.8100\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5403 - accuracy: 0.8838 - val_loss: 0.6984 - val_accuracy: 0.8150\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5240 - accuracy: 0.8950 - val_loss: 0.6909 - val_accuracy: 0.8200\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.5129 - accuracy: 0.8825 - val_loss: 0.6881 - val_accuracy: 0.8200\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4937 - accuracy: 0.8950 - val_loss: 0.6840 - val_accuracy: 0.8200\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5012 - accuracy: 0.8875 - val_loss: 0.6766 - val_accuracy: 0.8250\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4857 - accuracy: 0.8938 - val_loss: 0.6720 - val_accuracy: 0.8150\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4726 - accuracy: 0.8950 - val_loss: 0.6673 - val_accuracy: 0.8200\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4777 - accuracy: 0.8875 - val_loss: 0.6613 - val_accuracy: 0.8200\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4779 - accuracy: 0.8988 - val_loss: 0.6574 - val_accuracy: 0.8200\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.4618 - accuracy: 0.8975 - val_loss: 0.6530 - val_accuracy: 0.8200\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.4567 - accuracy: 0.9075 - val_loss: 0.6476 - val_accuracy: 0.8200\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.4565 - accuracy: 0.8850 - val_loss: 0.6434 - val_accuracy: 0.8150\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4453 - accuracy: 0.9075 - val_loss: 0.6373 - val_accuracy: 0.8150\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4317 - accuracy: 0.9038 - val_loss: 0.6352 - val_accuracy: 0.8200\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.4292 - accuracy: 0.9075 - val_loss: 0.6340 - val_accuracy: 0.8100\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4182 - accuracy: 0.9075 - val_loss: 0.6272 - val_accuracy: 0.8200\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4338 - accuracy: 0.8925 - val_loss: 0.6253 - val_accuracy: 0.8200\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4220 - accuracy: 0.9075 - val_loss: 0.6194 - val_accuracy: 0.8250\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4212 - accuracy: 0.9100 - val_loss: 0.6155 - val_accuracy: 0.8200\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3971 - accuracy: 0.9212 - val_loss: 0.6119 - val_accuracy: 0.8250\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3934 - accuracy: 0.9100 - val_loss: 0.6111 - val_accuracy: 0.8200\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3939 - accuracy: 0.9112 - val_loss: 0.6064 - val_accuracy: 0.8200\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3798 - accuracy: 0.9325 - val_loss: 0.6064 - val_accuracy: 0.8250\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3996 - accuracy: 0.9112 - val_loss: 0.5992 - val_accuracy: 0.8150\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3730 - accuracy: 0.9212 - val_loss: 0.5966 - val_accuracy: 0.8200\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3894 - accuracy: 0.9112 - val_loss: 0.5956 - val_accuracy: 0.8300\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3664 - accuracy: 0.9325 - val_loss: 0.5938 - val_accuracy: 0.8200\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3696 - accuracy: 0.9212 - val_loss: 0.5928 - val_accuracy: 0.8300\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3726 - accuracy: 0.9162 - val_loss: 0.5916 - val_accuracy: 0.8250\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3655 - accuracy: 0.9225 - val_loss: 0.5878 - val_accuracy: 0.8200\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3677 - accuracy: 0.9100 - val_loss: 0.5843 - val_accuracy: 0.8300\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3524 - accuracy: 0.9312 - val_loss: 0.5790 - val_accuracy: 0.8300\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3592 - accuracy: 0.9162 - val_loss: 0.5794 - val_accuracy: 0.8350\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3526 - accuracy: 0.9225 - val_loss: 0.5753 - val_accuracy: 0.8350\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3509 - accuracy: 0.9312 - val_loss: 0.5741 - val_accuracy: 0.8350\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3416 - accuracy: 0.9275 - val_loss: 0.5702 - val_accuracy: 0.8350\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3447 - accuracy: 0.9212 - val_loss: 0.5693 - val_accuracy: 0.8400\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3168 - accuracy: 0.9325 - val_loss: 0.5675 - val_accuracy: 0.8350\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3339 - accuracy: 0.9200 - val_loss: 0.5643 - val_accuracy: 0.8350\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3269 - accuracy: 0.9287 - val_loss: 0.5613 - val_accuracy: 0.8350\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3309 - accuracy: 0.9225 - val_loss: 0.5582 - val_accuracy: 0.8350\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3177 - accuracy: 0.9325 - val_loss: 0.5579 - val_accuracy: 0.8300\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3158 - accuracy: 0.9275 - val_loss: 0.5556 - val_accuracy: 0.8350\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3201 - accuracy: 0.9287 - val_loss: 0.5562 - val_accuracy: 0.8300\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3097 - accuracy: 0.9325 - val_loss: 0.5560 - val_accuracy: 0.8300\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3136 - accuracy: 0.9312 - val_loss: 0.5534 - val_accuracy: 0.8350\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3104 - accuracy: 0.9300 - val_loss: 0.5530 - val_accuracy: 0.8300\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3049 - accuracy: 0.9375 - val_loss: 0.5492 - val_accuracy: 0.8300\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3210 - accuracy: 0.9362 - val_loss: 0.5488 - val_accuracy: 0.8300\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.2984 - accuracy: 0.9350 - val_loss: 0.5453 - val_accuracy: 0.8300\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.2968 - accuracy: 0.9450 - val_loss: 0.5481 - val_accuracy: 0.8300\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.2853 - accuracy: 0.9463 - val_loss: 0.5481 - val_accuracy: 0.8300\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3092 - accuracy: 0.9350 - val_loss: 0.5426 - val_accuracy: 0.8300\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.2881 - accuracy: 0.9400 - val_loss: 0.5456 - val_accuracy: 0.8300\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.2815 - accuracy: 0.9375 - val_loss: 0.5459 - val_accuracy: 0.8250\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.2998 - accuracy: 0.9388 - val_loss: 0.5433 - val_accuracy: 0.8250\n",
            "fold 4\n",
            "Model: \"functional_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_3 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_3 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_3 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "xception (Functional)        (None, 10, 10, 2048)      20861480  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_3 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 20,881,970\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 20,861,480\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 1s 113ms/step - loss: 2.4179 - accuracy: 0.1100\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 8s 165ms/step - loss: 2.3259 - accuracy: 0.1388 - val_loss: 2.2593 - val_accuracy: 0.1800\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 2.1727 - accuracy: 0.2275 - val_loss: 2.1300 - val_accuracy: 0.2950\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 8s 170ms/step - loss: 2.0328 - accuracy: 0.3725 - val_loss: 2.0137 - val_accuracy: 0.4000\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 8s 169ms/step - loss: 1.9101 - accuracy: 0.4750 - val_loss: 1.9051 - val_accuracy: 0.5150\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 1.8123 - accuracy: 0.5375 - val_loss: 1.8067 - val_accuracy: 0.5900\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 1.6939 - accuracy: 0.6087 - val_loss: 1.7140 - val_accuracy: 0.6300\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 1.5984 - accuracy: 0.6762 - val_loss: 1.6288 - val_accuracy: 0.6600\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 1.5169 - accuracy: 0.6888 - val_loss: 1.5534 - val_accuracy: 0.7100\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 1.4345 - accuracy: 0.7287 - val_loss: 1.4831 - val_accuracy: 0.7150\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 1.3497 - accuracy: 0.7525 - val_loss: 1.4198 - val_accuracy: 0.7350\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 1.3088 - accuracy: 0.7550 - val_loss: 1.3649 - val_accuracy: 0.7400\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 1.2372 - accuracy: 0.7688 - val_loss: 1.3100 - val_accuracy: 0.7450\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 1.2074 - accuracy: 0.7738 - val_loss: 1.2658 - val_accuracy: 0.7500\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 1.1418 - accuracy: 0.7975 - val_loss: 1.2203 - val_accuracy: 0.7650\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 1.0888 - accuracy: 0.8112 - val_loss: 1.1810 - val_accuracy: 0.7650\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 1.0463 - accuracy: 0.8112 - val_loss: 1.1447 - val_accuracy: 0.7700\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 1.0143 - accuracy: 0.8188 - val_loss: 1.1094 - val_accuracy: 0.7650\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.9826 - accuracy: 0.8225 - val_loss: 1.0788 - val_accuracy: 0.7550\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.9619 - accuracy: 0.8188 - val_loss: 1.0509 - val_accuracy: 0.7550\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.9198 - accuracy: 0.8275 - val_loss: 1.0253 - val_accuracy: 0.7550\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.8900 - accuracy: 0.8388 - val_loss: 0.9999 - val_accuracy: 0.7550\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.8574 - accuracy: 0.8438 - val_loss: 0.9785 - val_accuracy: 0.7700\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.8386 - accuracy: 0.8438 - val_loss: 0.9552 - val_accuracy: 0.7750\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.8065 - accuracy: 0.8450 - val_loss: 0.9370 - val_accuracy: 0.7750\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.7853 - accuracy: 0.8587 - val_loss: 0.9169 - val_accuracy: 0.7900\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.7704 - accuracy: 0.8550 - val_loss: 0.9018 - val_accuracy: 0.7850\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.7432 - accuracy: 0.8612 - val_loss: 0.8867 - val_accuracy: 0.7900\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.7317 - accuracy: 0.8575 - val_loss: 0.8717 - val_accuracy: 0.7850\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.6973 - accuracy: 0.8763 - val_loss: 0.8569 - val_accuracy: 0.7850\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.6989 - accuracy: 0.8813 - val_loss: 0.8443 - val_accuracy: 0.7850\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.6949 - accuracy: 0.8512 - val_loss: 0.8314 - val_accuracy: 0.7850\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.6621 - accuracy: 0.8750 - val_loss: 0.8203 - val_accuracy: 0.7850\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.6586 - accuracy: 0.8587 - val_loss: 0.8090 - val_accuracy: 0.7850\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.6420 - accuracy: 0.8650 - val_loss: 0.7987 - val_accuracy: 0.7850\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.6381 - accuracy: 0.8600 - val_loss: 0.7882 - val_accuracy: 0.7850\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.6240 - accuracy: 0.8675 - val_loss: 0.7779 - val_accuracy: 0.7850\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.6091 - accuracy: 0.8737 - val_loss: 0.7691 - val_accuracy: 0.7950\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.5979 - accuracy: 0.8800 - val_loss: 0.7629 - val_accuracy: 0.7850\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.5967 - accuracy: 0.8575 - val_loss: 0.7531 - val_accuracy: 0.7900\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.5812 - accuracy: 0.8838 - val_loss: 0.7450 - val_accuracy: 0.7900\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5647 - accuracy: 0.8838 - val_loss: 0.7382 - val_accuracy: 0.7850\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5548 - accuracy: 0.8875 - val_loss: 0.7273 - val_accuracy: 0.7950\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5366 - accuracy: 0.8788 - val_loss: 0.7233 - val_accuracy: 0.7950\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5439 - accuracy: 0.8750 - val_loss: 0.7169 - val_accuracy: 0.7950\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5213 - accuracy: 0.8788 - val_loss: 0.7091 - val_accuracy: 0.7950\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5278 - accuracy: 0.8788 - val_loss: 0.7040 - val_accuracy: 0.7950\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.5164 - accuracy: 0.8800 - val_loss: 0.6987 - val_accuracy: 0.7950\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.4999 - accuracy: 0.8850 - val_loss: 0.6921 - val_accuracy: 0.8000\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5076 - accuracy: 0.8850 - val_loss: 0.6878 - val_accuracy: 0.8000\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4839 - accuracy: 0.8900 - val_loss: 0.6830 - val_accuracy: 0.8000\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4852 - accuracy: 0.8900 - val_loss: 0.6766 - val_accuracy: 0.8000\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.4687 - accuracy: 0.9025 - val_loss: 0.6745 - val_accuracy: 0.8050\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4954 - accuracy: 0.8913 - val_loss: 0.6676 - val_accuracy: 0.8050\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4576 - accuracy: 0.8963 - val_loss: 0.6636 - val_accuracy: 0.8050\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.4428 - accuracy: 0.8975 - val_loss: 0.6575 - val_accuracy: 0.8000\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4508 - accuracy: 0.9050 - val_loss: 0.6535 - val_accuracy: 0.8000\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.4467 - accuracy: 0.8975 - val_loss: 0.6500 - val_accuracy: 0.8050\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.4384 - accuracy: 0.9000 - val_loss: 0.6442 - val_accuracy: 0.8100\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.4368 - accuracy: 0.8963 - val_loss: 0.6410 - val_accuracy: 0.8050\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4307 - accuracy: 0.9075 - val_loss: 0.6370 - val_accuracy: 0.8100\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.4232 - accuracy: 0.9087 - val_loss: 0.6340 - val_accuracy: 0.8150\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4374 - accuracy: 0.9038 - val_loss: 0.6295 - val_accuracy: 0.8100\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4245 - accuracy: 0.9100 - val_loss: 0.6253 - val_accuracy: 0.8250\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3969 - accuracy: 0.9075 - val_loss: 0.6224 - val_accuracy: 0.8250\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.4062 - accuracy: 0.9062 - val_loss: 0.6208 - val_accuracy: 0.8300\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4090 - accuracy: 0.9025 - val_loss: 0.6152 - val_accuracy: 0.8300\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3981 - accuracy: 0.9112 - val_loss: 0.6123 - val_accuracy: 0.8250\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3974 - accuracy: 0.9187 - val_loss: 0.6105 - val_accuracy: 0.8300\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3777 - accuracy: 0.9262 - val_loss: 0.6082 - val_accuracy: 0.8250\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3931 - accuracy: 0.9013 - val_loss: 0.6046 - val_accuracy: 0.8250\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3716 - accuracy: 0.9150 - val_loss: 0.6006 - val_accuracy: 0.8300\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3817 - accuracy: 0.9125 - val_loss: 0.5980 - val_accuracy: 0.8200\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3517 - accuracy: 0.9275 - val_loss: 0.5960 - val_accuracy: 0.8300\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3687 - accuracy: 0.9125 - val_loss: 0.5935 - val_accuracy: 0.8300\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3580 - accuracy: 0.9325 - val_loss: 0.5923 - val_accuracy: 0.8300\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3683 - accuracy: 0.9212 - val_loss: 0.5881 - val_accuracy: 0.8300\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3503 - accuracy: 0.9175 - val_loss: 0.5875 - val_accuracy: 0.8300\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3401 - accuracy: 0.9300 - val_loss: 0.5860 - val_accuracy: 0.8350\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3559 - accuracy: 0.9237 - val_loss: 0.5821 - val_accuracy: 0.8300\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3352 - accuracy: 0.9275 - val_loss: 0.5808 - val_accuracy: 0.8350\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3307 - accuracy: 0.9350 - val_loss: 0.5785 - val_accuracy: 0.8300\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3244 - accuracy: 0.9337 - val_loss: 0.5752 - val_accuracy: 0.8350\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3430 - accuracy: 0.9312 - val_loss: 0.5729 - val_accuracy: 0.8350\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3307 - accuracy: 0.9312 - val_loss: 0.5698 - val_accuracy: 0.8350\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 0.3165 - accuracy: 0.9362 - val_loss: 0.5697 - val_accuracy: 0.8350\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3160 - accuracy: 0.9275 - val_loss: 0.5696 - val_accuracy: 0.8350\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3238 - accuracy: 0.9337 - val_loss: 0.5686 - val_accuracy: 0.8350\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3152 - accuracy: 0.9337 - val_loss: 0.5658 - val_accuracy: 0.8300\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3050 - accuracy: 0.9362 - val_loss: 0.5637 - val_accuracy: 0.8450\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3250 - accuracy: 0.9325 - val_loss: 0.5626 - val_accuracy: 0.8400\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.2948 - accuracy: 0.9425 - val_loss: 0.5612 - val_accuracy: 0.8400\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3105 - accuracy: 0.9362 - val_loss: 0.5591 - val_accuracy: 0.8400\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3022 - accuracy: 0.9375 - val_loss: 0.5577 - val_accuracy: 0.8400\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3001 - accuracy: 0.9400 - val_loss: 0.5547 - val_accuracy: 0.8400\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3089 - accuracy: 0.9325 - val_loss: 0.5550 - val_accuracy: 0.8400\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.2952 - accuracy: 0.9350 - val_loss: 0.5531 - val_accuracy: 0.8400\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3004 - accuracy: 0.9337 - val_loss: 0.5496 - val_accuracy: 0.8400\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3086 - accuracy: 0.9375 - val_loss: 0.5502 - val_accuracy: 0.8400\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.2778 - accuracy: 0.9362 - val_loss: 0.5488 - val_accuracy: 0.8400\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.2834 - accuracy: 0.9388 - val_loss: 0.5475 - val_accuracy: 0.8400\n",
            "fold 5\n",
            "Model: \"functional_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_4 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_4 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_4 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "xception (Functional)        (None, 10, 10, 2048)      20861480  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_4 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 20,881,970\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 20,861,480\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 1s 114ms/step - loss: 2.3622 - accuracy: 0.0850\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 8s 165ms/step - loss: 2.2989 - accuracy: 0.1475 - val_loss: 2.2081 - val_accuracy: 0.1750\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 2.1490 - accuracy: 0.2525 - val_loss: 2.0803 - val_accuracy: 0.3300\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 8s 170ms/step - loss: 2.0188 - accuracy: 0.3825 - val_loss: 1.9618 - val_accuracy: 0.4900\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 1.8815 - accuracy: 0.5337 - val_loss: 1.8510 - val_accuracy: 0.6100\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 1.7817 - accuracy: 0.5888 - val_loss: 1.7491 - val_accuracy: 0.6650\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 1.6733 - accuracy: 0.6475 - val_loss: 1.6569 - val_accuracy: 0.7050\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 1.5754 - accuracy: 0.6825 - val_loss: 1.5713 - val_accuracy: 0.7350\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 1.4828 - accuracy: 0.7250 - val_loss: 1.4926 - val_accuracy: 0.7850\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 1.4112 - accuracy: 0.7350 - val_loss: 1.4246 - val_accuracy: 0.8050\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 1.3501 - accuracy: 0.7525 - val_loss: 1.3567 - val_accuracy: 0.8200\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 1.2900 - accuracy: 0.7812 - val_loss: 1.3012 - val_accuracy: 0.8150\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 1.2255 - accuracy: 0.7825 - val_loss: 1.2476 - val_accuracy: 0.8300\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 1.1629 - accuracy: 0.8012 - val_loss: 1.1991 - val_accuracy: 0.8200\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 1.1061 - accuracy: 0.8200 - val_loss: 1.1590 - val_accuracy: 0.8200\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 1.0820 - accuracy: 0.8025 - val_loss: 1.1166 - val_accuracy: 0.8250\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 1.0431 - accuracy: 0.8062 - val_loss: 1.0803 - val_accuracy: 0.8250\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 1.0095 - accuracy: 0.8163 - val_loss: 1.0475 - val_accuracy: 0.8200\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.9648 - accuracy: 0.8200 - val_loss: 1.0150 - val_accuracy: 0.8200\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.9209 - accuracy: 0.8438 - val_loss: 0.9863 - val_accuracy: 0.8350\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.8968 - accuracy: 0.8413 - val_loss: 0.9621 - val_accuracy: 0.8400\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.8766 - accuracy: 0.8438 - val_loss: 0.9368 - val_accuracy: 0.8400\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.8420 - accuracy: 0.8500 - val_loss: 0.9140 - val_accuracy: 0.8400\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.8253 - accuracy: 0.8450 - val_loss: 0.8949 - val_accuracy: 0.8400\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.8066 - accuracy: 0.8413 - val_loss: 0.8740 - val_accuracy: 0.8450\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.7843 - accuracy: 0.8550 - val_loss: 0.8557 - val_accuracy: 0.8450\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.7653 - accuracy: 0.8413 - val_loss: 0.8377 - val_accuracy: 0.8450\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.7620 - accuracy: 0.8462 - val_loss: 0.8211 - val_accuracy: 0.8400\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.7122 - accuracy: 0.8687 - val_loss: 0.8066 - val_accuracy: 0.8450\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.6888 - accuracy: 0.8650 - val_loss: 0.7919 - val_accuracy: 0.8450\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.6888 - accuracy: 0.8763 - val_loss: 0.7794 - val_accuracy: 0.8500\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.6767 - accuracy: 0.8600 - val_loss: 0.7664 - val_accuracy: 0.8550\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.6835 - accuracy: 0.8500 - val_loss: 0.7541 - val_accuracy: 0.8550\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.6714 - accuracy: 0.8537 - val_loss: 0.7423 - val_accuracy: 0.8550\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.6298 - accuracy: 0.8737 - val_loss: 0.7320 - val_accuracy: 0.8500\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.6267 - accuracy: 0.8687 - val_loss: 0.7234 - val_accuracy: 0.8550\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.6257 - accuracy: 0.8712 - val_loss: 0.7115 - val_accuracy: 0.8500\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.5889 - accuracy: 0.8863 - val_loss: 0.7040 - val_accuracy: 0.8500\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5899 - accuracy: 0.8838 - val_loss: 0.6977 - val_accuracy: 0.8500\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.5848 - accuracy: 0.8800 - val_loss: 0.6901 - val_accuracy: 0.8550\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5727 - accuracy: 0.8813 - val_loss: 0.6814 - val_accuracy: 0.8550\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5716 - accuracy: 0.8725 - val_loss: 0.6743 - val_accuracy: 0.8550\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5425 - accuracy: 0.8938 - val_loss: 0.6678 - val_accuracy: 0.8550\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5494 - accuracy: 0.8800 - val_loss: 0.6600 - val_accuracy: 0.8600\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.5394 - accuracy: 0.8800 - val_loss: 0.6515 - val_accuracy: 0.8600\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.5581 - accuracy: 0.8687 - val_loss: 0.6481 - val_accuracy: 0.8600\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5248 - accuracy: 0.8788 - val_loss: 0.6383 - val_accuracy: 0.8600\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5125 - accuracy: 0.8750 - val_loss: 0.6340 - val_accuracy: 0.8600\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5006 - accuracy: 0.8875 - val_loss: 0.6322 - val_accuracy: 0.8600\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.5044 - accuracy: 0.8875 - val_loss: 0.6238 - val_accuracy: 0.8600\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.4914 - accuracy: 0.8988 - val_loss: 0.6196 - val_accuracy: 0.8600\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.4708 - accuracy: 0.9062 - val_loss: 0.6136 - val_accuracy: 0.8550\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4871 - accuracy: 0.8888 - val_loss: 0.6082 - val_accuracy: 0.8500\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.4641 - accuracy: 0.8975 - val_loss: 0.6051 - val_accuracy: 0.8500\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.4683 - accuracy: 0.8925 - val_loss: 0.5992 - val_accuracy: 0.8500\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4615 - accuracy: 0.8963 - val_loss: 0.5987 - val_accuracy: 0.8500\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.4639 - accuracy: 0.9050 - val_loss: 0.5958 - val_accuracy: 0.8500\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4346 - accuracy: 0.9062 - val_loss: 0.5913 - val_accuracy: 0.8500\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4293 - accuracy: 0.9075 - val_loss: 0.5842 - val_accuracy: 0.8500\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4413 - accuracy: 0.9025 - val_loss: 0.5842 - val_accuracy: 0.8500\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4292 - accuracy: 0.9187 - val_loss: 0.5827 - val_accuracy: 0.8500\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.4168 - accuracy: 0.9150 - val_loss: 0.5782 - val_accuracy: 0.8500\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.4241 - accuracy: 0.9050 - val_loss: 0.5732 - val_accuracy: 0.8500\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3952 - accuracy: 0.9262 - val_loss: 0.5720 - val_accuracy: 0.8550\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3960 - accuracy: 0.9200 - val_loss: 0.5638 - val_accuracy: 0.8550\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.4122 - accuracy: 0.9175 - val_loss: 0.5620 - val_accuracy: 0.8550\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.4088 - accuracy: 0.9100 - val_loss: 0.5600 - val_accuracy: 0.8550\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3968 - accuracy: 0.9200 - val_loss: 0.5547 - val_accuracy: 0.8550\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3769 - accuracy: 0.9325 - val_loss: 0.5534 - val_accuracy: 0.8550\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3930 - accuracy: 0.9150 - val_loss: 0.5504 - val_accuracy: 0.8550\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3851 - accuracy: 0.9100 - val_loss: 0.5467 - val_accuracy: 0.8600\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3802 - accuracy: 0.9150 - val_loss: 0.5459 - val_accuracy: 0.8600\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3777 - accuracy: 0.9162 - val_loss: 0.5453 - val_accuracy: 0.8600\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3536 - accuracy: 0.9312 - val_loss: 0.5420 - val_accuracy: 0.8600\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3732 - accuracy: 0.9175 - val_loss: 0.5383 - val_accuracy: 0.8600\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3703 - accuracy: 0.9300 - val_loss: 0.5376 - val_accuracy: 0.8550\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3711 - accuracy: 0.9200 - val_loss: 0.5363 - val_accuracy: 0.8600\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3452 - accuracy: 0.9250 - val_loss: 0.5330 - val_accuracy: 0.8600\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3542 - accuracy: 0.9350 - val_loss: 0.5286 - val_accuracy: 0.8600\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3554 - accuracy: 0.9212 - val_loss: 0.5295 - val_accuracy: 0.8650\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3479 - accuracy: 0.9287 - val_loss: 0.5306 - val_accuracy: 0.8550\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3455 - accuracy: 0.9225 - val_loss: 0.5304 - val_accuracy: 0.8600\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3367 - accuracy: 0.9200 - val_loss: 0.5247 - val_accuracy: 0.8600\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3226 - accuracy: 0.9388 - val_loss: 0.5225 - val_accuracy: 0.8600\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3268 - accuracy: 0.9413 - val_loss: 0.5219 - val_accuracy: 0.8600\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3328 - accuracy: 0.9237 - val_loss: 0.5207 - val_accuracy: 0.8600\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3235 - accuracy: 0.9237 - val_loss: 0.5176 - val_accuracy: 0.8650\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3164 - accuracy: 0.9275 - val_loss: 0.5176 - val_accuracy: 0.8600\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3414 - accuracy: 0.9237 - val_loss: 0.5193 - val_accuracy: 0.8600\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3113 - accuracy: 0.9275 - val_loss: 0.5176 - val_accuracy: 0.8600\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3065 - accuracy: 0.9362 - val_loss: 0.5128 - val_accuracy: 0.8650\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.3057 - accuracy: 0.9413 - val_loss: 0.5133 - val_accuracy: 0.8650\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3004 - accuracy: 0.9325 - val_loss: 0.5121 - val_accuracy: 0.8600\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3224 - accuracy: 0.9262 - val_loss: 0.5110 - val_accuracy: 0.8600\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.2838 - accuracy: 0.9413 - val_loss: 0.5090 - val_accuracy: 0.8600\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3061 - accuracy: 0.9375 - val_loss: 0.5074 - val_accuracy: 0.8600\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.2942 - accuracy: 0.9362 - val_loss: 0.5084 - val_accuracy: 0.8600\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.2917 - accuracy: 0.9388 - val_loss: 0.5092 - val_accuracy: 0.8650\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.2924 - accuracy: 0.9413 - val_loss: 0.5027 - val_accuracy: 0.8600\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 8s 169ms/step - loss: 0.2844 - accuracy: 0.9400 - val_loss: 0.5018 - val_accuracy: 0.8600\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.2897 - accuracy: 0.9375 - val_loss: 0.5017 - val_accuracy: 0.8700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2E72C3O30fv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c41a8fdc-a87e-4711-cc8a-659421fa2a41"
      },
      "source": [
        "# cross-validated accuracy for pre-trained model before training\n",
        "print(\"Base model accuracy:\", np.mean(base_model_acc_list))\n",
        "# cross-validated accuracy after training\n",
        "print(\"Final accuracy:\", np.mean(final_acc_list))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Base model accuracy: 0.10100000202655793\n",
            "Final accuracy: 0.8519999980926514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dtn-03T_ZaRX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "ec220aa0-9d9d-47b8-8df2-f7ecf6e2ae2f"
      },
      "source": [
        "# plot graph of cross-validated accuracies\n",
        "acc = np.mean(history_map['accuracy'], axis=0)\n",
        "val_acc = np.mean(history_map['val_accuracy'], axis=0)\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftCYvBkKZmGa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "c25f9f1f-f67d-4879-86ac-60f8333865d0"
      },
      "source": [
        "# plot graph of cross-validated losses\n",
        "loss = np.mean(history_map['loss'], axis=0)\n",
        "val_loss = np.mean(history_map['val_loss'], axis=0)\n",
        "\n",
        "plt.figure(2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "#plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}