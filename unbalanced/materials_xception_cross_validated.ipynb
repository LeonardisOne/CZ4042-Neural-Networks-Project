{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "materials-xception-cross-validated.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMphk8rrpEt6qyhvAdE+bxB"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERKJLyg2z-Uq"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fIbsvD01EQm",
        "outputId": "4f1b4236-486d-45a8-905c-de3448d5eea3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pathlib\n",
        "\n",
        "# set path to FMD image directory\n",
        "data_dir = pathlib.Path(\"image\")\n",
        "\n",
        "# total no. of images in FMD dataset\n",
        "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
        "print(image_count)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL3rBqoe3sK5"
      },
      "source": [
        "# set batch size for training\n",
        "batch_size = 16\n",
        "\n",
        "# set dimensions to change images into for training\n",
        "# 299x299 images is used to train for consistency between different pre-trained models\n",
        "img_height = 299\n",
        "img_width = 299"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap_FpvbEqWrS"
      },
      "source": [
        "# create dataset from the image directory\n",
        "list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*.jpg'), shuffle=False)\n",
        "# shuffle the 1,000 images with the random seed value of 123 before training\n",
        "list_ds = list_ds.shuffle(image_count, seed=123, reshuffle_each_iteration=False)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFKuhNRxqxcB",
        "outputId": "d3e6445c-461e-4d05-c789-22ec3bff243c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# get class names from the folder names in data_dir\n",
        "class_names = np.array(sorted([item.name for item in data_dir.glob('*') if item.name != \"LICENSE.txt\"]))\n",
        "print(class_names)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['fabric' 'foliage' 'glass' 'leather' 'metal' 'paper' 'plastic' 'stone'\n",
            " 'water' 'wood']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACILObxurdXN"
      },
      "source": [
        "# split dataset into 5 equal sized parts for 5-fold cross validation\n",
        "A = list_ds.shard(num_shards=5, index=0)\n",
        "B = list_ds.shard(num_shards=5, index=1)\n",
        "C = list_ds.shard(num_shards=5, index=2)\n",
        "D = list_ds.shard(num_shards=5, index=3)\n",
        "E = list_ds.shard(num_shards=5, index=4)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9oYdHkhrwdz",
        "outputId": "50eddd93-dfd5-4c1e-cd58-66bd509ccbfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# check no. of samples in each partition is the same\n",
        "print(A.cardinality().numpy())\n",
        "print(B.cardinality().numpy())\n",
        "print(C.cardinality().numpy())\n",
        "print(D.cardinality().numpy())\n",
        "print(E.cardinality().numpy())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BdD3FMHtGRV"
      },
      "source": [
        "def get_label(file_path):\n",
        "  # convert the path to a list of path components\n",
        "  parts = tf.strings.split(file_path, os.path.sep)\n",
        "  # The second to last is the class-directory\n",
        "  one_hot = parts[-2] == class_names\n",
        "  # Integer encode the label\n",
        "  return tf.argmax(one_hot)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfWduaL4tKDV"
      },
      "source": [
        "def decode_img(img):\n",
        "  # convert the compressed string to a 3D uint8 tensor\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\n",
        "  # resize the image to the desired size\n",
        "  return tf.image.resize(img, [img_height, img_width])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bGGe0KltMTC"
      },
      "source": [
        "def process_path(file_path):\n",
        "  label = get_label(file_path)\n",
        "  # load the raw data from the file as a string\n",
        "  img = tf.io.read_file(file_path)\n",
        "  img = decode_img(img)\n",
        "  return img, label"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcyZLwRxt1dy"
      },
      "source": [
        "# prompt the tf.data runtime to tune the number of elements to prefetch dynamically at runtime\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkBqAQlHtblh"
      },
      "source": [
        "# use the path of image to load the image into each partition of the dataset\n",
        "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
        "A = A.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "B = B.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "C = C.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "D = D.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "E = E.map(process_path, num_parallel_calls=AUTOTUNE)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc9pmaQ5t9H5",
        "outputId": "986bb343-7c1d-48ba-b7c2-33d672f68f29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for image, label in A.take(1):\n",
        "  print(\"Image shape: \", image.numpy().shape)\n",
        "  print(\"Label: \", label.numpy())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image shape:  (299, 299, 3)\n",
            "Label:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_T2KNdGub1X"
      },
      "source": [
        "# shuffle, batch, and prefetch the dataset\n",
        "def configure_for_performance(ds):\n",
        "  ds = ds.cache()\n",
        "  ds = ds.shuffle(buffer_size=1000)\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
        "  return ds"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWcojoVRuok5"
      },
      "source": [
        "# map the dataset partitions to integers for building training/test sets during cross-validation\n",
        "ds_fold_dict = {0:A, 1:B, 2:C, 3:D, 4:E}"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx3xKoMZU9Yv"
      },
      "source": [
        "# normalise the input values to the pre-trained model's required range of values\n",
        "preprocess_input = keras.applications.xception.preprocess_input"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVOP5fIPwEx8",
        "outputId": "863c4d1c-b4d1-4595-d8ba-0e590e980587",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# get pre-trained model\n",
        "base_model = keras.applications.Xception(include_top=False, input_shape=(img_height, img_width, 3))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS2kAceGVW0e"
      },
      "source": [
        "# don't train base model weights\n",
        "base_model.trainable = False"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGkReMX60ScJ",
        "outputId": "b134d99b-32ba-4171-d4cd-acaa140df42f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "base_model.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"xception\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 299, 299, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1 (Conv2D)           (None, 149, 149, 32) 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_bn (BatchNormaliza (None, 149, 149, 32) 128         block1_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_act (Activation)   (None, 149, 149, 32) 0           block1_conv1_bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2 (Conv2D)           (None, 147, 147, 64) 18432       block1_conv1_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_bn (BatchNormaliza (None, 147, 147, 64) 256         block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_act (Activation)   (None, 147, 147, 64) 0           block1_conv2_bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv1 (SeparableConv2 (None, 147, 147, 128 8768        block1_conv2_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv1_bn (BatchNormal (None, 147, 147, 128 512         block2_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2_act (Activation (None, 147, 147, 128 0           block2_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2 (SeparableConv2 (None, 147, 147, 128 17536       block2_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2_bn (BatchNormal (None, 147, 147, 128 512         block2_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 74, 74, 128)  8192        block1_conv2_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool (MaxPooling2D)      (None, 74, 74, 128)  0           block2_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 74, 74, 128)  512         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 74, 74, 128)  0           block2_pool[0][0]                \n",
            "                                                                 batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1_act (Activation (None, 74, 74, 128)  0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1 (SeparableConv2 (None, 74, 74, 256)  33920       block3_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1_bn (BatchNormal (None, 74, 74, 256)  1024        block3_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2_act (Activation (None, 74, 74, 256)  0           block3_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2 (SeparableConv2 (None, 74, 74, 256)  67840       block3_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2_bn (BatchNormal (None, 74, 74, 256)  1024        block3_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 37, 37, 256)  32768       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool (MaxPooling2D)      (None, 37, 37, 256)  0           block3_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 37, 37, 256)  1024        conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 37, 37, 256)  0           block3_pool[0][0]                \n",
            "                                                                 batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1_act (Activation (None, 37, 37, 256)  0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1 (SeparableConv2 (None, 37, 37, 728)  188672      block4_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1_bn (BatchNormal (None, 37, 37, 728)  2912        block4_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2_act (Activation (None, 37, 37, 728)  0           block4_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2 (SeparableConv2 (None, 37, 37, 728)  536536      block4_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2_bn (BatchNormal (None, 37, 37, 728)  2912        block4_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 19, 19, 728)  186368      add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool (MaxPooling2D)      (None, 19, 19, 728)  0           block4_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 19, 19, 728)  2912        conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 19, 19, 728)  0           block4_pool[0][0]                \n",
            "                                                                 batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1_act (Activation (None, 19, 19, 728)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block5_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block5_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2_act (Activation (None, 19, 19, 728)  0           block5_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block5_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block5_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3_act (Activation (None, 19, 19, 728)  0           block5_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block5_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block5_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 19, 19, 728)  0           block5_sepconv3_bn[0][0]         \n",
            "                                                                 add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1_act (Activation (None, 19, 19, 728)  0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block6_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block6_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2_act (Activation (None, 19, 19, 728)  0           block6_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block6_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block6_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3_act (Activation (None, 19, 19, 728)  0           block6_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block6_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block6_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 19, 19, 728)  0           block6_sepconv3_bn[0][0]         \n",
            "                                                                 add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1_act (Activation (None, 19, 19, 728)  0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block7_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block7_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2_act (Activation (None, 19, 19, 728)  0           block7_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block7_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block7_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3_act (Activation (None, 19, 19, 728)  0           block7_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block7_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block7_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 19, 19, 728)  0           block7_sepconv3_bn[0][0]         \n",
            "                                                                 add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1_act (Activation (None, 19, 19, 728)  0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block8_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block8_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2_act (Activation (None, 19, 19, 728)  0           block8_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block8_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block8_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3_act (Activation (None, 19, 19, 728)  0           block8_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block8_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block8_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 19, 19, 728)  0           block8_sepconv3_bn[0][0]         \n",
            "                                                                 add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1_act (Activation (None, 19, 19, 728)  0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block9_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block9_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2_act (Activation (None, 19, 19, 728)  0           block9_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block9_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block9_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3_act (Activation (None, 19, 19, 728)  0           block9_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block9_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block9_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 19, 19, 728)  0           block9_sepconv3_bn[0][0]         \n",
            "                                                                 add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block10_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block10_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2_act (Activatio (None, 19, 19, 728)  0           block10_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2 (SeparableConv (None, 19, 19, 728)  536536      block10_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2_bn (BatchNorma (None, 19, 19, 728)  2912        block10_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3_act (Activatio (None, 19, 19, 728)  0           block10_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3 (SeparableConv (None, 19, 19, 728)  536536      block10_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3_bn (BatchNorma (None, 19, 19, 728)  2912        block10_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 19, 19, 728)  0           block10_sepconv3_bn[0][0]        \n",
            "                                                                 add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block11_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block11_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2_act (Activatio (None, 19, 19, 728)  0           block11_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2 (SeparableConv (None, 19, 19, 728)  536536      block11_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2_bn (BatchNorma (None, 19, 19, 728)  2912        block11_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3_act (Activatio (None, 19, 19, 728)  0           block11_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3 (SeparableConv (None, 19, 19, 728)  536536      block11_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3_bn (BatchNorma (None, 19, 19, 728)  2912        block11_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 19, 19, 728)  0           block11_sepconv3_bn[0][0]        \n",
            "                                                                 add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block12_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block12_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2_act (Activatio (None, 19, 19, 728)  0           block12_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2 (SeparableConv (None, 19, 19, 728)  536536      block12_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2_bn (BatchNorma (None, 19, 19, 728)  2912        block12_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3_act (Activatio (None, 19, 19, 728)  0           block12_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3 (SeparableConv (None, 19, 19, 728)  536536      block12_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3_bn (BatchNorma (None, 19, 19, 728)  2912        block12_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 19, 19, 728)  0           block12_sepconv3_bn[0][0]        \n",
            "                                                                 add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block13_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block13_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2_act (Activatio (None, 19, 19, 728)  0           block13_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2 (SeparableConv (None, 19, 19, 1024) 752024      block13_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2_bn (BatchNorma (None, 19, 19, 1024) 4096        block13_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 10, 10, 1024) 745472      add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_pool (MaxPooling2D)     (None, 10, 10, 1024) 0           block13_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 10, 10, 1024) 4096        conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 10, 10, 1024) 0           block13_pool[0][0]               \n",
            "                                                                 batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1 (SeparableConv (None, 10, 10, 1536) 1582080     add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1_bn (BatchNorma (None, 10, 10, 1536) 6144        block14_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1_act (Activatio (None, 10, 10, 1536) 0           block14_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2 (SeparableConv (None, 10, 10, 2048) 3159552     block14_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2_bn (BatchNorma (None, 10, 10, 2048) 8192        block14_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2_act (Activatio (None, 10, 10, 2048) 0           block14_sepconv2_bn[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 20,861,480\n",
            "Trainable params: 0\n",
            "Non-trainable params: 20,861,480\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzhaWb2aX2if"
      },
      "source": [
        "# set a low learning rate to avoid overfitting too quickly\n",
        "base_learning_rate = 0.0001\n",
        "\n",
        "# create the model to train using the pre-trained model as base model\n",
        "def create_model():\n",
        "  # generate additional training data from input training data by augmenting them using random flip, rotation & zoom\n",
        "  data_augmentation = keras.Sequential(\n",
        "    [\n",
        "      layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n",
        "                                                  input_shape=(img_height, \n",
        "                                                                img_width,\n",
        "                                                                3)),\n",
        "      layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "      layers.experimental.preprocessing.RandomZoom(0.1),\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  # average over the spatial locations to convert the features to a single vector per image\n",
        "  global_average_layer = keras.layers.GlobalAveragePooling2D()\n",
        "  # convert these features into a single prediction per image\n",
        "  prediction_layer = keras.layers.Dense(10)\n",
        "\n",
        "  # Build a model by chaining together the layers using the Keras Functional API.\n",
        "  inputs = keras.Input(shape=(img_height, img_width, 3))\n",
        "  x = data_augmentation(inputs)\n",
        "  x = preprocess_input(x)\n",
        "  x = base_model(x, training=False) # use training=False as the base model contains a BatchNormalization layer\n",
        "  x = global_average_layer(x)\n",
        "  x = keras.layers.Dropout(0.2)(x) # add dropout to fully connected layer to reduce overfitting\n",
        "  outputs = prediction_layer(x)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  optimizer = keras.optimizers.Adam(lr=base_learning_rate)\n",
        "  loss= keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  # compile model with Adam optimizer with specified learning rate\n",
        "  model.compile(optimizer=optimizer,\n",
        "                loss=loss,\n",
        "                metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-5TEZRgY2l_"
      },
      "source": [
        "no_epochs = 100"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya698zRbu7Ep",
        "outputId": "d3348f59-f3cd-486a-dcb5-98c30502dfde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# store results to plot graphs and get cross-validated accuracies\n",
        "history_map = {}\n",
        "base_model_acc_list = []\n",
        "final_acc_list = []\n",
        "\n",
        "# do 5-fold cross-validation\n",
        "for i in range(5):\n",
        "  print('fold', i + 1)\n",
        "  temp_dict = ds_fold_dict.copy()\n",
        "  # get test set for this iteration\n",
        "  current_val_ds = temp_dict[i]\n",
        "\n",
        "  # get training set for this iteration from remaining data samples\n",
        "  del temp_dict[i]\n",
        "  current_train_ds = None\n",
        "  for ds_shard in temp_dict.values():\n",
        "    if current_train_ds is None:\n",
        "      current_train_ds = ds_shard\n",
        "    else:\n",
        "      current_train_ds = current_train_ds.concatenate(ds_shard)\n",
        "  \n",
        "  # configure both training and test sets to improve performance\n",
        "  current_train_ds = configure_for_performance(current_train_ds)\n",
        "  current_val_ds = configure_for_performance(current_val_ds)\n",
        "\n",
        "  # create a new model\n",
        "  model = create_model()\n",
        "  # get initial test accuracy\n",
        "  base_model_acc_list.append(model.evaluate(current_val_ds)[1])\n",
        "  # train for specified epochs\n",
        "  history = model.fit(current_train_ds,\n",
        "                    epochs=no_epochs,\n",
        "                    validation_data=current_val_ds)\n",
        "  \n",
        "  # save results\n",
        "  if i == 0:\n",
        "    history_map['accuracy'] = [history.history['accuracy']]\n",
        "    history_map['val_accuracy'] = [history.history['val_accuracy']]\n",
        "    history_map['loss'] = [history.history['loss']]\n",
        "    history_map['val_loss'] = [history.history['val_loss']]\n",
        "  else:\n",
        "    history_map['accuracy'].append(history.history['accuracy'])\n",
        "    history_map['val_accuracy'].append(history.history['val_accuracy'])\n",
        "    history_map['loss'].append(history.history['loss'])\n",
        "    history_map['val_loss'].append(history.history['val_loss'])\n",
        "  \n",
        "  # get final test accuracy by taking the max accuracy over the whole training\n",
        "  final_acc_list.append(np.amax(history.history['val_accuracy']))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fold 1\n",
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential (Sequential)      (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv (TensorF [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub (TensorFlowO [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "xception (Functional)        (None, 10, 10, 2048)      20861480  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 20,881,970\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 20,861,480\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 2s 125ms/step - loss: 2.3501 - accuracy: 0.0900\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 8s 158ms/step - loss: 2.3042 - accuracy: 0.1213 - val_loss: 2.2104 - val_accuracy: 0.1950\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 8s 161ms/step - loss: 2.1423 - accuracy: 0.2425 - val_loss: 2.0851 - val_accuracy: 0.3350\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 8s 164ms/step - loss: 2.0148 - accuracy: 0.3837 - val_loss: 1.9697 - val_accuracy: 0.5000\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 8s 166ms/step - loss: 1.8816 - accuracy: 0.5325 - val_loss: 1.8644 - val_accuracy: 0.5600\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 8s 169ms/step - loss: 1.7819 - accuracy: 0.5788 - val_loss: 1.7659 - val_accuracy: 0.6050\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 1.6721 - accuracy: 0.6413 - val_loss: 1.6767 - val_accuracy: 0.6400\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 9s 174ms/step - loss: 1.5743 - accuracy: 0.6988 - val_loss: 1.5952 - val_accuracy: 0.6500\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 9s 176ms/step - loss: 1.5007 - accuracy: 0.7000 - val_loss: 1.5220 - val_accuracy: 0.6700\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 9s 177ms/step - loss: 1.4068 - accuracy: 0.7575 - val_loss: 1.4535 - val_accuracy: 0.6900\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 9s 174ms/step - loss: 1.3298 - accuracy: 0.7625 - val_loss: 1.3877 - val_accuracy: 0.7150\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 1.2758 - accuracy: 0.7825 - val_loss: 1.3372 - val_accuracy: 0.7100\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 8s 170ms/step - loss: 1.2273 - accuracy: 0.7800 - val_loss: 1.2857 - val_accuracy: 0.7150\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 8s 169ms/step - loss: 1.1660 - accuracy: 0.8000 - val_loss: 1.2413 - val_accuracy: 0.7200\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 8s 169ms/step - loss: 1.1163 - accuracy: 0.8087 - val_loss: 1.2006 - val_accuracy: 0.7300\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 9s 171ms/step - loss: 1.0738 - accuracy: 0.8100 - val_loss: 1.1617 - val_accuracy: 0.7250\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.0306 - accuracy: 0.8213 - val_loss: 1.1293 - val_accuracy: 0.7250\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.9898 - accuracy: 0.8225 - val_loss: 1.0958 - val_accuracy: 0.7300\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.9616 - accuracy: 0.8250 - val_loss: 1.0667 - val_accuracy: 0.7450\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.9363 - accuracy: 0.8300 - val_loss: 1.0378 - val_accuracy: 0.7400\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.9092 - accuracy: 0.8413 - val_loss: 1.0132 - val_accuracy: 0.7400\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 9s 171ms/step - loss: 0.8602 - accuracy: 0.8350 - val_loss: 0.9902 - val_accuracy: 0.7400\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 9s 171ms/step - loss: 0.8474 - accuracy: 0.8375 - val_loss: 0.9706 - val_accuracy: 0.7350\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.8160 - accuracy: 0.8487 - val_loss: 0.9504 - val_accuracy: 0.7350\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.8080 - accuracy: 0.8338 - val_loss: 0.9289 - val_accuracy: 0.7500\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.7847 - accuracy: 0.8537 - val_loss: 0.9138 - val_accuracy: 0.7450\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.7545 - accuracy: 0.8550 - val_loss: 0.8995 - val_accuracy: 0.7500\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.7455 - accuracy: 0.8550 - val_loss: 0.8832 - val_accuracy: 0.7550\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.7243 - accuracy: 0.8575 - val_loss: 0.8674 - val_accuracy: 0.7550\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.6926 - accuracy: 0.8725 - val_loss: 0.8548 - val_accuracy: 0.7550\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.6884 - accuracy: 0.8562 - val_loss: 0.8403 - val_accuracy: 0.7600\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.6722 - accuracy: 0.8562 - val_loss: 0.8275 - val_accuracy: 0.7650\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.6704 - accuracy: 0.8662 - val_loss: 0.8153 - val_accuracy: 0.7650\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.6478 - accuracy: 0.8788 - val_loss: 0.8028 - val_accuracy: 0.7850\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.6380 - accuracy: 0.8850 - val_loss: 0.7931 - val_accuracy: 0.7750\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.6441 - accuracy: 0.8612 - val_loss: 0.7830 - val_accuracy: 0.7900\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.6083 - accuracy: 0.8763 - val_loss: 0.7728 - val_accuracy: 0.7800\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.5994 - accuracy: 0.8687 - val_loss: 0.7687 - val_accuracy: 0.7900\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5884 - accuracy: 0.8712 - val_loss: 0.7575 - val_accuracy: 0.7950\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5641 - accuracy: 0.8950 - val_loss: 0.7525 - val_accuracy: 0.7900\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5770 - accuracy: 0.8750 - val_loss: 0.7416 - val_accuracy: 0.7950\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5577 - accuracy: 0.8763 - val_loss: 0.7348 - val_accuracy: 0.7950\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.5501 - accuracy: 0.8825 - val_loss: 0.7281 - val_accuracy: 0.7950\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.5470 - accuracy: 0.8938 - val_loss: 0.7215 - val_accuracy: 0.7800\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.5389 - accuracy: 0.8813 - val_loss: 0.7118 - val_accuracy: 0.7950\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.5452 - accuracy: 0.8850 - val_loss: 0.7073 - val_accuracy: 0.7950\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5043 - accuracy: 0.8988 - val_loss: 0.6999 - val_accuracy: 0.7950\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5120 - accuracy: 0.8850 - val_loss: 0.6916 - val_accuracy: 0.7900\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.5004 - accuracy: 0.8938 - val_loss: 0.6869 - val_accuracy: 0.8000\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.5015 - accuracy: 0.9000 - val_loss: 0.6801 - val_accuracy: 0.8000\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4716 - accuracy: 0.9050 - val_loss: 0.6753 - val_accuracy: 0.8000\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4719 - accuracy: 0.8975 - val_loss: 0.6708 - val_accuracy: 0.8000\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4663 - accuracy: 0.9100 - val_loss: 0.6659 - val_accuracy: 0.8050\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4682 - accuracy: 0.9013 - val_loss: 0.6602 - val_accuracy: 0.8050\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4642 - accuracy: 0.9075 - val_loss: 0.6559 - val_accuracy: 0.8050\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4529 - accuracy: 0.8988 - val_loss: 0.6494 - val_accuracy: 0.8050\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4567 - accuracy: 0.8950 - val_loss: 0.6468 - val_accuracy: 0.8100\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4542 - accuracy: 0.8988 - val_loss: 0.6429 - val_accuracy: 0.8150\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4485 - accuracy: 0.8963 - val_loss: 0.6385 - val_accuracy: 0.8100\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4376 - accuracy: 0.8975 - val_loss: 0.6331 - val_accuracy: 0.8100\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4405 - accuracy: 0.9013 - val_loss: 0.6283 - val_accuracy: 0.8150\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4214 - accuracy: 0.9137 - val_loss: 0.6266 - val_accuracy: 0.8150\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4459 - accuracy: 0.8950 - val_loss: 0.6208 - val_accuracy: 0.8050\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4145 - accuracy: 0.9025 - val_loss: 0.6206 - val_accuracy: 0.8150\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4176 - accuracy: 0.9125 - val_loss: 0.6187 - val_accuracy: 0.8100\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4100 - accuracy: 0.9062 - val_loss: 0.6120 - val_accuracy: 0.8100\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4049 - accuracy: 0.8950 - val_loss: 0.6104 - val_accuracy: 0.8050\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4034 - accuracy: 0.9087 - val_loss: 0.6070 - val_accuracy: 0.8050\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3863 - accuracy: 0.9262 - val_loss: 0.6072 - val_accuracy: 0.8050\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3980 - accuracy: 0.9150 - val_loss: 0.6049 - val_accuracy: 0.8100\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3781 - accuracy: 0.9150 - val_loss: 0.5994 - val_accuracy: 0.8150\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3732 - accuracy: 0.9212 - val_loss: 0.5925 - val_accuracy: 0.8150\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3641 - accuracy: 0.9175 - val_loss: 0.5924 - val_accuracy: 0.8100\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3812 - accuracy: 0.9212 - val_loss: 0.5912 - val_accuracy: 0.8100\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3620 - accuracy: 0.9350 - val_loss: 0.5871 - val_accuracy: 0.8100\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3492 - accuracy: 0.9175 - val_loss: 0.5870 - val_accuracy: 0.8100\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3499 - accuracy: 0.9362 - val_loss: 0.5839 - val_accuracy: 0.8100\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3750 - accuracy: 0.9112 - val_loss: 0.5832 - val_accuracy: 0.8100\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3425 - accuracy: 0.9300 - val_loss: 0.5774 - val_accuracy: 0.8100\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3505 - accuracy: 0.9237 - val_loss: 0.5733 - val_accuracy: 0.8150\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3462 - accuracy: 0.9312 - val_loss: 0.5679 - val_accuracy: 0.8150\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3332 - accuracy: 0.9275 - val_loss: 0.5711 - val_accuracy: 0.8150\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3407 - accuracy: 0.9237 - val_loss: 0.5723 - val_accuracy: 0.8100\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3441 - accuracy: 0.9050 - val_loss: 0.5647 - val_accuracy: 0.8100\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3391 - accuracy: 0.9200 - val_loss: 0.5617 - val_accuracy: 0.8150\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3288 - accuracy: 0.9362 - val_loss: 0.5602 - val_accuracy: 0.8100\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3241 - accuracy: 0.9287 - val_loss: 0.5595 - val_accuracy: 0.8100\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3205 - accuracy: 0.9325 - val_loss: 0.5560 - val_accuracy: 0.8150\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3106 - accuracy: 0.9362 - val_loss: 0.5535 - val_accuracy: 0.8100\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3195 - accuracy: 0.9337 - val_loss: 0.5534 - val_accuracy: 0.8150\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.2969 - accuracy: 0.9375 - val_loss: 0.5530 - val_accuracy: 0.8100\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3250 - accuracy: 0.9287 - val_loss: 0.5525 - val_accuracy: 0.8100\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3343 - accuracy: 0.9200 - val_loss: 0.5524 - val_accuracy: 0.8100\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.2904 - accuracy: 0.9513 - val_loss: 0.5457 - val_accuracy: 0.8100\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.2960 - accuracy: 0.9375 - val_loss: 0.5447 - val_accuracy: 0.8150\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3054 - accuracy: 0.9337 - val_loss: 0.5492 - val_accuracy: 0.8100\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.2942 - accuracy: 0.9438 - val_loss: 0.5488 - val_accuracy: 0.8100\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3056 - accuracy: 0.9388 - val_loss: 0.5469 - val_accuracy: 0.8150\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.2880 - accuracy: 0.9388 - val_loss: 0.5402 - val_accuracy: 0.8100\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.2823 - accuracy: 0.9375 - val_loss: 0.5407 - val_accuracy: 0.8150\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.2971 - accuracy: 0.9362 - val_loss: 0.5400 - val_accuracy: 0.8200\n",
            "fold 2\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_1 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_1 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_1 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "xception (Functional)        (None, 10, 10, 2048)      20861480  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 20,881,970\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 20,861,480\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 2.3735 - accuracy: 0.0850\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 9s 171ms/step - loss: 2.3105 - accuracy: 0.1175 - val_loss: 2.2504 - val_accuracy: 0.1650\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 2.1633 - accuracy: 0.2200 - val_loss: 2.1288 - val_accuracy: 0.2700\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 9s 174ms/step - loss: 2.0224 - accuracy: 0.3800 - val_loss: 2.0135 - val_accuracy: 0.3900\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 9s 174ms/step - loss: 1.9091 - accuracy: 0.4787 - val_loss: 1.9038 - val_accuracy: 0.4950\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.7757 - accuracy: 0.6037 - val_loss: 1.8019 - val_accuracy: 0.5700\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 1.6909 - accuracy: 0.6363 - val_loss: 1.7104 - val_accuracy: 0.6250\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 9s 171ms/step - loss: 1.5892 - accuracy: 0.6837 - val_loss: 1.6247 - val_accuracy: 0.6800\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 9s 171ms/step - loss: 1.4918 - accuracy: 0.7262 - val_loss: 1.5455 - val_accuracy: 0.7000\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 9s 171ms/step - loss: 1.4277 - accuracy: 0.7212 - val_loss: 1.4768 - val_accuracy: 0.7300\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 1.3550 - accuracy: 0.7462 - val_loss: 1.4137 - val_accuracy: 0.7350\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.2915 - accuracy: 0.7550 - val_loss: 1.3558 - val_accuracy: 0.7450\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.2412 - accuracy: 0.7775 - val_loss: 1.3015 - val_accuracy: 0.7550\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.1787 - accuracy: 0.7987 - val_loss: 1.2497 - val_accuracy: 0.7850\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.1290 - accuracy: 0.8012 - val_loss: 1.2075 - val_accuracy: 0.7800\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 1.0813 - accuracy: 0.8075 - val_loss: 1.1652 - val_accuracy: 0.7900\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 1.0319 - accuracy: 0.8288 - val_loss: 1.1275 - val_accuracy: 0.7950\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 1.0123 - accuracy: 0.8213 - val_loss: 1.0925 - val_accuracy: 0.7950\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.9714 - accuracy: 0.8338 - val_loss: 1.0633 - val_accuracy: 0.8000\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.9174 - accuracy: 0.8388 - val_loss: 1.0375 - val_accuracy: 0.8000\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.9120 - accuracy: 0.8363 - val_loss: 1.0074 - val_accuracy: 0.8250\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.8825 - accuracy: 0.8450 - val_loss: 0.9797 - val_accuracy: 0.8100\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.8493 - accuracy: 0.8462 - val_loss: 0.9592 - val_accuracy: 0.8250\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.8246 - accuracy: 0.8537 - val_loss: 0.9402 - val_accuracy: 0.8150\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.8021 - accuracy: 0.8625 - val_loss: 0.9218 - val_accuracy: 0.8150\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.7820 - accuracy: 0.8487 - val_loss: 0.9041 - val_accuracy: 0.8150\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.7626 - accuracy: 0.8313 - val_loss: 0.8851 - val_accuracy: 0.8150\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.7381 - accuracy: 0.8625 - val_loss: 0.8698 - val_accuracy: 0.8250\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.7302 - accuracy: 0.8537 - val_loss: 0.8547 - val_accuracy: 0.8200\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.7117 - accuracy: 0.8675 - val_loss: 0.8413 - val_accuracy: 0.8300\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.7000 - accuracy: 0.8712 - val_loss: 0.8289 - val_accuracy: 0.8350\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.6755 - accuracy: 0.8725 - val_loss: 0.8171 - val_accuracy: 0.8300\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.6644 - accuracy: 0.8750 - val_loss: 0.8030 - val_accuracy: 0.8300\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.6502 - accuracy: 0.8600 - val_loss: 0.7923 - val_accuracy: 0.8350\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.6355 - accuracy: 0.8825 - val_loss: 0.7799 - val_accuracy: 0.8200\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.6509 - accuracy: 0.8775 - val_loss: 0.7701 - val_accuracy: 0.8350\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.6299 - accuracy: 0.8763 - val_loss: 0.7619 - val_accuracy: 0.8350\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.5931 - accuracy: 0.8763 - val_loss: 0.7516 - val_accuracy: 0.8300\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5799 - accuracy: 0.8750 - val_loss: 0.7431 - val_accuracy: 0.8350\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5852 - accuracy: 0.8775 - val_loss: 0.7348 - val_accuracy: 0.8350\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5776 - accuracy: 0.8825 - val_loss: 0.7278 - val_accuracy: 0.8300\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5804 - accuracy: 0.8750 - val_loss: 0.7192 - val_accuracy: 0.8250\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5522 - accuracy: 0.8888 - val_loss: 0.7121 - val_accuracy: 0.8350\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5360 - accuracy: 0.8950 - val_loss: 0.7068 - val_accuracy: 0.8250\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5288 - accuracy: 0.8963 - val_loss: 0.6998 - val_accuracy: 0.8250\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.5194 - accuracy: 0.8800 - val_loss: 0.6923 - val_accuracy: 0.8300\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.5251 - accuracy: 0.8850 - val_loss: 0.6876 - val_accuracy: 0.8300\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.5131 - accuracy: 0.8863 - val_loss: 0.6811 - val_accuracy: 0.8350\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4950 - accuracy: 0.8925 - val_loss: 0.6758 - val_accuracy: 0.8400\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 9s 171ms/step - loss: 0.4924 - accuracy: 0.8938 - val_loss: 0.6700 - val_accuracy: 0.8300\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4962 - accuracy: 0.8950 - val_loss: 0.6642 - val_accuracy: 0.8300\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4858 - accuracy: 0.8825 - val_loss: 0.6611 - val_accuracy: 0.8350\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4781 - accuracy: 0.8950 - val_loss: 0.6555 - val_accuracy: 0.8300\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4568 - accuracy: 0.9025 - val_loss: 0.6486 - val_accuracy: 0.8300\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4595 - accuracy: 0.9075 - val_loss: 0.6474 - val_accuracy: 0.8350\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4381 - accuracy: 0.9150 - val_loss: 0.6444 - val_accuracy: 0.8250\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4584 - accuracy: 0.9025 - val_loss: 0.6371 - val_accuracy: 0.8350\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4381 - accuracy: 0.9062 - val_loss: 0.6321 - val_accuracy: 0.8350\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4590 - accuracy: 0.8850 - val_loss: 0.6308 - val_accuracy: 0.8300\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4309 - accuracy: 0.9100 - val_loss: 0.6245 - val_accuracy: 0.8350\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4260 - accuracy: 0.9062 - val_loss: 0.6209 - val_accuracy: 0.8400\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4203 - accuracy: 0.9087 - val_loss: 0.6172 - val_accuracy: 0.8350\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4267 - accuracy: 0.9050 - val_loss: 0.6122 - val_accuracy: 0.8350\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3947 - accuracy: 0.9162 - val_loss: 0.6120 - val_accuracy: 0.8350\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4022 - accuracy: 0.9262 - val_loss: 0.6080 - val_accuracy: 0.8350\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3981 - accuracy: 0.9187 - val_loss: 0.6047 - val_accuracy: 0.8350\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3996 - accuracy: 0.9175 - val_loss: 0.6016 - val_accuracy: 0.8300\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3934 - accuracy: 0.9100 - val_loss: 0.5976 - val_accuracy: 0.8400\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3848 - accuracy: 0.9150 - val_loss: 0.5956 - val_accuracy: 0.8300\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3818 - accuracy: 0.9150 - val_loss: 0.5915 - val_accuracy: 0.8350\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3647 - accuracy: 0.9275 - val_loss: 0.5887 - val_accuracy: 0.8350\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3703 - accuracy: 0.9262 - val_loss: 0.5880 - val_accuracy: 0.8400\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3701 - accuracy: 0.9212 - val_loss: 0.5851 - val_accuracy: 0.8400\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3770 - accuracy: 0.9175 - val_loss: 0.5810 - val_accuracy: 0.8400\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3574 - accuracy: 0.9212 - val_loss: 0.5809 - val_accuracy: 0.8350\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3517 - accuracy: 0.9312 - val_loss: 0.5757 - val_accuracy: 0.8350\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3384 - accuracy: 0.9362 - val_loss: 0.5732 - val_accuracy: 0.8450\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3551 - accuracy: 0.9250 - val_loss: 0.5726 - val_accuracy: 0.8350\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3364 - accuracy: 0.9325 - val_loss: 0.5704 - val_accuracy: 0.8400\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3222 - accuracy: 0.9375 - val_loss: 0.5673 - val_accuracy: 0.8450\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3496 - accuracy: 0.9262 - val_loss: 0.5664 - val_accuracy: 0.8300\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3454 - accuracy: 0.9237 - val_loss: 0.5616 - val_accuracy: 0.8300\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3365 - accuracy: 0.9225 - val_loss: 0.5603 - val_accuracy: 0.8350\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3231 - accuracy: 0.9413 - val_loss: 0.5604 - val_accuracy: 0.8350\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3414 - accuracy: 0.9300 - val_loss: 0.5574 - val_accuracy: 0.8400\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3245 - accuracy: 0.9300 - val_loss: 0.5570 - val_accuracy: 0.8300\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3401 - accuracy: 0.9237 - val_loss: 0.5541 - val_accuracy: 0.8300\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3209 - accuracy: 0.9388 - val_loss: 0.5509 - val_accuracy: 0.8350\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3265 - accuracy: 0.9337 - val_loss: 0.5456 - val_accuracy: 0.8300\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3116 - accuracy: 0.9300 - val_loss: 0.5453 - val_accuracy: 0.8300\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3066 - accuracy: 0.9388 - val_loss: 0.5438 - val_accuracy: 0.8300\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3052 - accuracy: 0.9350 - val_loss: 0.5467 - val_accuracy: 0.8300\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3058 - accuracy: 0.9413 - val_loss: 0.5449 - val_accuracy: 0.8350\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.2839 - accuracy: 0.9450 - val_loss: 0.5425 - val_accuracy: 0.8350\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.2896 - accuracy: 0.9362 - val_loss: 0.5408 - val_accuracy: 0.8350\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.2922 - accuracy: 0.9375 - val_loss: 0.5398 - val_accuracy: 0.8350\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.2885 - accuracy: 0.9450 - val_loss: 0.5357 - val_accuracy: 0.8350\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3023 - accuracy: 0.9388 - val_loss: 0.5349 - val_accuracy: 0.8350\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.2984 - accuracy: 0.9400 - val_loss: 0.5324 - val_accuracy: 0.8350\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.2798 - accuracy: 0.9400 - val_loss: 0.5326 - val_accuracy: 0.8350\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3001 - accuracy: 0.9350 - val_loss: 0.5305 - val_accuracy: 0.8350\n",
            "fold 3\n",
            "Model: \"functional_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_2 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_2 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_2 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "xception (Functional)        (None, 10, 10, 2048)      20861480  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 20,881,970\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 20,861,480\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 2s 117ms/step - loss: 2.3199 - accuracy: 0.1050\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 9s 171ms/step - loss: 2.2600 - accuracy: 0.1637 - val_loss: 2.1837 - val_accuracy: 0.2300\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 2.1107 - accuracy: 0.2875 - val_loss: 2.0579 - val_accuracy: 0.3900\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 9s 174ms/step - loss: 1.9926 - accuracy: 0.3988 - val_loss: 1.9449 - val_accuracy: 0.5000\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.8698 - accuracy: 0.5213 - val_loss: 1.8364 - val_accuracy: 0.5700\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.7649 - accuracy: 0.5788 - val_loss: 1.7333 - val_accuracy: 0.6250\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 1.6646 - accuracy: 0.6338 - val_loss: 1.6464 - val_accuracy: 0.6400\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 1.5681 - accuracy: 0.6662 - val_loss: 1.5641 - val_accuracy: 0.6750\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 1.4884 - accuracy: 0.7100 - val_loss: 1.4862 - val_accuracy: 0.6900\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 1.4098 - accuracy: 0.7387 - val_loss: 1.4182 - val_accuracy: 0.7150\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 1.3426 - accuracy: 0.7550 - val_loss: 1.3535 - val_accuracy: 0.7150\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 1.2981 - accuracy: 0.7513 - val_loss: 1.2963 - val_accuracy: 0.7250\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.2216 - accuracy: 0.7800 - val_loss: 1.2433 - val_accuracy: 0.7300\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.1859 - accuracy: 0.7775 - val_loss: 1.1968 - val_accuracy: 0.7400\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.1292 - accuracy: 0.7975 - val_loss: 1.1534 - val_accuracy: 0.7550\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.0956 - accuracy: 0.8012 - val_loss: 1.1157 - val_accuracy: 0.7650\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.0433 - accuracy: 0.7975 - val_loss: 1.0808 - val_accuracy: 0.7700\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.9997 - accuracy: 0.8100 - val_loss: 1.0447 - val_accuracy: 0.7800\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.9542 - accuracy: 0.8213 - val_loss: 1.0148 - val_accuracy: 0.7850\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.9516 - accuracy: 0.8163 - val_loss: 0.9897 - val_accuracy: 0.8000\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.9221 - accuracy: 0.8325 - val_loss: 0.9619 - val_accuracy: 0.8050\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.8783 - accuracy: 0.8275 - val_loss: 0.9384 - val_accuracy: 0.8100\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.8517 - accuracy: 0.8400 - val_loss: 0.9136 - val_accuracy: 0.8100\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.8224 - accuracy: 0.8438 - val_loss: 0.8941 - val_accuracy: 0.8150\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.8199 - accuracy: 0.8175 - val_loss: 0.8749 - val_accuracy: 0.8050\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.7881 - accuracy: 0.8587 - val_loss: 0.8552 - val_accuracy: 0.8000\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.7725 - accuracy: 0.8500 - val_loss: 0.8383 - val_accuracy: 0.8100\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.7704 - accuracy: 0.8413 - val_loss: 0.8236 - val_accuracy: 0.8100\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.7363 - accuracy: 0.8512 - val_loss: 0.8085 - val_accuracy: 0.8100\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.7309 - accuracy: 0.8500 - val_loss: 0.7961 - val_accuracy: 0.8150\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.7007 - accuracy: 0.8612 - val_loss: 0.7837 - val_accuracy: 0.8200\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.6908 - accuracy: 0.8587 - val_loss: 0.7696 - val_accuracy: 0.8150\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.6640 - accuracy: 0.8687 - val_loss: 0.7607 - val_accuracy: 0.8100\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.6622 - accuracy: 0.8700 - val_loss: 0.7477 - val_accuracy: 0.8150\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.6387 - accuracy: 0.8788 - val_loss: 0.7357 - val_accuracy: 0.8200\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.6366 - accuracy: 0.8750 - val_loss: 0.7227 - val_accuracy: 0.8200\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.6355 - accuracy: 0.8587 - val_loss: 0.7159 - val_accuracy: 0.8200\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.6113 - accuracy: 0.8675 - val_loss: 0.7067 - val_accuracy: 0.8200\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.6004 - accuracy: 0.8825 - val_loss: 0.6957 - val_accuracy: 0.8200\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5911 - accuracy: 0.8763 - val_loss: 0.6881 - val_accuracy: 0.8200\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5660 - accuracy: 0.8838 - val_loss: 0.6794 - val_accuracy: 0.8250\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.5647 - accuracy: 0.8763 - val_loss: 0.6759 - val_accuracy: 0.8250\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5716 - accuracy: 0.8737 - val_loss: 0.6682 - val_accuracy: 0.8250\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5506 - accuracy: 0.8863 - val_loss: 0.6606 - val_accuracy: 0.8350\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5350 - accuracy: 0.8925 - val_loss: 0.6527 - val_accuracy: 0.8250\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5414 - accuracy: 0.8737 - val_loss: 0.6462 - val_accuracy: 0.8350\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5337 - accuracy: 0.8838 - val_loss: 0.6414 - val_accuracy: 0.8300\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5172 - accuracy: 0.8925 - val_loss: 0.6336 - val_accuracy: 0.8350\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5070 - accuracy: 0.8888 - val_loss: 0.6298 - val_accuracy: 0.8350\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4844 - accuracy: 0.9013 - val_loss: 0.6229 - val_accuracy: 0.8400\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4981 - accuracy: 0.8950 - val_loss: 0.6167 - val_accuracy: 0.8400\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5010 - accuracy: 0.8838 - val_loss: 0.6136 - val_accuracy: 0.8450\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4950 - accuracy: 0.8988 - val_loss: 0.6120 - val_accuracy: 0.8450\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4732 - accuracy: 0.8938 - val_loss: 0.6024 - val_accuracy: 0.8450\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4650 - accuracy: 0.8963 - val_loss: 0.5989 - val_accuracy: 0.8400\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4588 - accuracy: 0.8988 - val_loss: 0.5952 - val_accuracy: 0.8450\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4611 - accuracy: 0.8950 - val_loss: 0.5898 - val_accuracy: 0.8450\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4490 - accuracy: 0.9038 - val_loss: 0.5865 - val_accuracy: 0.8450\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4425 - accuracy: 0.9025 - val_loss: 0.5857 - val_accuracy: 0.8450\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4373 - accuracy: 0.9038 - val_loss: 0.5833 - val_accuracy: 0.8450\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4415 - accuracy: 0.9137 - val_loss: 0.5752 - val_accuracy: 0.8450\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4272 - accuracy: 0.9025 - val_loss: 0.5742 - val_accuracy: 0.8400\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4308 - accuracy: 0.9112 - val_loss: 0.5700 - val_accuracy: 0.8400\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4355 - accuracy: 0.8825 - val_loss: 0.5651 - val_accuracy: 0.8450\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4339 - accuracy: 0.9013 - val_loss: 0.5617 - val_accuracy: 0.8400\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4237 - accuracy: 0.9038 - val_loss: 0.5601 - val_accuracy: 0.8450\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4083 - accuracy: 0.9087 - val_loss: 0.5550 - val_accuracy: 0.8350\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3878 - accuracy: 0.9150 - val_loss: 0.5559 - val_accuracy: 0.8350\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3909 - accuracy: 0.9087 - val_loss: 0.5524 - val_accuracy: 0.8350\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3898 - accuracy: 0.9112 - val_loss: 0.5473 - val_accuracy: 0.8400\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4043 - accuracy: 0.9025 - val_loss: 0.5459 - val_accuracy: 0.8400\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3859 - accuracy: 0.9262 - val_loss: 0.5421 - val_accuracy: 0.8400\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3706 - accuracy: 0.9275 - val_loss: 0.5396 - val_accuracy: 0.8300\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3723 - accuracy: 0.9162 - val_loss: 0.5399 - val_accuracy: 0.8350\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3728 - accuracy: 0.9100 - val_loss: 0.5377 - val_accuracy: 0.8400\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3541 - accuracy: 0.9237 - val_loss: 0.5361 - val_accuracy: 0.8350\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3669 - accuracy: 0.9287 - val_loss: 0.5331 - val_accuracy: 0.8450\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3515 - accuracy: 0.9262 - val_loss: 0.5300 - val_accuracy: 0.8450\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3622 - accuracy: 0.9287 - val_loss: 0.5298 - val_accuracy: 0.8450\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3460 - accuracy: 0.9237 - val_loss: 0.5269 - val_accuracy: 0.8450\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3670 - accuracy: 0.9175 - val_loss: 0.5283 - val_accuracy: 0.8400\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3545 - accuracy: 0.9212 - val_loss: 0.5232 - val_accuracy: 0.8400\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3328 - accuracy: 0.9212 - val_loss: 0.5221 - val_accuracy: 0.8400\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3343 - accuracy: 0.9225 - val_loss: 0.5190 - val_accuracy: 0.8400\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3421 - accuracy: 0.9337 - val_loss: 0.5180 - val_accuracy: 0.8400\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3433 - accuracy: 0.9337 - val_loss: 0.5158 - val_accuracy: 0.8450\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3299 - accuracy: 0.9275 - val_loss: 0.5188 - val_accuracy: 0.8400\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3364 - accuracy: 0.9300 - val_loss: 0.5207 - val_accuracy: 0.8350\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3198 - accuracy: 0.9325 - val_loss: 0.5159 - val_accuracy: 0.8350\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3182 - accuracy: 0.9300 - val_loss: 0.5128 - val_accuracy: 0.8350\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3305 - accuracy: 0.9287 - val_loss: 0.5077 - val_accuracy: 0.8450\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3257 - accuracy: 0.9212 - val_loss: 0.5081 - val_accuracy: 0.8450\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.2993 - accuracy: 0.9375 - val_loss: 0.5096 - val_accuracy: 0.8350\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3145 - accuracy: 0.9275 - val_loss: 0.5047 - val_accuracy: 0.8450\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3031 - accuracy: 0.9413 - val_loss: 0.5022 - val_accuracy: 0.8450\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.2932 - accuracy: 0.9413 - val_loss: 0.5048 - val_accuracy: 0.8400\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3022 - accuracy: 0.9400 - val_loss: 0.5027 - val_accuracy: 0.8400\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3023 - accuracy: 0.9362 - val_loss: 0.4996 - val_accuracy: 0.8450\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3061 - accuracy: 0.9337 - val_loss: 0.4999 - val_accuracy: 0.8550\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3023 - accuracy: 0.9312 - val_loss: 0.4991 - val_accuracy: 0.8450\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3007 - accuracy: 0.9325 - val_loss: 0.4988 - val_accuracy: 0.8450\n",
            "fold 4\n",
            "Model: \"functional_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_3 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_3 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_3 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "xception (Functional)        (None, 10, 10, 2048)      20861480  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_3 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 20,881,970\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 20,861,480\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 2s 118ms/step - loss: 2.3745 - accuracy: 0.0750\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 9s 171ms/step - loss: 2.3397 - accuracy: 0.1075 - val_loss: 2.2514 - val_accuracy: 0.1450\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 2.1764 - accuracy: 0.2262 - val_loss: 2.1334 - val_accuracy: 0.2650\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 9s 174ms/step - loss: 2.0647 - accuracy: 0.3338 - val_loss: 2.0199 - val_accuracy: 0.3900\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 9s 175ms/step - loss: 1.9131 - accuracy: 0.4837 - val_loss: 1.9064 - val_accuracy: 0.5000\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 9s 174ms/step - loss: 1.8269 - accuracy: 0.5450 - val_loss: 1.8024 - val_accuracy: 0.5850\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.7180 - accuracy: 0.6050 - val_loss: 1.7061 - val_accuracy: 0.6400\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 1.6057 - accuracy: 0.6737 - val_loss: 1.6208 - val_accuracy: 0.6800\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 1.5417 - accuracy: 0.6762 - val_loss: 1.5413 - val_accuracy: 0.7300\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 1.4510 - accuracy: 0.7212 - val_loss: 1.4671 - val_accuracy: 0.7400\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 1.3782 - accuracy: 0.7462 - val_loss: 1.4013 - val_accuracy: 0.7650\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.3122 - accuracy: 0.7700 - val_loss: 1.3398 - val_accuracy: 0.7700\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 9s 174ms/step - loss: 1.2430 - accuracy: 0.7875 - val_loss: 1.2826 - val_accuracy: 0.7900\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.1871 - accuracy: 0.7812 - val_loss: 1.2346 - val_accuracy: 0.8000\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.1389 - accuracy: 0.7975 - val_loss: 1.1889 - val_accuracy: 0.8000\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.0946 - accuracy: 0.8138 - val_loss: 1.1459 - val_accuracy: 0.8000\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.0678 - accuracy: 0.8025 - val_loss: 1.1070 - val_accuracy: 0.8050\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 1.0335 - accuracy: 0.8012 - val_loss: 1.0696 - val_accuracy: 0.8100\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.9894 - accuracy: 0.8050 - val_loss: 1.0378 - val_accuracy: 0.8050\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.9440 - accuracy: 0.8188 - val_loss: 1.0075 - val_accuracy: 0.8100\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.8957 - accuracy: 0.8375 - val_loss: 0.9806 - val_accuracy: 0.8050\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.8846 - accuracy: 0.8413 - val_loss: 0.9545 - val_accuracy: 0.8200\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.8561 - accuracy: 0.8462 - val_loss: 0.9318 - val_accuracy: 0.8100\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.8333 - accuracy: 0.8350 - val_loss: 0.9080 - val_accuracy: 0.8150\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.8212 - accuracy: 0.8512 - val_loss: 0.8869 - val_accuracy: 0.8050\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.7866 - accuracy: 0.8450 - val_loss: 0.8664 - val_accuracy: 0.8250\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.7854 - accuracy: 0.8625 - val_loss: 0.8523 - val_accuracy: 0.8100\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.7599 - accuracy: 0.8512 - val_loss: 0.8336 - val_accuracy: 0.8300\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.7341 - accuracy: 0.8637 - val_loss: 0.8182 - val_accuracy: 0.8350\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.7303 - accuracy: 0.8462 - val_loss: 0.8053 - val_accuracy: 0.8300\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.6913 - accuracy: 0.8725 - val_loss: 0.7954 - val_accuracy: 0.8350\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.6816 - accuracy: 0.8675 - val_loss: 0.7785 - val_accuracy: 0.8300\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.6671 - accuracy: 0.8600 - val_loss: 0.7680 - val_accuracy: 0.8200\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.6706 - accuracy: 0.8537 - val_loss: 0.7556 - val_accuracy: 0.8250\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.6454 - accuracy: 0.8725 - val_loss: 0.7473 - val_accuracy: 0.8150\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.6325 - accuracy: 0.8612 - val_loss: 0.7355 - val_accuracy: 0.8150\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.6142 - accuracy: 0.8763 - val_loss: 0.7260 - val_accuracy: 0.8150\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.6193 - accuracy: 0.8625 - val_loss: 0.7164 - val_accuracy: 0.8150\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5896 - accuracy: 0.8863 - val_loss: 0.7059 - val_accuracy: 0.8150\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5830 - accuracy: 0.8763 - val_loss: 0.6966 - val_accuracy: 0.8150\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5804 - accuracy: 0.8938 - val_loss: 0.6893 - val_accuracy: 0.8100\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.5505 - accuracy: 0.8875 - val_loss: 0.6826 - val_accuracy: 0.8100\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5777 - accuracy: 0.8512 - val_loss: 0.6753 - val_accuracy: 0.8150\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5453 - accuracy: 0.8813 - val_loss: 0.6689 - val_accuracy: 0.8200\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5505 - accuracy: 0.8687 - val_loss: 0.6680 - val_accuracy: 0.8150\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5196 - accuracy: 0.8888 - val_loss: 0.6593 - val_accuracy: 0.8150\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5171 - accuracy: 0.8925 - val_loss: 0.6502 - val_accuracy: 0.8200\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5042 - accuracy: 0.9000 - val_loss: 0.6461 - val_accuracy: 0.8150\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4906 - accuracy: 0.8988 - val_loss: 0.6403 - val_accuracy: 0.8200\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4976 - accuracy: 0.8950 - val_loss: 0.6335 - val_accuracy: 0.8200\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4991 - accuracy: 0.8963 - val_loss: 0.6315 - val_accuracy: 0.8250\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4776 - accuracy: 0.8988 - val_loss: 0.6273 - val_accuracy: 0.8150\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4641 - accuracy: 0.9087 - val_loss: 0.6201 - val_accuracy: 0.8250\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4760 - accuracy: 0.8938 - val_loss: 0.6187 - val_accuracy: 0.8300\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4798 - accuracy: 0.8900 - val_loss: 0.6151 - val_accuracy: 0.8300\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4710 - accuracy: 0.8850 - val_loss: 0.6133 - val_accuracy: 0.8250\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4745 - accuracy: 0.8888 - val_loss: 0.6086 - val_accuracy: 0.8200\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4357 - accuracy: 0.9025 - val_loss: 0.6009 - val_accuracy: 0.8200\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4505 - accuracy: 0.9125 - val_loss: 0.5994 - val_accuracy: 0.8250\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4325 - accuracy: 0.9038 - val_loss: 0.5954 - val_accuracy: 0.8250\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4262 - accuracy: 0.9137 - val_loss: 0.5947 - val_accuracy: 0.8250\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4212 - accuracy: 0.9000 - val_loss: 0.5905 - val_accuracy: 0.8300\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4167 - accuracy: 0.9087 - val_loss: 0.5839 - val_accuracy: 0.8250\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4196 - accuracy: 0.9050 - val_loss: 0.5843 - val_accuracy: 0.8250\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3902 - accuracy: 0.9137 - val_loss: 0.5820 - val_accuracy: 0.8250\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3985 - accuracy: 0.9150 - val_loss: 0.5781 - val_accuracy: 0.8250\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3942 - accuracy: 0.9175 - val_loss: 0.5764 - val_accuracy: 0.8250\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3952 - accuracy: 0.9112 - val_loss: 0.5758 - val_accuracy: 0.8300\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3751 - accuracy: 0.9225 - val_loss: 0.5719 - val_accuracy: 0.8250\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3824 - accuracy: 0.9275 - val_loss: 0.5723 - val_accuracy: 0.8250\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3560 - accuracy: 0.9275 - val_loss: 0.5685 - val_accuracy: 0.8250\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3711 - accuracy: 0.9237 - val_loss: 0.5664 - val_accuracy: 0.8250\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3678 - accuracy: 0.9187 - val_loss: 0.5644 - val_accuracy: 0.8250\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3704 - accuracy: 0.9137 - val_loss: 0.5591 - val_accuracy: 0.8250\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3557 - accuracy: 0.9100 - val_loss: 0.5553 - val_accuracy: 0.8300\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3463 - accuracy: 0.9337 - val_loss: 0.5551 - val_accuracy: 0.8300\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3550 - accuracy: 0.9162 - val_loss: 0.5562 - val_accuracy: 0.8250\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3576 - accuracy: 0.9300 - val_loss: 0.5548 - val_accuracy: 0.8250\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3454 - accuracy: 0.9337 - val_loss: 0.5520 - val_accuracy: 0.8200\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3398 - accuracy: 0.9262 - val_loss: 0.5494 - val_accuracy: 0.8250\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3429 - accuracy: 0.9250 - val_loss: 0.5464 - val_accuracy: 0.8250\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3339 - accuracy: 0.9413 - val_loss: 0.5463 - val_accuracy: 0.8250\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3276 - accuracy: 0.9325 - val_loss: 0.5458 - val_accuracy: 0.8250\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3282 - accuracy: 0.9275 - val_loss: 0.5481 - val_accuracy: 0.8250\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3240 - accuracy: 0.9388 - val_loss: 0.5475 - val_accuracy: 0.8300\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3367 - accuracy: 0.9187 - val_loss: 0.5469 - val_accuracy: 0.8200\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3305 - accuracy: 0.9262 - val_loss: 0.5421 - val_accuracy: 0.8250\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.3065 - accuracy: 0.9438 - val_loss: 0.5418 - val_accuracy: 0.8250\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3069 - accuracy: 0.9375 - val_loss: 0.5440 - val_accuracy: 0.8300\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3222 - accuracy: 0.9300 - val_loss: 0.5380 - val_accuracy: 0.8250\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3136 - accuracy: 0.9237 - val_loss: 0.5369 - val_accuracy: 0.8250\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3020 - accuracy: 0.9337 - val_loss: 0.5346 - val_accuracy: 0.8250\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.2948 - accuracy: 0.9400 - val_loss: 0.5387 - val_accuracy: 0.8300\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 9s 174ms/step - loss: 0.3038 - accuracy: 0.9362 - val_loss: 0.5359 - val_accuracy: 0.8300\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 9s 174ms/step - loss: 0.2956 - accuracy: 0.9350 - val_loss: 0.5336 - val_accuracy: 0.8300\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 9s 174ms/step - loss: 0.2788 - accuracy: 0.9475 - val_loss: 0.5335 - val_accuracy: 0.8250\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 9s 174ms/step - loss: 0.2996 - accuracy: 0.9312 - val_loss: 0.5309 - val_accuracy: 0.8250\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.2728 - accuracy: 0.9600 - val_loss: 0.5312 - val_accuracy: 0.8250\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.2709 - accuracy: 0.9500 - val_loss: 0.5321 - val_accuracy: 0.8250\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.2870 - accuracy: 0.9350 - val_loss: 0.5301 - val_accuracy: 0.8250\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.2803 - accuracy: 0.9475 - val_loss: 0.5285 - val_accuracy: 0.8250\n",
            "fold 5\n",
            "Model: \"functional_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_4 (Sequential)    (None, 299, 299, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_RealDiv_4 (Tenso [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Sub_4 (TensorFlo [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "xception (Functional)        (None, 10, 10, 2048)      20861480  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_4 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 20,881,970\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 20,861,480\n",
            "_________________________________________________________________\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 2.3018 - accuracy: 0.1150\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 2.2837 - accuracy: 0.1400 - val_loss: 2.1620 - val_accuracy: 0.2200\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 2.1395 - accuracy: 0.2600 - val_loss: 2.0458 - val_accuracy: 0.3400\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 9s 175ms/step - loss: 1.9978 - accuracy: 0.4050 - val_loss: 1.9318 - val_accuracy: 0.4700\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 9s 174ms/step - loss: 1.8751 - accuracy: 0.4988 - val_loss: 1.8278 - val_accuracy: 0.5750\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 9s 174ms/step - loss: 1.7826 - accuracy: 0.5738 - val_loss: 1.7331 - val_accuracy: 0.6550\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.6860 - accuracy: 0.6212 - val_loss: 1.6483 - val_accuracy: 0.6900\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.5915 - accuracy: 0.6925 - val_loss: 1.5681 - val_accuracy: 0.7150\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.5052 - accuracy: 0.7163 - val_loss: 1.4939 - val_accuracy: 0.7200\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 1.4110 - accuracy: 0.7475 - val_loss: 1.4277 - val_accuracy: 0.7250\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.3570 - accuracy: 0.7588 - val_loss: 1.3664 - val_accuracy: 0.7550\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.2978 - accuracy: 0.7525 - val_loss: 1.3110 - val_accuracy: 0.7550\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.2268 - accuracy: 0.7887 - val_loss: 1.2623 - val_accuracy: 0.7650\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.1756 - accuracy: 0.7912 - val_loss: 1.2154 - val_accuracy: 0.7700\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.1377 - accuracy: 0.8163 - val_loss: 1.1711 - val_accuracy: 0.7700\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 9s 174ms/step - loss: 1.0863 - accuracy: 0.7937 - val_loss: 1.1336 - val_accuracy: 0.7800\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.0171 - accuracy: 0.8200 - val_loss: 1.0985 - val_accuracy: 0.7800\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 1.0043 - accuracy: 0.8138 - val_loss: 1.0658 - val_accuracy: 0.8100\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 9s 174ms/step - loss: 0.9680 - accuracy: 0.8475 - val_loss: 1.0383 - val_accuracy: 0.8050\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.9456 - accuracy: 0.8188 - val_loss: 1.0068 - val_accuracy: 0.8150\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.9213 - accuracy: 0.8288 - val_loss: 0.9825 - val_accuracy: 0.8150\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.8674 - accuracy: 0.8388 - val_loss: 0.9547 - val_accuracy: 0.8300\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.8411 - accuracy: 0.8487 - val_loss: 0.9335 - val_accuracy: 0.8250\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.8275 - accuracy: 0.8425 - val_loss: 0.9138 - val_accuracy: 0.8200\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.8120 - accuracy: 0.8487 - val_loss: 0.8908 - val_accuracy: 0.8300\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.7929 - accuracy: 0.8475 - val_loss: 0.8739 - val_accuracy: 0.8400\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.7603 - accuracy: 0.8550 - val_loss: 0.8590 - val_accuracy: 0.8300\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.7662 - accuracy: 0.8512 - val_loss: 0.8396 - val_accuracy: 0.8400\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.7049 - accuracy: 0.8675 - val_loss: 0.8274 - val_accuracy: 0.8400\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.7174 - accuracy: 0.8550 - val_loss: 0.8138 - val_accuracy: 0.8400\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 9s 174ms/step - loss: 0.6834 - accuracy: 0.8562 - val_loss: 0.7976 - val_accuracy: 0.8350\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.6783 - accuracy: 0.8662 - val_loss: 0.7860 - val_accuracy: 0.8400\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.6694 - accuracy: 0.8512 - val_loss: 0.7715 - val_accuracy: 0.8400\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.6421 - accuracy: 0.8788 - val_loss: 0.7619 - val_accuracy: 0.8450\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.6382 - accuracy: 0.8675 - val_loss: 0.7515 - val_accuracy: 0.8400\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.6322 - accuracy: 0.8712 - val_loss: 0.7401 - val_accuracy: 0.8300\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.6117 - accuracy: 0.8725 - val_loss: 0.7320 - val_accuracy: 0.8500\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.6004 - accuracy: 0.8775 - val_loss: 0.7220 - val_accuracy: 0.8500\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.6072 - accuracy: 0.8712 - val_loss: 0.7129 - val_accuracy: 0.8500\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5858 - accuracy: 0.8687 - val_loss: 0.7069 - val_accuracy: 0.8500\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5866 - accuracy: 0.8637 - val_loss: 0.6994 - val_accuracy: 0.8500\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5665 - accuracy: 0.8825 - val_loss: 0.6895 - val_accuracy: 0.8500\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5650 - accuracy: 0.8700 - val_loss: 0.6833 - val_accuracy: 0.8500\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5647 - accuracy: 0.8562 - val_loss: 0.6747 - val_accuracy: 0.8500\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 9s 174ms/step - loss: 0.5404 - accuracy: 0.8813 - val_loss: 0.6692 - val_accuracy: 0.8450\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5269 - accuracy: 0.8825 - val_loss: 0.6638 - val_accuracy: 0.8450\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5415 - accuracy: 0.8737 - val_loss: 0.6559 - val_accuracy: 0.8500\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5194 - accuracy: 0.8825 - val_loss: 0.6488 - val_accuracy: 0.8500\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5019 - accuracy: 0.8900 - val_loss: 0.6474 - val_accuracy: 0.8500\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4843 - accuracy: 0.9013 - val_loss: 0.6405 - val_accuracy: 0.8500\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4944 - accuracy: 0.8938 - val_loss: 0.6386 - val_accuracy: 0.8500\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.5018 - accuracy: 0.8675 - val_loss: 0.6290 - val_accuracy: 0.8550\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4695 - accuracy: 0.9150 - val_loss: 0.6235 - val_accuracy: 0.8500\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4738 - accuracy: 0.9013 - val_loss: 0.6187 - val_accuracy: 0.8500\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4670 - accuracy: 0.9038 - val_loss: 0.6120 - val_accuracy: 0.8500\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4655 - accuracy: 0.9000 - val_loss: 0.6066 - val_accuracy: 0.8500\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4653 - accuracy: 0.8988 - val_loss: 0.6057 - val_accuracy: 0.8500\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4374 - accuracy: 0.9100 - val_loss: 0.6008 - val_accuracy: 0.8500\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4618 - accuracy: 0.8925 - val_loss: 0.5957 - val_accuracy: 0.8500\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4287 - accuracy: 0.9087 - val_loss: 0.5936 - val_accuracy: 0.8500\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4274 - accuracy: 0.9000 - val_loss: 0.5860 - val_accuracy: 0.8500\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4303 - accuracy: 0.9075 - val_loss: 0.5839 - val_accuracy: 0.8500\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4167 - accuracy: 0.9075 - val_loss: 0.5798 - val_accuracy: 0.8500\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4017 - accuracy: 0.9050 - val_loss: 0.5769 - val_accuracy: 0.8500\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3984 - accuracy: 0.9175 - val_loss: 0.5717 - val_accuracy: 0.8500\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.4184 - accuracy: 0.9013 - val_loss: 0.5703 - val_accuracy: 0.8500\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 9s 174ms/step - loss: 0.4162 - accuracy: 0.9050 - val_loss: 0.5645 - val_accuracy: 0.8500\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3850 - accuracy: 0.9262 - val_loss: 0.5614 - val_accuracy: 0.8500\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3843 - accuracy: 0.9225 - val_loss: 0.5623 - val_accuracy: 0.8500\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3785 - accuracy: 0.9187 - val_loss: 0.5550 - val_accuracy: 0.8500\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3884 - accuracy: 0.9137 - val_loss: 0.5563 - val_accuracy: 0.8500\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 9s 174ms/step - loss: 0.3848 - accuracy: 0.9125 - val_loss: 0.5539 - val_accuracy: 0.8500\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3663 - accuracy: 0.9325 - val_loss: 0.5504 - val_accuracy: 0.8500\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3887 - accuracy: 0.8988 - val_loss: 0.5438 - val_accuracy: 0.8500\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3842 - accuracy: 0.9175 - val_loss: 0.5413 - val_accuracy: 0.8500\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3678 - accuracy: 0.9112 - val_loss: 0.5405 - val_accuracy: 0.8450\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3499 - accuracy: 0.9237 - val_loss: 0.5359 - val_accuracy: 0.8500\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3381 - accuracy: 0.9337 - val_loss: 0.5356 - val_accuracy: 0.8500\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3600 - accuracy: 0.9150 - val_loss: 0.5302 - val_accuracy: 0.8500\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3619 - accuracy: 0.9175 - val_loss: 0.5262 - val_accuracy: 0.8450\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3365 - accuracy: 0.9187 - val_loss: 0.5261 - val_accuracy: 0.8500\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3519 - accuracy: 0.9212 - val_loss: 0.5253 - val_accuracy: 0.8500\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3280 - accuracy: 0.9362 - val_loss: 0.5250 - val_accuracy: 0.8450\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3244 - accuracy: 0.9312 - val_loss: 0.5207 - val_accuracy: 0.8450\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3297 - accuracy: 0.9212 - val_loss: 0.5195 - val_accuracy: 0.8450\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3137 - accuracy: 0.9325 - val_loss: 0.5181 - val_accuracy: 0.8450\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3242 - accuracy: 0.9312 - val_loss: 0.5170 - val_accuracy: 0.8500\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3308 - accuracy: 0.9375 - val_loss: 0.5151 - val_accuracy: 0.8500\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3073 - accuracy: 0.9425 - val_loss: 0.5084 - val_accuracy: 0.8500\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3215 - accuracy: 0.9375 - val_loss: 0.5095 - val_accuracy: 0.8450\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3175 - accuracy: 0.9325 - val_loss: 0.5063 - val_accuracy: 0.8450\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3007 - accuracy: 0.9325 - val_loss: 0.5064 - val_accuracy: 0.8450\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 9s 174ms/step - loss: 0.3036 - accuracy: 0.9438 - val_loss: 0.5041 - val_accuracy: 0.8450\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.2999 - accuracy: 0.9350 - val_loss: 0.5042 - val_accuracy: 0.8450\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.2932 - accuracy: 0.9400 - val_loss: 0.5035 - val_accuracy: 0.8450\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3048 - accuracy: 0.9362 - val_loss: 0.5024 - val_accuracy: 0.8400\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.2855 - accuracy: 0.9400 - val_loss: 0.5022 - val_accuracy: 0.8400\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.2952 - accuracy: 0.9375 - val_loss: 0.5000 - val_accuracy: 0.8400\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.2870 - accuracy: 0.9475 - val_loss: 0.4935 - val_accuracy: 0.8450\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.2919 - accuracy: 0.9375 - val_loss: 0.4899 - val_accuracy: 0.8450\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.2860 - accuracy: 0.9375 - val_loss: 0.4943 - val_accuracy: 0.8400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2E72C3O30fv",
        "outputId": "bb94476e-e002-4ecf-ee48-b23e7aceb39d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# cross-validated accuracy for pre-trained model before training\n",
        "print(\"Base model accuracy:\", np.mean(base_model_acc_list))\n",
        "# cross-validated accuracy after training\n",
        "print(\"Final accuracy:\", np.mean(final_acc_list))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Base model accuracy: 0.09400000125169754\n",
            "Final accuracy: 0.8420000076293945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dtn-03T_ZaRX",
        "outputId": "c603e7ae-a043-4a27-cb19-ce0a33c2044e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "# plot graph of cross-validated accuracies\n",
        "acc = np.mean(history_map['accuracy'], axis=0)\n",
        "val_acc = np.mean(history_map['val_accuracy'], axis=0)\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftCYvBkKZmGa",
        "outputId": "00075142-a8a1-4701-cca9-f34c06da516b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# plot graph of cross-validated losses\n",
        "loss = np.mean(history_map['loss'], axis=0)\n",
        "val_loss = np.mean(history_map['val_loss'], axis=0)\n",
        "\n",
        "plt.figure(2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "#plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3gc1dX48e9R711ukmzLHfciF5qxKcaAwaHGDs1AMPBSEpI3kORHYkJ5IQkJhCQkEHo1BAIxzXRjwMbduPcquUiWrWZ17fn9MSN7kSVZkrValfN5nnm8M3Nn5uwu7NGde+deUVWMMcaYmgL8HYAxxpjWyRKEMcaYWlmCMMYYUytLEMYYY2plCcIYY0ytLEEYY4yplSUI43Mi8qGIXNvcZf1JRHaIyNk+OO88Efmx+/pKEfm4IWWbcJ3uIlIkIoFNjdW0f5YgTK3cH4/qxSMiJV7rVzbmXKp6nqq+0NxlWyMR+aWIzK9le5KIlIvI4IaeS1VfUdVJzRTX9xKaqu5S1ShVrWqO89e4lopIn+Y+r2l5liBMrdwfjyhVjQJ2ARd6bXulupyIBPkvylbpZeAUEUmvsX0asFpV1/ghJmOaxBKEaRQRmSAimSJyt4jsA54TkXgReU9EckTkkPs61esY79smM0TkaxF5xC27XUTOa2LZdBGZLyKFIvKpiPxdRF6uI+6GxHi/iHzjnu9jEUny2n+1iOwUkVwR+X91fT6qmgl8DlxdY9c1wIvHi6NGzDNE5Guv9XNEZIOI5IvI3wDx2tdbRD534zsgIq+ISJy77yWgO/CuWwO8S0R6un/pB7lluonIHBE5KCJbRORGr3PfKyJviMiL7mezVkQy6voM6iIise45ctzP8h4RCXD39RGRL933dkBEXne3i4g8KiLZIlIgIqsbUwszJ8YShGmKLkAC0AOYifPf0XPuenegBPhbPcePBTYCScAfgGdERJpQ9lVgMZAI3MuxP8reGhLjj4DrgE5ACPC/ACIyEPiHe/5u7vVq/VF3veAdi4j0B4a78Tb2s6o+RxLwH+AenM9iK3CqdxHgITe+k4A0nM8EVb2a79cC/1DLJWYDme7xlwH/JyJneu2/yC0TB8xpSMy1+CsQC/QCzsBJmte5++4HPgbicT7bv7rbJwHjgX7usVcAuU24tmkKVbXFlnoXYAdwtvt6AlAOhNVTfjhwyGt9HvBj9/UMYIvXvghAgS6NKYvz41oJRHjtfxl4uYHvqbYY7/Fa/x9grvv6t8Bsr32R7mdwdh3njgAKgFPc9QeB/zbxs/rafX0N8K1XOcH5Qf9xHef9AbCitu/QXe/pfpZBOMmkCoj22v8Q8Lz7+l7gU699A4GSej5bBfrU2BbofmYDvbbdBMxzX78IPAWk1jjuTGATMA4I8Pf/Cx1tsRqEaYocVS2tXhGRCBF50r1tUADMB+Kk7h4y+6pfqGqx+zKqkWW7AQe9tgHsrivgBsa4z+t1sVdM3bzPraqHqeevWDemfwPXuLWdK3F+AJvyWVWrGYN6r4tIZxGZLSJZ7nlfxqlpNET1Z1notW0nkOK1XvOzCZPGtT8lAcHueWu7xl04SW+xewvregBV/RyntvJ3IFtEnhKRmEZc15wASxCmKWoOAfxzoD8wVlVjcG4JgNc9ch/YCySISITXtrR6yp9IjHu9z+1eM/E4x7yAczvkHCAaePcE46gZg/D99/t/ON/LEPe8V9U4Z33DNu/B+SyjvbZ1B7KOE1NjHAAqcG6tHXMNVd2nqjeqajecmsUT4vaEUtXHVXUUTs2lH/CLZozL1MMShGkO0Tj30vNEJAGY5esLqupOYClwr4iEiMjJwIU+ivFNYIqInCYiIcB9HP//na+APJzbJrNVtfwE43gfGCQil7h/ud+Bc6utWjRQBOSLSArH/ojux7n3fwxV3Q0sAB4SkTARGQrcgFMLaaoQ91xhIhLmbnsDeFBEokWkB/Cz6muIyOVejfWHcBKaR0RGi8hYEQkGDgOlgOcE4jKNYAnCNIfHgHCcvxK/Bea20HWvBE7Gud3zAPA6UFZH2SbHqKprgVtxGpn34vyAZR7nGMW5rdTD/feE4lDVA8DlwMM477cv8I1Xkd8BI4F8nGTynxqneAi4R0TyROR/a7nEdJx2iT3A28AsVf20IbHVYS1OIqxergNux/mR3wZ8jfN5PuuWHw0sEpEinEbwn6jqNiAG+BfOZ74T573/8QTiMo0gbkOQMW2e2zVyg6r6vAZjTEdgNQjTZrm3H3qLSICITAamAu/4Oy5j2gt7Cta0ZV1wbqUk4tzyuUVVV/g3JGPaD7vFZIwxplZ2i8kYY0yt2tUtpqSkJO3Zs6e/wzDGmDZj2bJlB1Q1ubZ97SpB9OzZk6VLl/o7DGOMaTNEZGdd++wWkzHGmFpZgjDGGFMrSxDGGGNq1a7aIIwxLaOiooLMzExKS0uPX9i0CmFhYaSmphIcHNzgYyxBGGMaLTMzk+joaHr27Endcz2Z1kJVyc3NJTMzk/T0mrPh1s1uMRljGq20tJTExERLDm2EiJCYmNjoGp8lCGNMk1hyaFua8n11+ARRXunhH/O2Mn9Tjr9DMcaYVqXDJ4jgQOGp+Vt5f9Vef4dijGmg3Nxchg8fzvDhw+nSpQspKSlH1svLy+s9dunSpdxxxx3HvcYpp5zSLLHOmzePKVOmNMu5WlqHb6QWEYamxrEqK9/foRhjGigxMZGVK1cCcO+99xIVFcX//u/ReZAqKysJCqr95y0jI4OMjIzjXmPBggXNE2wb5rMahIikicgXIrLOnYT8J7WUuVJEVonIahFZICLDvPbtcLevFBGfjp8xNDWWTfsLKSmv8uVljDE+NGPGDG6++WbGjh3LXXfdxeLFizn55JMZMWIEp5xyChs3bgS+/xf9vffey/XXX8+ECRPo1asXjz/++JHzRUVFHSk/YcIELrvsMgYMGMCVV15J9SjYH3zwAQMGDGDUqFHccccdjaopvPbaawwZMoTBgwdz9913A1BVVcWMGTMYPHgwQ4YM4dFHHwXg8ccfZ+DAgQwdOpRp06ad+IfVQL6sQVQCP1fV5e5k6MtE5BNVXedVZjtwhqoeEpHzcObvHeu1f6I71aJPDUmJpcqjrNtbwKge8b6+nDHtyu/eXcu6PQXNes6B3WKYdeGgRh+XmZnJggULCAwMpKCggK+++oqgoCA+/fRTfv3rX/PWW28dc8yGDRv44osvKCwspH///txyyy3HPCuwYsUK1q5dS7du3Tj11FP55ptvyMjI4KabbmL+/Pmkp6czffr0Bse5Z88e7r77bpYtW0Z8fDyTJk3inXfeIS0tjaysLNasWQNAXl4eAA8//DDbt28nNDT0yLaW4LMahKruVdXl7utCYD2QUqPMAlU95K5+C6TiB0NT4wBYndlyH7wxpvldfvnlBAYGApCfn8/ll1/O4MGDufPOO1m7dm2tx1xwwQWEhoaSlJREp06d2L9//zFlxowZQ2pqKgEBAQwfPpwdO3awYcMGevXqdeS5gsYkiCVLljBhwgSSk5MJCgriyiuvZP78+fTq1Ytt27Zx++23M3fuXGJiYgAYOnQoV155JS+//HKdt858oUWuJCI9gRHAonqK3QB86LWuwMciosCTqvpUHeeeCcwE6N69e5Pi6xIbRqfoUGuHMKYJmvKXvq9ERkYeef2b3/yGiRMn8vbbb7Njxw4mTJhQ6zGhoaFHXgcGBlJZWdmkMs0hPj6e7777jo8++oh//vOfvPHGGzz77LO8//77zJ8/n3fffZcHH3yQ1atXt0ii8HkvJhGJAt4CfqqqtdZDRWQiToK422vzaao6EjgPuFVExtd2rKo+paoZqpqRnFzrkOYNMjQ1ltWZliCMaS/y8/NJSXFuWjz//PPNfv7+/fuzbds2duzYAcDrr7/e4GPHjBnDl19+yYEDB6iqquK1117jjDPO4MCBA3g8Hi699FIeeOABli9fjsfjYffu3UycOJHf//735OfnU1RU1OzvpzY+TUEiEoyTHF5R1f/UUWYo8DRwnqrmVm9X1Sz332wReRsYA8z3VaxDusXy2YZsisoqiQrt8J27jGnz7rrrLq699loeeOABLrjggmY/f3h4OE888QSTJ08mMjKS0aNH11n2s88+IzX16B30f//73zz88MNMnDgRVeWCCy5g6tSpfPfdd1x33XV4PB4AHnroIaqqqrjqqqvIz89HVbnjjjuIi4tr9vdTG5/NSS3OY3svAAdV9ad1lOkOfA5co6oLvLZHAgGqWui+/gS4T1Xn1nfNjIwMbfSEQRWl8OwktnSezNnfDuP1meMY2yuxcecwpoNZv349J510kr/D8LuioiKioqJQVW699Vb69u3LnXfe6e+w6lTb9yYiy1S11n6/vrzFdCpwNXCm21V1pYicLyI3i8jNbpnfAonAEzW6s3YGvhaR74DFwPvHSw5NFhwGleV0z1sMwGprhzDGNNC//vUvhg8fzqBBg8jPz+emm27yd0jNymf3UlT1a6DewT9U9cfAj2vZvg0YduwRPpI+npAVL9Ej9hZWWTuEMaaB7rzzzlZdYzhRHX6oDQDSx0NFMVMS97LKuroaYwxgCcLR81RAGB+8jh25xeQXV/g7ImOM8TtLEADh8dB1GANKnLFd1uyx20zGGGMJolr6eGJyVxBGmbVDGGMMliCOSj8DqSrngrhdLN916PjljTF+M3HiRD766KPvbXvssce45ZZb6jxmwoQJVHeDP//882sd0+jee+/lkUceqffa77zzDuvWHR1S7re//S2ffvppY8KvVWscFtwSRLXu4yAgiAuiNrNkx0E8Ht88H2KMOXHTp09n9uzZ39s2e/bsBo+H9MEHHzT5YbOaCeK+++7j7LPPbtK5WjtLENVCoyAlg2GVq8grrmBTdqG/IzLG1OGyyy7j/fffPzI50I4dO9izZw+nn346t9xyCxkZGQwaNIhZs2bVenzPnj05cMAZKPrBBx+kX79+nHbaaUeGBAfnGYfRo0czbNgwLr30UoqLi1mwYAFz5szhF7/4BcOHD2fr1q3MmDGDN998E3CemB4xYgRDhgzh+uuvp6ys7Mj1Zs2axciRIxkyZAgbNmxo8Hv157DgNqaEt/TxJHz1CNEUs3j7QQZ0ifF3RMa0fh/+Evatbt5zdhkC5z1c5+6EhATGjBnDhx9+yNSpU5k9ezZXXHEFIsKDDz5IQkICVVVVnHXWWaxatYqhQ4fWep5ly5Yxe/ZsVq5cSWVlJSNHjmTUqFEAXHLJJdx4440A3HPPPTzzzDPcfvvtXHTRRUyZMoXLLrvse+cqLS1lxowZfPbZZ/Tr149rrrmGf/zjH/z0p85AEklJSSxfvpwnnniCRx55hKeffvq4H4O/hwW3GoS39PGIejg3ahuLth30dzTGmHp432byvr30xhtvMHLkSEaMGMHatWu/dzuopq+++oqLL76YiIgIYmJiuOiii47sW7NmDaeffjpDhgzhlVdeqXO48GobN24kPT2dfv36AXDttdcyf/7R4eMuueQSAEaNGnVkgL/j8few4FaD8JY6GoLCuDBmMz/fPgpVxRlSyhhTp3r+0velqVOncuedd7J8+XKKi4sZNWoU27dv55FHHmHJkiXEx8czY8YMSktLm3T+GTNm8M477zBs2DCef/555s2bd0LxVg8Z3hzDhbfUsOBWg/AWHAbdT2ZY+QoOFJWx7cBhf0dkjKlDVFQUEydO5Prrrz9SeygoKCAyMpLY2Fj279/Phx9+WO85xo8fzzvvvENJSQmFhYW8++67R/YVFhbStWtXKioqeOWVV45sj46OprDw2DbK/v37s2PHDrZs2QLASy+9xBlnnHFC79Hfw4JbDaKm3mcSt+03dOYgi7YdpHdylL8jMsbUYfr06Vx88cVHbjUNGzaMESNGMGDAANLS0jj11FPrPX7kyJH88Ic/ZNiwYXTq1Ol7Q3bff//9jB07luTkZMaOHXskKUybNo0bb7yRxx9//EjjNEBYWBjPPfccl19+OZWVlYwePZqbb775mGvWp7UNC+6z4b79oUnDfde0bw3881R+F3grh/pezmPTRjRPcMa0Izbcd9vUmob7bps6D4LITpwfsZ5F2w/SnhKoMcY0hiWImkSg95kMKV3Bvvxidh8s8XdExhjjF5YgatP7TMIqDjFQdrJoe+7xyxvTAVntum1pyvflswQhImki8oWIrBORtSLyk1rKiIg8LiJbRGSViIz02netiGx2l2t9FWetek0A4NywtSzabs9DGFNTWFgYubm5liTaCFUlNzeXsLCwRh3ny15MlcDPVXW5iEQDy0TkE1X1fmrlPKCvu4wF/gGMFZEEYBaQAah77BxVbZlR9KI7Q+chTMpfx0xLEMYcIzU1lczMTHJycvwdimmgsLCw7/WQaghfTjm6F9jrvi4UkfVACuCdIKYCL6rzZ8i3IhInIl2BCcAnqnoQQEQ+ASYDr/kq3mP0nkjfhU+QU3KQvfkldI0Nb7FLG9PaBQcHk56e7u8wjI+1SBuEiPQERgCLauxKAXZ7rWe62+raXtu5Z4rIUhFZ2qx/zfQ+k0CtZGzAehZbLcIY0wH5PEGISBTwFvBTVS1o7vOr6lOqmqGqGcnJyc134u4no0FhnB28xhKEMaZD8mmCEJFgnOTwiqr+p5YiWUCa13qqu62u7S0nOAzpeRpnBa+2BGGM6ZB82YtJgGeA9ar65zqKzQGucXszjQPy3baLj4BJIhIvIvHAJHdby+o7ia6VmZTnbCG3qKzFL2+MMf7ky15MpwJXA6tFZKW77ddAdwBV/SfwAXA+sAUoBq5z9x0UkfuBJe5x91U3WLeoPs4sURMCvmPJjvOZPLhri4dgjDH+4steTF8D9Y6V7fZeurWOfc8Cz/ogtIZL7I0m9OGs3JV8sf2gJQhjTIdiT1Ifh/SbxDhZx3fb9vg7FGOMaVGWII6n7zmEUEFc9iIKSiv8HY0xxrQYSxDH0+NUqgLDmSArWbajZR7kNsaY1sASxPEEhUKvCUwMXMmibTZwnzGm47AE0QCB/SeRJjns2rTC36EYY0yLsQTREH3OAaBbztdkFzRtAnRjjGlrLEE0RFwapQn9OStgBZ9vyPZ3NMYY0yIsQTRQ6MALGB24gYVrNvs7FGOMaRGWIBpIBkwhCA+hOz6lpLzK3+EYY4zPWYJoqG4jKAvvzARdyjdbDvg7GmOM8TlLEA0VEEDQSeczIeA75q3d5e9ojDHG5yxBNELgwClESBmHN3yGx2Nz8Rpj2jdLEI3RczwVQVGMLVvIqqx8f0djjDE+ZQmiMYJC0D7ncHbgcj5fZ4P3GWPaN0sQjRQy+EKSpIDM1fP9HYoxxviUL2eUe1ZEskVkTR37fyEiK91ljYhUiUiCu2+HiKx29y31VYxN0uccqiSI/nnz2ZVb7O9ojDHGZ3xZg3gemFzXTlX9o6oOV9XhwK+AL2vMGjfR3Z/hwxgbLyyG8rTTmBywhI/X7vV3NMYY4zM+SxCqOh9o6DSh04HXfBVLcwsffhk9ArLZ+t1X/g7FGGN8xu9tECISgVPTeMtrswIfi8gyEZl5nONnishSEVmak5Pjy1CPOulCKiWYvvvnkltU1jLXNMaYFub3BAFcCHxT4/bSaao6EjgPuFVExtd1sKo+paoZqpqRnJzs61gd4XEUdz+TCwIX8vl6u81kjGmfWkOCmEaN20uqmuX+mw28DYzxQ1z1ih49jc6Sx45ln/g7FGOM8Qm/JggRiQXOAP7rtS1SRKKrXwOTgFp7QvmT9JtMWUAEPfZ8QHF5pb/DMcaYZufLbq6vAQuB/iKSKSI3iMjNInKzV7GLgY9V9bDXts7A1yLyHbAYeF9V5/oqziYLiaCg+zlMkkV8tcEemjPGtD9Bvjqxqk5vQJnncbrDem/bBgzzTVTNK37cjwja8V92L3kXht7q73CMMaZZtYY2iDYrqO9ZFAXG0m33+1RWefwdjjHGNCtLECciMJiDPc5jgi5l2aad/o7GGGOalSWIE5R82gwipIx9377h71CMMaZZWYI4QeHp49gblEbPXW+janNEGGPaD0sQJ0qE7D6XMkzXs2XDKn9HY4wxzcYSRDNIPeN6qlQ4uOB5f4dijDHN5rgJQkRuF5H4lgimrUrs2oPvQkeRnvkueKr8HY4xxjSLhtQgOgNLROQNEZksIuLroNqi3L6X0UlzyFltQ28YY9qH4yYIVb0H6As8A8wANovI/4lIbx/H1qb0Pf0K8jSSgoXP+zsUY4xpFg1qg1Cne84+d6kE4oE3ReQPPoytTenZJZH5IWeQtu8zKDnk73CMMeaENaQN4icisgz4A/ANMERVbwFGAZf6OL425eBJ0wmhnMJFL/k7FGOMOWENqUEkAJeo6rmq+m9VrQBQVQ8wxafRtTHjTz+T5Z4+VC5+BuyZCGNMG9eQNohZQKKI3OH2aBrptW+9T6NrY3olR7EwfirxxTvQHTYdqTGmbWvILabfAC8AiUAS8JyI3OPrwNqqlNN+RJ5Gkvvlk/4OxRhjTkhDbjFdBYxW1VlubWIccLVvw2q7zh2WzhwmELdjLhRl+zscY4xpsoYkiD1AmNd6KJDlm3DavvCQQA6edCVBVFK6+AV/h2OMMU3WkASRD6wVkedF5Dmc6T/zRORxEXm8roNE5FkRyRaRWqcLFZEJIpIvIivd5bde+yaLyEYR2SIiv2zsm/K3s08/jQVVA6lc/Kw9WW2MabMaMqPc2+5SbV4Dz/088DfgxXrKfKWq3+sJJSKBwN+Bc4BMnKe456jqugZe1+8Gp8Tyf7EXcUrRw7DhfRh4kb9DMsaYRjtuglDVF0QkBOjnbtpY3dX1OMfNF5GeTYhpDLDFnXoUEZkNTAXaTIIASDvlCnbNfYakLx8jwhKEMaYNakgvpgnAZpy/6p8ANonI+Ga6/ski8p2IfCgig9xtKcBurzKZ7ra64pspIktFZGlOTk4zhXXiLhyexgt6ARH7l8GuRf4OxxhjGq0hbRB/Aiap6hmqOh44F3i0Ga69HOihqsOAvwLvNOUkqvqUqmaoakZycnIzhNU84iJCyO17OflE4VnwV3+HY4wxjdaQBBGsqhurV1R1ExB8ohdW1QJVLXJffwAEi0gSTg+pNK+iqbTRXlNTMvryUuVZyIb3IHerv8MxxphGaUiCWCYiT7u9jiaIyL+ApSd6YRHpUj10uIiMcWPJBZYAfUUk3W37mAbMOdHr+cMZ/ZN5N2QKlQTBt//wdzjGGNMoDUkQN+M0EN/hLuuAW453kIi8BiwE+otIpojcICI3i8jNbpHLgDUi8h3wODBNHZXAbcBHwHrgDVVd29g31hoEBwZw6ojBvOM5FV3xMhzO9XdIxhjTYKL1DCrndjldq6oDWi6kpsvIyNClS0+4ctOs1mTl85O/vc6noXchp/8Mzvrt8Q8yxpgWIiLLVDWjtn311iBUtQrYKCLdfRJZBzCoWwzBnQfwTeipsOgpmyvCGNNmNOQWUzzOk9Sficic6sXXgbUXIsJlo1J5oGAKlBc6ScIYY9qAhjxJ/RufR9HOXTYqlT9/ks6aqFMZ/O0TMO4WCIvxd1jGGFOvhtQgzlfVL70X4HxfB9aexEWEcEVGGr/JOw9K82DJ0/4OyRhjjqshCeKcWrad19yBtHc3nJbOd1W92Bo7Dhb+DcoP+zskY4ypV50JQkRuEZHVON1UV3kt24HVLRdi+5CWEMH5Q7oyK28KFOfCwr/7OyRjjKlXfTWIV4ELcR5Su9BrGaWqV7ZAbO3OzPG9+LqsF9uTz4SvH4PC/f4OyRhj6lRnglDVfFXdoarTcQbMqwAUiLJur00zNDWOcb0SuDvvErSqDOY95O+QjDGmTg0ZzfU2YD/wCfC+u7zn47jarZvG92ZxYQIb066A5S9A9gZ/h2SMMbVqSCP1T4H+qjpIVYe4y1BfB9ZeTeifTEaPeG7LOgcNiYRPZ/k7JGOMqVVDEsRunGlHTTMQEX51/klsKQrlm64zYNNc2DbP32EZY8wxGvKg3DZgnoi8D5RVb1TVP/ssqnZuVI94Jg/qwu2bx7A0vgeBH94NN38NgSc8iroxxjSbhtQgduG0P4QA0V6LOQG/mNyfgsogZifeAjkbYLENwWGMaV0aMif172puE5GG1DxMPXonRzF9TBqzFisX9z2TiHkPw5DLIaqTv0Mzxhig/gflvvZ6/VKN3Yt9FlEHcsdZfQkKDOAvQddDRQl8eq+/QzLGmCPqu8UU6fV6cI19crwTi8izIpItImvq2H+l+2T2ahFZICLDvPbtcLevFJHWNcFDM+oUHcaVY3vw9PpA8ofPhJWvwK5F/g7LGGOA+hOE1vG6tvXaPA9Mrmf/duAMVR0C3A/UvAk/UVWH1zWRRXtx0/heBAUIfyi+EGLTYM5tUFHq77CMMabeBBEnIheLyKXu60vc5VIg9ngnVtX5wMF69i9Q1erZc74FUhsTeHvRKSaM6WO6M3vVIbIn/AEObIIvf+/vsIwxpt4E8SVwETDFfV09FtMUYH4zx3ED8KHXugIfi8gyEZlZ34EiMlNElorI0pycnGYOq2XcMqE3gQHCn7amwoir4Ju/wJ4V/g7LGNPB1Tsn9QmfXKQn8J6q1mzD8C4zEXgCOE1Vc91tKaqaJSKdcLrY3u7WSOrVGuekbqhZ/13DK4t2Me/2EaS+OhHCE2DmPAgK8Xdoxph2rMlzUvuaiAwFngamVicHAFXNcv/NBt4GxvgnwpZzy4Q+BAYID32xF6Y8BtlrbTA/Y4xf+S1BuCPC/ge4WlU3eW2PFJHo6tfAJKDWnlDtSZfYMG6Z0Jv3V+1lYdBoGHE1fP0obPvS36EZYzoonyUIEXkNWIgz4VCmiNwgIjeLyM1ukd8CicATNbqzdga+FpHvcJ63eF9V5/oqztbk5jN6kxIXzu/eXUvlpIcgsQ+8fRMczj3+wcYY08yO2wYhIpcDc1W1UETuAUYCD6jq8pYIsDHachtEtblr9nLzy8v53UWDuDY9H54+G/qcDdNeBTnu4yfGGNMoJ9oG8Rs3OZwGnA08A/yjOQM0R507qAun9knkTx9v5GDMSXD2vbDxAxuryRjT4hqSIKrcfy8AnlLV93EG7jM+ICLcezcsRp0AACAASURBVOEgisur+L8P1sPYW6DfZPjo1/aUtTGmRTUkQWSJyJPAD4EPRCS0gceZJurbOZqbzujFm8sy+WprLlz8T4hNhX9fa/NYG2NaTEN+6K8APgLOVdU8IAH4hU+jMtx+Zl96JUfyq/+s5nBANPzwFSjJgzevg6oKf4dnjOkAGpIguuL0JNosIhOAy7HRXH0uLDiQ3186lMxDJfzp403QZTBc9Djs/Abm/gp8+ICjMcZAwxLEW0CViPTBGVAvDXjVp1EZAEb3TODqcT14bsF2lu86BEOvgJNvgyX/goV/93d4xph2riEJwqOqlcAlwF9V9Rc4tQrTAu6a3J9useH87PWVHC6rhHPuh5Mugo/vgbXv+Ds8Y0w71pAEUSEi04FrgPfcbTZ5cguJDgvmz1cMY+fBYu57dx0EBMAlT0HaGPjPTNj1rb9DNMa0Uw1JENcBJwMPqup2EUkHas4wZ3xobK9E/mdCb15fupsPV++F4HCY9prTs+m1aZCz6fgnMcaYRjpuglDVdcD/AqtFZDCQqao2YUEL++nZ/RiaGssv/7OavfklEJkIV70FAUHw8iVQsNffIRpj2pnjJgi359Jm4O84w3JvEpHxPo7L1BAcGMBfpo2gosrD3W+tRlUhIR2ufBNKDsErl0Fpvr/DNMa0Iw25xfQnYJKqnqGq44FzgUd9G5apTXpSJL88bwDzN+Xw1vIsZ2O34fDDlyBnA7z6Qygt8G+Qxph2oyEJIlhVN1avuENzWyO1n1w1tgcZPeK5/711ZBe6c1f3PhMufRoyl8CLF9nor8aYZtGQBLFMRJ4WkQnu8i+gbQ+Z2oYFBAi/v2woJRVVzPrv2qM7Bl3sjPi6fx08f761SRhjTlhDEsTNwDrgDndZB9ziy6BM/XonR/HTs/vy4Zp9vLdqz9Ed/c6Fq96E/Ex49lzI3eq/II0xbV69CUJEAoHvVPXPqnqJuzyqqmUtFJ+pw42n92JYaiw/e+M7vtiQfXRH+ni4Zg6UF8Ezk2DPCv8FaYxp0+pNEKpaBWx0pwdtNBF5VkSyRaTWKUPF8biIbBGRVSIy0mvftSKy2V2ubcr127PgwACev24M/TpHMfOlpcxds+/oztRRcP3HEBwBz0+BrZ/7L1BjTJvVkFtM8cBaEflMROZULw08//PA5Hr2nwf0dZeZuBMRiUgCMAsYC4wBZolIfAOv2WHER4bwyo/HMTgllltfXc77q7zaHZL6wA0fQ3xPeOVyWG7PNhpjGieoAWV+09STq+p8EelZT5GpwIvqzHv6rYjEiUhXYALwiaoeBBCRT3ASzWtNjaW9ig0P5qUbxnLdc4u5842VpMaHMywtztkZ0xWu+wD+PQPm3AYHt8GZv3GG6zDGmOOo85dCRPqIyKmq+qX3gjPDXGYzXT8F2O21nuluq2t7bXHOFJGlIrI0JyenmcJqW6JCg3jy6gySo0K56aVl5BR6NRGFxcKP3oBR18HXf4Y3Z0B5sd9iNca0HfX9KfkYUNtTV/nuvlZBVZ9S1QxVzUhOTvZ3OH6TEBnCU9eMIq+knFtfWU55pefozsBgmPIoTHoA1s1xejjl7a77ZMYYQ/0JorOqrq650d3Ws5mun4Uzv0S1VHdbXdtNPQZ1i+X3lw5l8Y6DzJqzBo/Ha1IhETjldqc2cWgH/Gsi7Fzot1iNMa1ffQkirp594c10/TnANW5vpnFAvqruxZnidJKIxLuN05PcbeY4pg5P4X8m9Oa1xbv59durqfLUmHmu3yT48WcQGgMvTIEv/whVlf4J1hjTqtXXSL1URG5U1X95bxSRHwPLGnJyEXkNp8E5SUQycXomBQOo6j+BD4DzgS1AMc7Q4qjqQRG5H1jinuq+6gZrc3y/OLc/ASL87YstlFRU8afLhxEU6PW3QHI/uPFzeP/n8MUDsPljuORJSOjlv6CNMa2OaB1zG4tIZ+BtoJyjCSEDCAEuVtV9tR7oRxkZGbp0qY0CUu3vX2zhjx9t5JyBnXl82gjCQwKPLbT6TXj/Z04t4uxZMPpG6+VkTAciIstUNaPWfXUlCK+DJwKD3dW1qtpqn7qyBHGsFxbs4N531zI0NY6nr8kgOTr02EL5WfDuT2DLJ9D9ZLjor5DUt+WDNca0uBNKEG2JJYjafbx2Hz+ZvZLEqBCemzGavp2jjy2kCt/Nhrm/hIoSmPhrOPk2CGzIozLGmLaqvgRh9xI6gEmDuvD6TeMoq/Rw+ZML2X7g8LGFRGD4dLh1MfQ9Bz6dBc+c44wOa4zpkCxBdBBDU+N48+aTEeCGF5aQX1JRe8HozvDDl+Gy5yBvJzw5Hj75rU1EZEwHZAmiA+mRGMk/rxrF7oPF3PbqciqrPLUXFIHBlzi1iSGXwzd/gb+OguUvgqeqZYM2xviNJYgOZmyvRB74wWC+2nyA+95bR71tUJFJcPE/4MYvnC6wc26HJ8Y5PZ88dSQXY0y7YQmiA/rh6O7MHN+LFxfu5I7ZKykpP06tIGUkXD8XrngRAoLgrRvgH6fA+vecxm1jTLtkCaKD+tV5A7hrcn/eW7WHy/65gKy8kvoPEIGBU+Hmb5z2Ca2C1690xnWyITuMaZcsQXRQIsL/TOjDM9dmsCu3mIv++jULth44/oEBAU77xC0L4cLHIW8XPDcZXvwBbJ9vNQpj2hFLEB3cmQM68/atpxAbEcxVTy/iyS+31t8uUS0wCEZdC7cvh7N/B/vXwgsXwtNnwZq3oLLc98EbY3zKHpQzABSWVnDXm6v4cM0+zh3UmZ+d05/+XWp5oK4uFSWw8lVY8LgzWmxUZxg1w1liuvkoamPMibInqU2DqCpPf7WdP360kfIqD8PT4pg+Jo1LR6Z+f7C/+niqYMunsORp2PwJSACcNMUZ46nnaU5bhjGm1bAEYRrl4OFy/rM8kzeW7mbT/iJO75vE36aPJDYiuJEn2gZLn3Xmwy7Ng+STYOxNMPQKCIn0TfDGmEaxBGGaRFV5Y+lu7nlnDanxEfzrmgz6dIpq/IkqSpxnJxY/CftWQ1gcjLgKRlwNnQY0f+DGmAazBGFOyJIdB7n5pWWUV3r43dRBXDwiBWnKrSJV2PWtkyjWvweeCkjJcJLFoIshvL45qowxvuC3BCEik4G/AIHA06r6cI39jwIT3dUIoJOqxrn7qoDqKU93qepFx7ueJQjfycor4bZXl7NiVx6n903iwR8MoXtiRNNPePgArHrduf2Usx6CwmDABTBsOvSa4MyjbYzxOb8kCBEJBDYB5wCZOLPDTVfVWocHFZHbgRGqer27XqSqjbqfYQnCt6o8yiuLdvKHuRup9HiYOiyFqSO6MTY9kcCAJjY+q8Ke5bDyNVj9b6etIjweBkxxnrdIPwMCapnoyBjTLPyVIE4G7lXVc931XwGo6kN1lF8AzFLVT9x1SxCt1N78Ev708SY+XL2Xw+VVdIkJY9aFAzlvSNcTO3FlGWz5DNa+DRs/gPIiiOoCgy+FIZdB1+E2250xzcxfCeIyYLKq/thdvxoYq6q31VK2B/AtkKqqVe62SmAlUAk8rKrvHO+aliBaVkl5FZ+u38+/vtrGmqx8/njZMC4dldo8J68ogU0fObWKTR857RWRydBrIvQ+E3pPhOguzXMtYzqw+hJEa5kubBrwZnVycPVQ1SwR6QV8LiKrVXVrzQNFZCYwE6B79+4tE60BIDwkkAuHdeOskzpx44tL+fm/v6O0soorx/Y48ZMHh8OgHzhL8UEnSWz9HLZ9AavfcMp0HuwkiwEXQOoYq10Y08xaxS0mEVkB3KqqC+o41/PAe6r6Zn3XtBqE/5RWVPE/ryzn8w3ZTBnalUmDunBGv2Riw5u5sdnjgf2rnWSx9XNnoEBPhfPk9oALIH08pI2DmBO83WVMB+GvW0xBOI3UZwFZOI3UP1LVtTXKDQDmAunqBiMi8UCxqpaJSBKwEJhaVwN3NUsQ/lVe6eGhD9fz35V7OHi4nKAA4fKMNGZdOJCwYB81NJfmO09sr58Dmz+FCnc61bgekH66c0sqfTxEdfLN9Y1p4/zZzfV84DGcbq7PquqDInIfsFRV57hl7gXCVPWXXsedAjwJeHAGFHxMVZ853vUsQbQOVR5l5e5D/HflHl5cuJMBXaJ54sqR9EpuwkN2jbpwBexdBbu/hZ0LYMdXTgIBSOrn3IZKGwNpY511uyVljD0oZ/xn3sZs7nx9JRVVyt3nDeDSkSlEhLRQ05enCvauhG1fwu5FzlJyyNkXFgupo52kkZrhLGGxLROXMa2IJQjjV3vySrjjtRUs3XmImLAgLs9I46pxPUhPauHxmFQhdwvsXuwmjMWQswFQQCCxD3Qb7nSn7TYCug6DUB/XeozxM0sQxu9UlSU7DvHiwh3MXbOPSo9yap9Erhzbg3MGdia4oaPFNrfSfMhaDplLYM8K2LMSCvc4+yQAkgdAt5FHaxnJJzlzYRjTTliCMK1KdkEpry/Zzewlu8nKKyEpKpTLM1KZNjqNHomtYJTXomwnWWQth6xlzlJy0NkXHOEmjFFOTSOhFySk2+0p02ZZgjCtUpVHmb8ph1cX7+LzDdlUeZTx/ZK5bWIfxqQn+Du8o1Sdocszl0LWUufffaud7rXVIpIgZSSkjHJuTyX0grjuEBTqv7iNaQBLEKbV25dfyhtLd/Piwh0cKCrn5F6JXDWuB+EhAahCfGQII9LimjaKrC9UlMKBTc7seYd2QM5Gp6ZxpE0DQJzZ9JL6Or2mvJfoLjZ5kmkVLEGYNqOkvIpXF+/in19uJaew7Hv7zhrQiXsvGkRawgmMIutrpQXO/Nx5O53EcXC7k0gObIbywqPlQmOg00DoMhg6Dzpa44hJhaAQv4VvOh5LEKbNKa2oYuO+wur+RSzefpBHP92ER5U7zurL9aem++7hO19QhcK9TrLI2eTUNLLXOcmkrMCroFvriOsB8T2/vyT2gchEv4Rv2i9LEKZd2JNXwr1z1vLxuv0kRYVy4+npXDWuB5GhbbhXkSrk74ZDOyFvl7vsdNYP7Tjao6paeIJziyquuzN4YWQiRHd1E0oP57UNj24awRKEaVcWbcvlb19s4avNB4gND+biESlcOjKVwSkxraeNorlUlDpJ49B25xmO6hpIQZYz6VL10CLVAkOPtnkkpDtJJCLRafNI6u8MOdLePiNzQixBmHZp+a5DPPPVdj5Zt5/yKg/9Okdx8YhUpg7vRre4cH+H1zLKi51bV4d2ODWP3K1Oe8eBjZC3G743QDJOd9yE3hAWAyFRznp0F+e2VkyKUwOJSXGSig1F0iFYgjDtWn5xBe+t3sNbyzJZvisPERjTM4HhaXF0T4ygZ2Iko3rEt602i+bg8Tgz9BXnQn6mW/vY4CSTsiJnQqaSPCjaf2wiCQw5mixiujoJIywWwuKOJpfQGKdGEpfmbLeaSZtkCcJ0GDtzD/POij18uGYv23IOU17lASAxMoQfje3OVeN60DkmzM9RtjKeKufhwII9TptHwR7nFlaB1+uSQ04PLer4vQiJdpJFRIKTTCKTnQQT3cW9zZXgTCUbkeSsW+2k1bAEYTqkKo+yv6CUjfsKeXXxLj5dv59AETpFhxIcFEBwYACjeyZw4+npvh9ptj3weJweV2WFR5fCvU7tJH83HM5xaivFuVCUA4ezQT3Hnicg2Ekc0V3chJHoNL6HRkNIpFMzqU404fHO0+vBEc6+YEvuzc0ShDE4tYvXl+wmu7CMyioPRWVVzN+cQ0WVh8mDunD1yT0Y0zOBIH+NC9XeVFU6SeLwAbcGkne0plKwx0kuxQeh2N1fWXr8c4bFQnQ357ZXdDen7SS6i7O9OsGERLmvoyAw2BlTKyAQgiOt5lILSxDG1CGnsIznvtnOS9/upLC0koTIEM45qTNnndSJcb0TiQlr5hnxTN2qKpx2kdICZ+yr4lynjaSiBCqKndpL4X4nsVQnmKL9tddSaiMBTk0lMuloW0pojJtUIp1pboPDa9RYIiAkwklAMSnu7bH21ZZlCcKY4ygur+TLjTnMXbuPz9dnU1hWSYDAkNQ4xqYnMDglliEpsfRIiCAgwBpjW42qSufWVlmB2/Be6P572HldVek0wHsqncRTnOvUWErznfWyAqcnWHUS8h5fqzYBQRDZyamhhEY7SSY83llCY5yGelVAncRVnbwiko42+B+p7UQ5+ytLobLceYI+LNZpz2nBmo4/Z5SbDPwFZ0a5p1X14Rr7ZwB/xJmSFOBvqvq0u+9a4B53+wOq+sLxrmcJwjSH8koPK3Yd4pstB/hmay6rM/OPNHYnR4cyZWhXfjA8haGpse3vuYuOrqrCSS4VxW7iOOz8W3LIacDPz3Juk5W7bTCl+U4tp+TQ0dkLRQBxahoScDQJNJg4ySMk0pmPJDzB6QAQ1clJREFuTScw2CkLTi1nxFVNesv+mpM6EGdO6nOATJw5qad7zyvtJogMVb2txrEJwFIgA6fbxDJglKoequ+aliCML5RXeti0v5A1WfnM25jD5xuyKa/y0KdTFDecls7FI1I6Xhda0zilBc4tscK9bs2l0LmdFhDoPNwYFOokkdICJ9GUFx0tU3LISUqF+5x9Nbskg1Or+cXmJoVWX4Lw5RgFY4AtqrrNDWI2MBVYV+9RjnOBT1T1oHvsJ8Bk4DUfxWpMnUKCAhicEsvglFimjelOfnEFH6zZy8vf7uRX/1nNnz7eyLTR3Tm5dyLD0uKIastDfxjfCItxluT+J36uqgqnhlNVyfdGDvYBX/6XnALs9lrPBMbWUu5SERmPU9u4U1V313FsSm0XEZGZwEyA7t27N0PYxtQvNiKY6WO6M210Ggu35fLU/G387Yst/O2LLQQI9EyMJCTIuYccEhRASlw43RMi6JUcyTkDu5AQaaO1mhMQGAyBLTNBlb//1HkXeE1Vy0TkJuAF4MzGnEBVnwKeAucWU/OHaEztRIRTeidxSu8k8osrWLH7ECt25bE5u5Aqj+JRKKv0sHF/IZ+td25L/ea/a5kytCtXjevBsNQ4Aq3B27RivkwQWUCa13oqRxujAVDVXK/Vp4E/eB07ocax85o9QmOaSWxEMBP6d2JC/0617q/yqPvA3k7eXp7Ff5ZnERIUQK+kSPp0iqJzTBgJkSEkRIYwqkc8fTtFWQO48TtfNlIH4dw2OgvnB38J8CNVXetVpquq7nVfXwzcrarj3EbqZcBIt+hynEbqg/Vd0xqpTVtQWFrBx2v3s3F/IVuyi9iWU0ROYRmHy482PvZMjOCcgZ1RhbV7Cti4v5AhKbHcP3Uw3RNb8YRJps3xSyO1qlaKyG3ARzjdXJ9V1bUich+wVFXnAHeIyEVAJXAQmOEee1BE7sdJKgD3HS85GNNWRIcFc+mo1GO2l1ZUkVNYxvzNOXy0dj/PL9hBgAgDusYwoX8yH6/dz7mPzed/z+3PmJ4JfLJ+P59v2E9seDD/7/yBDOwW44d3Y9oze1DOmFaqpLyK4EA5MvTHnrwS/t/bq/liYw4AAQIju8ez/cBhDhWXc+0pPbltYh9iw4NtuBDTYPYktTHthKry6fpsCkoqmNA/mcSoUPKLK/jjxxt4ZdEuqv93DgoQkqNDOblXIqf0SWJAl2hyisrYn19KlSpThnQjNsKGETGWIIzpENZk5bNg6wFKKzyUVlSx62AxC7fmknu4/JiyESGB/HB0Gtefmk5agrVpdGT+elDOGNOCqh/m8+bxKBv2FbIz9zCdYkLpHBNGXnEFz369nZcW7uS5b3YQHxFMr+Qo0uLDqfQopRVVlFcpqfHh9EmOol/naDJ6dsAJl4zVIIzpqPbml/D+qr1szTnMtpwisvJKCAkMICw4kKBAYWduMfklzuB1MWFBXDisG5eNSmV4Wpx1wW1H7BaTMabRVJUDReWsycrnvyuzmLt2H6UVHjrHhHJ632TG90smNjyYwtIKikor6RYXzqge8UTaUCNtiiUIY8wJKyytYO6afczbmMNXm3MoKK08pkxQgDA0NZb0pCgiQgIJDwmke0IEEwd0IiUuHIDMQ8XMXbOPgtJKLhzalb6do1v6rRgvliCMMc2qssrDur0FVFR5iA4LJiIkkG05h/l2Wy6Lth9kX34pJRVVFJdXUlrhDJU+oEs0IUEBrMp0hsUOEPAoDEmJ5dKRKVw8ItV6VvmBJQhjjF+oKltzDvP5hv18tj6biioPkwZ1YfKgLkSFBfHflXt4a1km6/YWEBYcwJSh3Zg6vBvJ0aFEhwUTHChkF5SxN7+U/JIK+nSKYkCXaGswb0aWIIwxrdqarHxeWbSLOSuzvjfkSG0CA4S+naLo3yWavp2i6JkUycHD5ew4UExWXjEJkaH0SoqkV3IkGT0TiA23Wkl9LEEYY9qEorJKVuw6REFJJYWlFZRXeegUHUbX2DCiw4LYtL+Q1Vn5rMkqYEu20/OqWnhwICnx4eQWlXGo2Ol9FRwojO+bzJRhXRmSEktcRAhx4cEUllayI/cwO3OLCQkKYGDXGLp30OlkLUEYY9qlorJKduUWkxQVQnJ06JHut4cOl7M5u4hP1u3jvVV72Zt//Ck/I0MC6RwbhsejVHqUmLBghqXFMaJ7HBk94klPimyX3XstQRhjOiyPR/kuM49dB4vJK67gUHE5UaFB9EyMpEdiBKUVHtbtzWfdngIOHC4nKEAIFCGnqIyVu/ModHtrpcaHc0a/ZAZ0jWFPXgk7cw9z6HAFvTtFMqBLDH06RREdFkRESBCRoYEkRYa2iRqJJQhjjGkCj0fZdqCIhdsOMn9TDgu2HOCwO4hiWnwEMeHBbM0uorDs2C6/IUEBpMaH0zMxknG9EpjYvxN9WuE8H5YgjDGmGZRXesgpKqNzdOiREXNVlT35pWzLKeJwWRUlFZUUllaSdaiEXQeL2ZxdxJbsIgBS4sLplRxJcnQoydGhRAQHERQoBAcKwYEBhAQFEBIYQKVHKSqtpKisktjwYAZ0jWZAl5hjpqstrahi98Fi8koqGN0zoUnvycZiMsaYZlA9x7g3ESElLvyY7d725JUwb2MO32w5QGZeCVuzi8gpKqOiqnF/oEeEBLpLEKUVVWQXlgGQFBXC0nvOafwbOg5LEMYY42Pd4sL50dju/Ghs9+9tr/IoFVUeKj1KRaWHskoP5ZUegoOEyNAgIkOCyD1cxsZ9hWzYW8j+glKKK6ooLqskODCAHokRpCVE0N1HI/L6NEGIyGTgLzgzyj2tqg/X2P8z4Mc4M8rlANer6k53XxWw2i26S1Uv8mWsxhjT0gIDhMAA96G/0NrLdIoOo1N0GKf3TW65wFw+SxAiEgj8HTgHyASWiMgcVV3nVWwFkKGqxSJyC/AH4IfuvhJVHe6r+IwxxtTPl/MSjgG2qOo2VS0HZgNTvQuo6heqWuyufgscO1GvMcYYv/BlgkgBdnutZ7rb6nID8KHXepiILBWRb0XkB3UdJCIz3XJLc3JyTixiY4wxR7SKRmoRuQrIAM7w2txDVbNEpBfwuYisVtWtNY9V1aeAp8Dp5toiARtjTAfgyxpEFpDmtZ7qbvseETkb+H/ARapaVr1dVbPcf7cB84ARPozVGGNMDb5MEEuAviKSLiIhwDRgjncBERkBPImTHLK9tseLSKj7Ogk4FfBu3DbGGONjPrvFpKqVInIb8BFON9dnVXWtiNwHLFXVOcAfgSjg3+7j59XdWU8CnhQRD04Se7hG7ydjjDE+ZkNtGGNMB9ZhxmISkRxgZxMPTwIONGM4bUFHfM/QMd93R3zP0DHfd2Pfcw9VrfUpvHaVIE6EiCytK4u2Vx3xPUPHfN8d8T1Dx3zfzfmefdlIbYwxpg2zBGGMMaZWliCOesrfAfhBR3zP0DHfd0d8z9Ax33ezvWdrgzDGGFMrq0EYY4yplSUIY4wxterwCUJEJovIRhHZIiK/9Hc8viIiaSLyhYisE5G1IvITd3uCiHwiIpvdf+P9HWtzE5FAEVkhIu+56+kissj9zl93h4JpV0QkTkTeFJENIrJeRE5u79+1iNzp/re9RkReE5Gw9vhdi8izIpItImu8ttX63Yrjcff9rxKRkY25VodOEF6TGp0HDASmi8hA/0blM5XAz1V1IDAOuNV9r78EPlPVvsBn7np78xNgvdf674FHVbUPcAhnqPn25i/AXFUdAAzDef/t9rsWkRTgDpwJyAbjDO8zjfb5XT8PTK6xra7v9jygr7vMBP7RmAt16ARBAyY1ai9Uda+qLndfF+L8YKTgvN8X3GIvAHXOvdEWiUgqcAHwtLsuwJnAm26R9vieY4HxwDMAqlquqnm08+8aZ2y5cBEJAiKAvbTD71pV5wMHa2yu67udCryojm+BOBHp2tBrdfQE0dhJjdoFEemJM3z6IqCzqu51d+0DOvspLF95DLgL8LjriUCeqla66+3xO0/HmeP9OffW2tMiEkk7/q7d6QEeAXbhJIZ8YBnt/7uuVtd3e0K/cR09QXQ4IhIFvAX8VFULvPep0+e53fR7FpEpQLaqLvN3LC0sCBgJ/ENVRwCHqXE7qR1+1/E4fy2nA92AyP/f3v2EWFWHYRz/PpGFoiBCbpKSFCIEHQhEUmHIVhLhwhTSFKFdGxeBKEUotK2Ngi5cGA2VxZguJYtBF2qRRpA7E5qFfxYiSAhij4vf79rNzjDXdObquc9nM3PPPfdwD+/cec95zz3vy3/LMAPhUcZ20BNET0ON2kLSDEpyGLE9Whdf6Zxy1p9XJ3r9E2gl8JakS5Ty4euU2vzcWoaAdsZ8HBi3faY+/paSMNoc6zeAP2xfs30bGKXEv+2x7pgotg/1P27QE8SkQ43aotbeDwIXbH/a9dQxYGv9fStwdLrf21SxvdP2AtsLKbH9wfYm4EdgfV2tVfsMYPsy8Kekl+uiNZSBW62NNaW0tELSrPq33tnnVse6y0SxPQZsqd9mWgHc6CpFTWrg76SWtJZSp+4MNfqkz29pSkhaBZwEfuOfevwuiLYEYQAAAhhJREFUynWIw8ALlFbpG2zffwHsiSdpGPjA9pt1zvlXwDzgHLC5e9xtG0gaolyYfwa4CGyjHBC2NtaSdgMbKd/YOwe8R6m3tyrWkr4Ehiltva8AHwPf0RDbmiz3UsptfwHbbPc8NGfgE0RERDQb9BJTRERMIAkiIiIaJUFERESjJIiIiGiUBBEREY2SICIeA5KGO91mIx4XSRAREdEoCSLiAUjaLOmspPOSDtRZEzclfVZnEZyQ9Fxdd0jS6dqH/0hXj/7Fkr6X9KukXyQtqpuf3TXDYaTe5BTRN0kQET2S9ArlTt2VtoeAO8AmSmO4n20vAcYod7YCfA7ssL2Ucgd7Z/kIsM/2MuA1SvdRKB12t1Nmk7xE6SUU0TdPT75KRFRrgFeBn+rB/UxKU7S/ga/rOl8Ao3Umw1zbY3X5IeAbSXOA520fAbB9C6Bu76zt8fr4PLAQODX1uxXRLAkioncCDtne+a+F0kf3rfd/+9d09wi6Qz6f0WcpMUX07gSwXtJ8uDcH+EXK56jTMfQd4JTtG8B1Savr8neBsTrNb1zSurqNZyXNmta9iOhRjlAiemT7d0kfAsclPQXcBt6nDORZXp+7SrlOAaXt8v6aADodVaEkiwOS9tRtvD2NuxHRs3RzjXhIkm7ant3v9xHxqKXEFBERjXIGERERjXIGERERjZIgIiKiURJEREQ0SoKIiIhGSRAREdHoLi0nI+BqJwd2AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}